{"metadata":{"jupytext":{"encoding":"# -*- coding: utf-8 -*-"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install trax","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:18:31.579568Z","iopub.execute_input":"2023-06-11T05:18:31.580322Z","iopub.status.idle":"2023-06-11T05:18:37.019238Z","shell.execute_reply.started":"2023-06-11T05:18:31.580282Z","shell.execute_reply":"2023-06-11T05:18:37.018305Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: trax in /usr/local/lib/python3.8/site-packages (1.4.1)\nRequirement already satisfied: gin-config in /usr/local/lib/python3.8/site-packages (from trax) (0.5.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.8/site-packages (from trax) (5.9.5)\nRequirement already satisfied: jaxlib in /usr/local/lib/python3.8/site-packages (from trax) (0.4.6)\nRequirement already satisfied: scipy in /usr/local/lib/python3.8/site-packages (from trax) (1.10.1)\nRequirement already satisfied: jax in /usr/local/lib/python3.8/site-packages (from trax) (0.4.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from trax) (1.23.5)\nRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/site-packages (from trax) (4.9.2)\nRequirement already satisfied: tensorflow-text in /usr/local/lib/python3.8/site-packages (from trax) (2.12.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/site-packages (from trax) (3.7.1)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.8/site-packages (from trax) (1.4.0)\nRequirement already satisfied: gym in /usr/local/lib/python3.8/site-packages (from trax) (0.26.2)\nRequirement already satisfied: six in /usr/local/lib/python3.8/site-packages (from trax) (1.16.0)\nRequirement already satisfied: funcsigs in /usr/local/lib/python3.8/site-packages (from trax) (1.0.2)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/site-packages (from gym->trax) (1.6.0)\nRequirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/site-packages (from gym->trax) (6.6.0)\nRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/site-packages (from gym->trax) (0.0.8)\nRequirement already satisfied: opt-einsum in /usr/local/lib/python3.8/site-packages (from jax->trax) (3.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/site-packages (from matplotlib->trax) (1.0.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from matplotlib->trax) (23.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/site-packages (from matplotlib->trax) (1.4.4)\nRequirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/site-packages (from matplotlib->trax) (5.12.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/site-packages (from matplotlib->trax) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/site-packages (from matplotlib->trax) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/site-packages (from matplotlib->trax) (4.39.4)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/site-packages (from matplotlib->trax) (3.0.9)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/site-packages (from matplotlib->trax) (9.5.0)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/site-packages (from tensorflow-datasets->trax) (2.31.0)\nRequirement already satisfied: promise in /usr/local/lib/python3.8/site-packages (from tensorflow-datasets->trax) (2.3)\nRequirement already satisfied: toml in /usr/local/lib/python3.8/site-packages (from tensorflow-datasets->trax) (0.10.2)\nRequirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.8/site-packages (from tensorflow-datasets->trax) (1.3.0)\nRequirement already satisfied: array-record in /usr/local/lib/python3.8/site-packages (from tensorflow-datasets->trax) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from tensorflow-datasets->trax) (4.65.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.8/site-packages (from tensorflow-datasets->trax) (2.3.0)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.8/site-packages (from tensorflow-datasets->trax) (3.20.3)\nRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/site-packages (from tensorflow-datasets->trax) (1.13.1)\nRequirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from tensorflow-datasets->trax) (8.1.3)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.8/site-packages (from tensorflow-datasets->trax) (0.1.8)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.8/site-packages (from tensorflow-datasets->trax) (1.14.1)\nRequirement already satisfied: tensorflow<2.13,>=2.12.0 in /usr/local/lib/python3.8/site-packages (from tensorflow-text->trax) (2.12.0)\nRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/site-packages (from tensorflow-text->trax) (0.13.0)\nRequirement already satisfied: zipp in /usr/local/lib/python3.8/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->trax) (3.15.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.8/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->trax) (4.6.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2023.5.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.1.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.26.16)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.4)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (1.55.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (0.32.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (1.6.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (2.12.0)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (2.12.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (57.5.0)\nRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (23.5.9)\nRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (3.8.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (2.12.3)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (16.0.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (0.2.0)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (0.4.0)\nRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/site-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.59.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (0.40.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (3.4.3)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (2.18.1)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (1.0.0)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (2.3.4)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (0.7.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (4.9)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (0.3.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (5.3.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (2.1.2)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (0.5.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->trax) (3.2.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"1\"></a>\n## 1 - Data Preparation\n","metadata":{}},{"cell_type":"markdown","source":"<a name=\"1-1\"></a>\n### 1.1 - Importing the Data\n\nWe will first start by importing the packages we will use in this assignment. As in the previous course of this specialization, we will use the [Trax](https://github.com/google/trax) library created and maintained by the [Google Brain team](https://research.google/teams/brain/) to do most of the heavy lifting. It provides submodules to fetch and process the datasets, as well as build and train the model.","metadata":{}},{"cell_type":"code","source":"from termcolor import colored\nimport random\nimport numpy as np\n\nimport trax\nfrom trax import layers as tl\nfrom trax.fastmath import numpy as fastnp\nfrom trax.supervised import training","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:18:37.021102Z","iopub.execute_input":"2023-06-11T05:18:37.021378Z","iopub.status.idle":"2023-06-11T05:19:38.523824Z","shell.execute_reply.started":"2023-06-11T05:18:37.021352Z","shell.execute_reply":"2023-06-11T05:19:38.522689Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Next, we will import the dataset we will use to train the model. To meet the storage constraints in this lab environment, we will just use a small dataset from [Opus](http://opus.nlpl.eu/), a growing collection of translated texts from the web. Particularly, we will get an English to German translation subset specified as `opus/medical` which has medical related texts. If storage is not an issue, you can opt to get a larger corpus such as the English to German translation dataset from [ParaCrawl](https://paracrawl.eu/), a large multi-lingual translation dataset created by the European Union. Both of these datasets are available via [Tensorflow Datasets (TFDS)](https://www.tensorflow.org/datasets)\nand you can browse through the other available datasets [here](https://www.tensorflow.org/datasets/catalog/overview). We have downloaded the data for you in the `data/` directory of your workspace. As you'll see below, you can easily access this dataset from TFDS with `trax.data.TFDS`. The result is a python generator function yielding tuples. Use the `keys` argument to select what appears at which position in the tuple. For example, `keys=('en', 'de')` below will return pairs as (English sentence, German sentence).  ","metadata":{}},{"cell_type":"code","source":"# Get generator function for the training set\n# This will download the train dataset if no data_dir is specified.\ntrain_stream_fn = trax.data.TFDS('opus/medical',\n                                 data_dir='/kaggle/working/',\n                                 keys=('en', 'de'),\n                                 eval_holdout_size=0.01, # 1% for eval\n                                 train=True\n                                )\n\n# Get generator function for the eval set\neval_stream_fn = trax.data.TFDS('opus/medical',\n                                data_dir='/kaggle/working/',\n                                keys=('en', 'de'),\n                                eval_holdout_size=0.01, # 1% for eval                                \n                                train=False\n                               )","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:19:38.525172Z","iopub.execute_input":"2023-06-11T05:19:38.525866Z","iopub.status.idle":"2023-06-11T05:21:48.351615Z","shell.execute_reply.started":"2023-06-11T05:19:38.525835Z","shell.execute_reply":"2023-06-11T05:21:48.350569Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/jax/_src/xla_bridge.py:613: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mDownloading and preparing dataset 34.29 MiB (download: 34.29 MiB, generated: 188.85 MiB, total: 223.13 MiB) to /kaggle/working/opus/medical/0.1.0...\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"Dl Completed...: 0 url [00:00, ? url/s]\nDl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\nDl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\nDl Size...:   0%|          | 0/34 [00:01<?, ? MiB/s]\u001b[A\n\nExtraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]MiB]\u001b[A\nDl Size...:   3%|▎         | 1/34 [00:02<01:27,  2.65s/ MiB]\u001b[A\n\nExtraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]MiB]\u001b[A\nDl Size...:   6%|▌         | 2/34 [00:02<00:37,  1.17s/ MiB]\u001b[A\n\nExtraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]B/s]\u001b[A\nDl Size...:   9%|▉         | 3/34 [00:02<00:21,  1.45 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\nDl Size...:  12%|█▏        | 4/34 [00:02<00:20,  1.45 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\nDl Size...:  15%|█▍        | 5/34 [00:02<00:20,  1.45 MiB/s]\u001b[A\n\nExtraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]B/s]\u001b[A\nDl Size...:  18%|█▊        | 6/34 [00:03<00:07,  3.80 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:  21%|██        | 7/34 [00:03<00:07,  3.80 MiB/s]\u001b[A\n\nExtraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]B/s]\u001b[A\nDl Size...:  24%|██▎       | 8/34 [00:03<00:04,  5.41 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:  26%|██▋       | 9/34 [00:03<00:04,  5.41 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:  29%|██▉       | 10/34 [00:03<00:04,  5.41 MiB/s]\u001b[A\n\nExtraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]iB/s]\u001b[A\nDl Size...:  32%|███▏      | 11/34 [00:03<00:02,  8.23 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:  35%|███▌      | 12/34 [00:03<00:02,  8.23 MiB/s]\u001b[A\n\nExtraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]iB/s]\u001b[A\nDl Size...:  38%|███▊      | 13/34 [00:03<00:02,  9.64 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:  41%|████      | 14/34 [00:03<00:02,  9.64 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:  44%|████▍     | 15/34 [00:03<00:01,  9.64 MiB/s]\u001b[A\n\nExtraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]iB/s]\u001b[A\nDl Size...:  47%|████▋     | 16/34 [00:03<00:01, 12.45 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:  50%|█████     | 17/34 [00:03<00:01, 12.45 MiB/s]\u001b[A\n\nExtraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]iB/s]\u001b[A\nDl Size...:  53%|█████▎    | 18/34 [00:03<00:01, 13.26 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:  56%|█████▌    | 19/34 [00:03<00:01, 13.26 MiB/s]\u001b[A\n\nExtraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]iB/s]\u001b[A\nDl Size...:  59%|█████▉    | 20/34 [00:03<00:01, 13.93 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:  62%|██████▏   | 21/34 [00:03<00:00, 13.93 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:  65%|██████▍   | 22/34 [00:03<00:00, 13.93 MiB/s]\u001b[A\n\nExtraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]iB/s]\u001b[A\nDl Size...:  68%|██████▊   | 23/34 [00:03<00:00, 16.29 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:  71%|███████   | 24/34 [00:03<00:00, 16.29 MiB/s]\u001b[A\n\nExtraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]iB/s]\u001b[A\nDl Size...:  74%|███████▎  | 25/34 [00:04<00:00, 16.22 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\nDl Size...:  76%|███████▋  | 26/34 [00:04<00:00, 16.22 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\nDl Size...:  79%|███████▉  | 27/34 [00:04<00:00, 16.22 MiB/s]\u001b[A\n\nExtraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]iB/s]\u001b[A\nDl Size...:  82%|████████▏ | 28/34 [00:04<00:00, 18.03 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\nDl Size...:  85%|████████▌ | 29/34 [00:04<00:00, 18.03 MiB/s]\u001b[A\n\nExtraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]iB/s]\u001b[A\nDl Size...:  88%|████████▊ | 30/34 [00:04<00:00, 17.48 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\nDl Size...:  91%|█████████ | 31/34 [00:04<00:00, 17.48 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\nDl Size...:  94%|█████████▍| 32/34 [00:04<00:00, 17.48 MiB/s]\u001b[A\n\nExtraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]iB/s]\u001b[A\nDl Size...:  97%|█████████▋| 33/34 [00:04<00:00, 18.97 MiB/s]\u001b[A\n\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\nDl Size...: 100%|██████████| 34/34 [00:04<00:00, 18.97 MiB/s]\u001b[A\n\nDl Completed...: 100%|██████████| 1/1 [00:04<00:00,  4.48s/ url]\nDl Size...: 100%|██████████| 34/34 [00:04<00:00, 18.97 MiB/s]\u001b[A\n\nDl Completed...: 100%|██████████| 1/1 [00:04<00:00,  4.48s/ url]\nDl Size...: 100%|██████████| 34/34 [00:04<00:00, 18.97 MiB/s]\u001b[A\n\nDl Completed...: 100%|██████████| 1/1 [00:05<00:00,  4.48s/ url]]\u001b[A\u001b[A\nDl Size...: 100%|██████████| 34/34 [00:05<00:00, 18.97 MiB/s]\u001b[A\n\nDl Completed...: 100%|██████████| 1/1 [00:05<00:00,  4.48s/ url]]\u001b[A\u001b[A\nDl Size...: 100%|██████████| 34/34 [00:05<00:00, 18.97 MiB/s]\u001b[A\n\nExtraction completed...:   0%|          | 0/3 [00:05<?, ? file/s]\u001b[A\u001b[A\n\nDl Completed...: 100%|██████████| 1/1 [00:05<00:00,  4.48s/ url]4s/ file]\u001b[A\u001b[A\nDl Size...: 100%|██████████| 34/34 [00:05<00:00, 18.97 MiB/s]\u001b[A\n\nDl Completed...: 100%|██████████| 1/1 [00:05<00:00,  4.48s/ url]4s/ file]\u001b[A\u001b[A\nDl Size...: 100%|██████████| 34/34 [00:05<00:00, 18.97 MiB/s]\u001b[A\n\nDl Completed...: 100%|██████████| 1/1 [00:05<00:00,  4.48s/ url]4s/ file]\u001b[A\u001b[A\nDl Size...: 100%|██████████| 34/34 [00:05<00:00, 18.97 MiB/s]\u001b[A\n\nExtraction completed...: 100%|██████████| 3/3 [00:05<00:00,  1.82s/ file]\u001b[A\u001b[A\nDl Size...: 100%|██████████| 34/34 [00:05<00:00,  6.23 MiB/s]\nDl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.46s/ url]\nGenerating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]\nGenerating train examples...:   0%|          | 0/1108752 [00:00<?, ? examples/s]\u001b[A\nGenerating train examples...:   0%|          | 895/1108752 [00:00<02:03, 8945.08 examples/s]\u001b[A\nGenerating train examples...:   0%|          | 1824/1108752 [00:00<02:01, 9145.20 examples/s]\u001b[A\nGenerating train examples...:   0%|          | 2739/1108752 [00:00<02:01, 9102.73 examples/s]\u001b[A\nGenerating train examples...:   0%|          | 3685/1108752 [00:00<01:59, 9241.47 examples/s]\u001b[A\nGenerating train examples...:   0%|          | 4659/1108752 [00:00<01:57, 9417.67 examples/s]\u001b[A\nGenerating train examples...:   1%|          | 5601/1108752 [00:00<01:57, 9357.20 examples/s]\u001b[A\nGenerating train examples...:   1%|          | 6537/1108752 [00:00<01:58, 9263.85 examples/s]\u001b[A\nGenerating train examples...:   1%|          | 7464/1108752 [00:00<02:00, 9171.23 examples/s]\u001b[A\nGenerating train examples...:   1%|          | 8403/1108752 [00:00<01:59, 9237.88 examples/s]\u001b[A\nGenerating train examples...:   1%|          | 9376/1108752 [00:01<01:57, 9385.66 examples/s]\u001b[A\nGenerating train examples...:   1%|          | 10423/1108752 [00:01<01:53, 9713.66 examples/s]\u001b[A\nGenerating train examples...:   1%|          | 11446/1108752 [00:01<01:51, 9868.20 examples/s]\u001b[A\nGenerating train examples...:   1%|          | 12434/1108752 [00:01<01:52, 9774.34 examples/s]\u001b[A\nGenerating train examples...:   1%|          | 13463/1108752 [00:01<01:50, 9926.89 examples/s]\u001b[A\nGenerating train examples...:   1%|▏         | 14492/1108752 [00:01<01:49, 10033.40 examples/s]\u001b[A\nGenerating train examples...:   1%|▏         | 15519/1108752 [00:01<01:48, 10103.06 examples/s]\u001b[A\nGenerating train examples...:   1%|▏         | 16552/1108752 [00:01<01:47, 10170.27 examples/s]\u001b[A\nGenerating train examples...:   2%|▏         | 17591/1108752 [00:01<01:46, 10234.37 examples/s]\u001b[A\nGenerating train examples...:   2%|▏         | 18636/1108752 [00:01<01:45, 10297.31 examples/s]\u001b[A\nGenerating train examples...:   2%|▏         | 19666/1108752 [00:02<01:46, 10242.02 examples/s]\u001b[A\nGenerating train examples...:   2%|▏         | 20691/1108752 [00:02<01:46, 10227.18 examples/s]\u001b[A\nGenerating train examples...:   2%|▏         | 21716/1108752 [00:02<01:46, 10233.54 examples/s]\u001b[A\nGenerating train examples...:   2%|▏         | 22740/1108752 [00:02<01:46, 10233.95 examples/s]\u001b[A\nGenerating train examples...:   2%|▏         | 23782/1108752 [00:02<01:45, 10287.56 examples/s]\u001b[A\nGenerating train examples...:   2%|▏         | 24817/1108752 [00:02<01:45, 10305.24 examples/s]\u001b[A\nGenerating train examples...:   2%|▏         | 25854/1108752 [00:02<01:44, 10323.43 examples/s]\u001b[A\nGenerating train examples...:   2%|▏         | 26887/1108752 [00:02<01:44, 10318.24 examples/s]\u001b[A\nGenerating train examples...:   3%|▎         | 27919/1108752 [00:02<01:44, 10299.78 examples/s]\u001b[A\nGenerating train examples...:   3%|▎         | 28958/1108752 [00:02<01:44, 10325.74 examples/s]\u001b[A\nGenerating train examples...:   3%|▎         | 29991/1108752 [00:03<01:44, 10318.74 examples/s]\u001b[A\nGenerating train examples...:   3%|▎         | 31027/1108752 [00:03<01:44, 10330.30 examples/s]\u001b[A\nGenerating train examples...:   3%|▎         | 32061/1108752 [00:03<01:44, 10329.21 examples/s]\u001b[A\nGenerating train examples...:   3%|▎         | 33098/1108752 [00:03<01:44, 10339.26 examples/s]\u001b[A\nGenerating train examples...:   3%|▎         | 34134/1108752 [00:03<01:43, 10344.36 examples/s]\u001b[A\nGenerating train examples...:   3%|▎         | 35173/1108752 [00:03<01:43, 10356.10 examples/s]\u001b[A\nGenerating train examples...:   3%|▎         | 36209/1108752 [00:03<01:43, 10330.24 examples/s]\u001b[A\nGenerating train examples...:   3%|▎         | 37243/1108752 [00:03<01:44, 10286.80 examples/s]\u001b[A\nGenerating train examples...:   3%|▎         | 38272/1108752 [00:03<01:44, 10282.69 examples/s]\u001b[A\nGenerating train examples...:   4%|▎         | 39302/1108752 [00:03<01:43, 10285.50 examples/s]\u001b[A\nGenerating train examples...:   4%|▎         | 40331/1108752 [00:04<01:44, 10185.57 examples/s]\u001b[A\nGenerating train examples...:   4%|▎         | 41350/1108752 [00:04<01:45, 10072.89 examples/s]\u001b[A\nGenerating train examples...:   4%|▍         | 42358/1108752 [00:04<01:46, 10051.93 examples/s]\u001b[A\nGenerating train examples...:   4%|▍         | 43392/1108752 [00:04<01:45, 10136.13 examples/s]\u001b[A\nGenerating train examples...:   4%|▍         | 44428/1108752 [00:04<01:44, 10200.01 examples/s]\u001b[A\nGenerating train examples...:   4%|▍         | 45449/1108752 [00:04<01:44, 10128.10 examples/s]\u001b[A\nGenerating train examples...:   4%|▍         | 46464/1108752 [00:04<01:44, 10132.34 examples/s]\u001b[A\nGenerating train examples...:   4%|▍         | 47478/1108752 [00:04<01:45, 10055.50 examples/s]\u001b[A\nGenerating train examples...:   4%|▍         | 48484/1108752 [00:04<01:46, 9991.80 examples/s] \u001b[A\nGenerating train examples...:   4%|▍         | 49503/1108752 [00:04<01:45, 10049.93 examples/s]\u001b[A\nGenerating train examples...:   5%|▍         | 50524/1108752 [00:05<01:44, 10097.34 examples/s]\u001b[A\nGenerating train examples...:   5%|▍         | 51534/1108752 [00:05<01:45, 10064.23 examples/s]\u001b[A\nGenerating train examples...:   5%|▍         | 52553/1108752 [00:05<01:44, 10099.42 examples/s]\u001b[A\nGenerating train examples...:   5%|▍         | 53590/1108752 [00:05<01:43, 10178.88 examples/s]\u001b[A\nGenerating train examples...:   5%|▍         | 54608/1108752 [00:05<01:45, 10026.20 examples/s]\u001b[A\nGenerating train examples...:   5%|▌         | 55617/1108752 [00:05<01:44, 10043.38 examples/s]\u001b[A\nGenerating train examples...:   5%|▌         | 56622/1108752 [00:05<01:44, 10024.72 examples/s]\u001b[A\nGenerating train examples...:   5%|▌         | 57625/1108752 [00:05<01:47, 9795.77 examples/s] \u001b[A\nGenerating train examples...:   5%|▌         | 58606/1108752 [00:05<01:48, 9674.35 examples/s]\u001b[A\nGenerating train examples...:   5%|▌         | 59616/1108752 [00:05<01:47, 9797.27 examples/s]\u001b[A\nGenerating train examples...:   5%|▌         | 60597/1108752 [00:06<01:47, 9786.66 examples/s]\u001b[A\nGenerating train examples...:   6%|▌         | 61577/1108752 [00:06<01:47, 9745.70 examples/s]\u001b[A\nGenerating train examples...:   6%|▌         | 62553/1108752 [00:06<01:47, 9741.86 examples/s]\u001b[A\nGenerating train examples...:   6%|▌         | 63542/1108752 [00:06<01:46, 9785.52 examples/s]\u001b[A\nGenerating train examples...:   6%|▌         | 64521/1108752 [00:06<01:47, 9712.80 examples/s]\u001b[A\nGenerating train examples...:   6%|▌         | 65534/1108752 [00:06<01:46, 9835.99 examples/s]\u001b[A\nGenerating train examples...:   6%|▌         | 66518/1108752 [00:06<01:49, 9475.20 examples/s]\u001b[A\nGenerating train examples...:   6%|▌         | 67469/1108752 [00:06<01:49, 9475.76 examples/s]\u001b[A\nGenerating train examples...:   6%|▌         | 68419/1108752 [00:06<01:51, 9347.79 examples/s]\u001b[A\nGenerating train examples...:   6%|▋         | 69356/1108752 [00:06<01:51, 9291.17 examples/s]\u001b[A\nGenerating train examples...:   6%|▋         | 70287/1108752 [00:07<01:54, 9064.24 examples/s]\u001b[A\nGenerating train examples...:   6%|▋         | 71195/1108752 [00:07<01:56, 8934.62 examples/s]\u001b[A\nGenerating train examples...:   7%|▋         | 72090/1108752 [00:07<01:56, 8886.37 examples/s]\u001b[A\nGenerating train examples...:   7%|▋         | 72997/1108752 [00:07<01:55, 8938.00 examples/s]\u001b[A\nGenerating train examples...:   7%|▋         | 73906/1108752 [00:07<01:55, 8980.01 examples/s]\u001b[A\nGenerating train examples...:   7%|▋         | 74805/1108752 [00:07<01:55, 8926.31 examples/s]\u001b[A\nGenerating train examples...:   7%|▋         | 75753/1108752 [00:07<01:53, 9087.72 examples/s]\u001b[A\nGenerating train examples...:   7%|▋         | 76761/1108752 [00:07<01:50, 9380.23 examples/s]\u001b[A\nGenerating train examples...:   7%|▋         | 77745/1108752 [00:07<01:48, 9516.24 examples/s]\u001b[A\nGenerating train examples...:   7%|▋         | 78753/1108752 [00:07<01:46, 9683.75 examples/s]\u001b[A\nGenerating train examples...:   7%|▋         | 79728/1108752 [00:08<01:46, 9701.15 examples/s]\u001b[A\nGenerating train examples...:   7%|▋         | 80699/1108752 [00:08<01:46, 9610.56 examples/s]\u001b[A\nGenerating train examples...:   7%|▋         | 81661/1108752 [00:08<01:46, 9609.02 examples/s]\u001b[A\nGenerating train examples...:   7%|▋         | 82687/1108752 [00:08<01:44, 9800.97 examples/s]\u001b[A\nGenerating train examples...:   8%|▊         | 83715/1108752 [00:08<01:43, 9943.08 examples/s]\u001b[A\nGenerating train examples...:   8%|▊         | 84743/1108752 [00:08<01:41, 10042.12 examples/s]\u001b[A\nGenerating train examples...:   8%|▊         | 85767/1108752 [00:08<01:41, 10101.09 examples/s]\u001b[A\nGenerating train examples...:   8%|▊         | 86794/1108752 [00:08<01:40, 10150.41 examples/s]\u001b[A\nGenerating train examples...:   8%|▊         | 87827/1108752 [00:08<01:40, 10202.88 examples/s]\u001b[A\nGenerating train examples...:   8%|▊         | 88860/1108752 [00:09<01:39, 10239.49 examples/s]\u001b[A\nGenerating train examples...:   8%|▊         | 89889/1108752 [00:09<01:39, 10252.05 examples/s]\u001b[A\nGenerating train examples...:   8%|▊         | 90920/1108752 [00:09<01:39, 10266.87 examples/s]\u001b[A\nGenerating train examples...:   8%|▊         | 91947/1108752 [00:09<01:39, 10230.35 examples/s]\u001b[A\nGenerating train examples...:   8%|▊         | 92983/1108752 [00:09<01:38, 10267.29 examples/s]\u001b[A\nGenerating train examples...:   8%|▊         | 94010/1108752 [00:09<01:38, 10261.35 examples/s]\u001b[A\nGenerating train examples...:   9%|▊         | 95037/1108752 [00:09<01:39, 10229.50 examples/s]\u001b[A\nGenerating train examples...:   9%|▊         | 96060/1108752 [00:09<01:39, 10214.68 examples/s]\u001b[A\nGenerating train examples...:   9%|▉         | 97087/1108752 [00:09<01:38, 10230.26 examples/s]\u001b[A\nGenerating train examples...:   9%|▉         | 98118/1108752 [00:09<01:38, 10253.00 examples/s]\u001b[A\nGenerating train examples...:   9%|▉         | 99144/1108752 [00:10<01:38, 10252.46 examples/s]\u001b[A\nGenerating train examples...:   9%|▉         | 100170/1108752 [00:10<01:38, 10252.91 examples/s]\u001b[A\nGenerating train examples...:   9%|▉         | 101196/1108752 [00:10<01:38, 10207.64 examples/s]\u001b[A\nGenerating train examples...:   9%|▉         | 102228/1108752 [00:10<01:38, 10239.05 examples/s]\u001b[A\nGenerating train examples...:   9%|▉         | 103263/1108752 [00:10<01:37, 10271.64 examples/s]\u001b[A\nGenerating train examples...:   9%|▉         | 104299/1108752 [00:10<01:37, 10296.37 examples/s]\u001b[A\nGenerating train examples...:   9%|▉         | 105329/1108752 [00:10<01:37, 10289.49 examples/s]\u001b[A\nGenerating train examples...:  10%|▉         | 106358/1108752 [00:10<01:37, 10254.48 examples/s]\u001b[A\nGenerating train examples...:  10%|▉         | 107384/1108752 [00:10<01:37, 10231.00 examples/s]\u001b[A\nGenerating train examples...:  10%|▉         | 108408/1108752 [00:10<01:38, 10192.14 examples/s]\u001b[A\nGenerating train examples...:  10%|▉         | 109428/1108752 [00:11<01:38, 10145.43 examples/s]\u001b[A\nGenerating train examples...:  10%|▉         | 110443/1108752 [00:11<01:39, 9991.75 examples/s] \u001b[A\nGenerating train examples...:  10%|█         | 111443/1108752 [00:11<01:40, 9960.43 examples/s]\u001b[A\nGenerating train examples...:  10%|█         | 112440/1108752 [00:11<01:40, 9907.97 examples/s]\u001b[A\nGenerating train examples...:  10%|█         | 113432/1108752 [00:11<01:41, 9820.31 examples/s]\u001b[A\nGenerating train examples...:  10%|█         | 114429/1108752 [00:11<01:40, 9863.48 examples/s]\u001b[A\nGenerating train examples...:  10%|█         | 115416/1108752 [00:11<01:40, 9844.25 examples/s]\u001b[A\nGenerating train examples...:  10%|█         | 116401/1108752 [00:11<01:41, 9741.03 examples/s]\u001b[A\nGenerating train examples...:  11%|█         | 117376/1108752 [00:11<01:41, 9725.91 examples/s]\u001b[A\nGenerating train examples...:  11%|█         | 118399/1108752 [00:11<01:40, 9872.90 examples/s]\u001b[A\nGenerating train examples...:  11%|█         | 119389/1108752 [00:12<01:40, 9878.23 examples/s]\u001b[A\nGenerating train examples...:  11%|█         | 120405/1108752 [00:12<01:39, 9960.06 examples/s]\u001b[A\nGenerating train examples...:  11%|█         | 121420/1108752 [00:12<01:38, 10013.76 examples/s]\u001b[A\nGenerating train examples...:  11%|█         | 122443/1108752 [00:12<01:37, 10077.01 examples/s]\u001b[A\nGenerating train examples...:  11%|█         | 123454/1108752 [00:12<01:37, 10084.47 examples/s]\u001b[A\nGenerating train examples...:  11%|█         | 124475/1108752 [00:12<01:37, 10120.04 examples/s]\u001b[A\nGenerating train examples...:  11%|█▏        | 125491/1108752 [00:12<01:37, 10129.47 examples/s]\u001b[A\nGenerating train examples...:  11%|█▏        | 126504/1108752 [00:12<01:37, 10083.74 examples/s]\u001b[A\nGenerating train examples...:  12%|█▏        | 127526/1108752 [00:12<01:36, 10124.13 examples/s]\u001b[A\nGenerating train examples...:  12%|█▏        | 128539/1108752 [00:12<01:38, 9987.14 examples/s] \u001b[A\nGenerating train examples...:  12%|█▏        | 129548/1108752 [00:13<01:37, 10014.29 examples/s]\u001b[A\nGenerating train examples...:  12%|█▏        | 130550/1108752 [00:13<01:40, 9688.56 examples/s] \u001b[A\nGenerating train examples...:  12%|█▏        | 131522/1108752 [00:13<01:43, 9459.91 examples/s]\u001b[A\nGenerating train examples...:  12%|█▏        | 132471/1108752 [00:13<01:44, 9331.49 examples/s]\u001b[A\nGenerating train examples...:  12%|█▏        | 133406/1108752 [00:13<01:46, 9191.36 examples/s]\u001b[A\nGenerating train examples...:  12%|█▏        | 134327/1108752 [00:13<01:46, 9158.30 examples/s]\u001b[A\nGenerating train examples...:  12%|█▏        | 135258/1108752 [00:13<01:45, 9201.04 examples/s]\u001b[A\nGenerating train examples...:  12%|█▏        | 136179/1108752 [00:13<01:46, 9104.18 examples/s]\u001b[A\nGenerating train examples...:  12%|█▏        | 137090/1108752 [00:13<01:47, 9069.56 examples/s]\u001b[A\nGenerating train examples...:  12%|█▏        | 137998/1108752 [00:13<01:47, 9036.08 examples/s]\u001b[A\nGenerating train examples...:  13%|█▎        | 138904/1108752 [00:14<01:47, 9042.21 examples/s]\u001b[A\nGenerating train examples...:  13%|█▎        | 139842/1108752 [00:14<01:45, 9141.97 examples/s]\u001b[A\nGenerating train examples...:  13%|█▎        | 140832/1108752 [00:14<01:43, 9366.16 examples/s]\u001b[A\nGenerating train examples...:  13%|█▎        | 141838/1108752 [00:14<01:41, 9570.24 examples/s]\u001b[A\nGenerating train examples...:  13%|█▎        | 142849/1108752 [00:14<01:39, 9731.36 examples/s]\u001b[A\nGenerating train examples...:  13%|█▎        | 143871/1108752 [00:14<01:37, 9874.77 examples/s]\u001b[A\nGenerating train examples...:  13%|█▎        | 144889/1108752 [00:14<01:36, 9963.19 examples/s]\u001b[A\nGenerating train examples...:  13%|█▎        | 145902/1108752 [00:14<01:36, 10013.03 examples/s]\u001b[A\nGenerating train examples...:  13%|█▎        | 146914/1108752 [00:14<01:35, 10044.50 examples/s]\u001b[A\nGenerating train examples...:  13%|█▎        | 147931/1108752 [00:14<01:35, 10081.55 examples/s]\u001b[A\nGenerating train examples...:  13%|█▎        | 148962/1108752 [00:15<01:34, 10149.39 examples/s]\u001b[A\nGenerating train examples...:  14%|█▎        | 149979/1108752 [00:15<01:34, 10154.76 examples/s]\u001b[A\nGenerating train examples...:  14%|█▎        | 151001/1108752 [00:15<01:34, 10172.61 examples/s]\u001b[A\nGenerating train examples...:  14%|█▎        | 152024/1108752 [00:15<01:33, 10186.73 examples/s]\u001b[A\nGenerating train examples...:  14%|█▍        | 153043/1108752 [00:15<01:33, 10172.34 examples/s]\u001b[A\nGenerating train examples...:  14%|█▍        | 154061/1108752 [00:15<01:33, 10173.31 examples/s]\u001b[A\nGenerating train examples...:  14%|█▍        | 155079/1108752 [00:15<01:33, 10165.64 examples/s]\u001b[A\nGenerating train examples...:  14%|█▍        | 156102/1108752 [00:15<01:33, 10184.74 examples/s]\u001b[A\nGenerating train examples...:  14%|█▍        | 157124/1108752 [00:15<01:33, 10193.39 examples/s]\u001b[A\nGenerating train examples...:  14%|█▍        | 158144/1108752 [00:15<01:33, 10114.96 examples/s]\u001b[A\nGenerating train examples...:  14%|█▍        | 159163/1108752 [00:16<01:33, 10136.33 examples/s]\u001b[A\nGenerating train examples...:  14%|█▍        | 160180/1108752 [00:16<01:33, 10144.57 examples/s]\u001b[A\nGenerating train examples...:  15%|█▍        | 161201/1108752 [00:16<01:33, 10164.02 examples/s]\u001b[A\nGenerating train examples...:  15%|█▍        | 162228/1108752 [00:16<01:32, 10193.99 examples/s]\u001b[A\nGenerating train examples...:  15%|█▍        | 163248/1108752 [00:16<01:33, 10156.66 examples/s]\u001b[A\nGenerating train examples...:  15%|█▍        | 164264/1108752 [00:16<01:33, 10093.25 examples/s]\u001b[A\nGenerating train examples...:  15%|█▍        | 165274/1108752 [00:16<01:36, 9740.90 examples/s] \u001b[A\nGenerating train examples...:  15%|█▍        | 166284/1108752 [00:16<01:35, 9845.19 examples/s]\u001b[A\nGenerating train examples...:  15%|█▌        | 167292/1108752 [00:16<01:34, 9912.09 examples/s]\u001b[A\nGenerating train examples...:  15%|█▌        | 168286/1108752 [00:16<01:34, 9918.28 examples/s]\u001b[A\nGenerating train examples...:  15%|█▌        | 169279/1108752 [00:17<01:35, 9836.23 examples/s]\u001b[A\nGenerating train examples...:  15%|█▌        | 170270/1108752 [00:17<01:35, 9855.94 examples/s]\u001b[A\nGenerating train examples...:  15%|█▌        | 171257/1108752 [00:17<01:35, 9790.49 examples/s]\u001b[A\nGenerating train examples...:  16%|█▌        | 172260/1108752 [00:17<01:34, 9859.05 examples/s]\u001b[A\nGenerating train examples...:  16%|█▌        | 173275/1108752 [00:17<01:34, 9944.54 examples/s]\u001b[A\nGenerating train examples...:  16%|█▌        | 174270/1108752 [00:17<01:34, 9923.21 examples/s]\u001b[A\nGenerating train examples...:  16%|█▌        | 175263/1108752 [00:17<01:35, 9777.99 examples/s]\u001b[A\nGenerating train examples...:  16%|█▌        | 176242/1108752 [00:17<01:36, 9620.22 examples/s]\u001b[A\nGenerating train examples...:  16%|█▌        | 177222/1108752 [00:17<01:36, 9671.13 examples/s]\u001b[A\nGenerating train examples...:  16%|█▌        | 178241/1108752 [00:18<01:34, 9822.56 examples/s]\u001b[A\nGenerating train examples...:  16%|█▌        | 179264/1108752 [00:18<01:33, 9942.84 examples/s]\u001b[A\nGenerating train examples...:  16%|█▋        | 180286/1108752 [00:18<01:32, 10023.49 examples/s]\u001b[A\nGenerating train examples...:  16%|█▋        | 181299/1108752 [00:18<01:32, 10054.44 examples/s]\u001b[A\nGenerating train examples...:  16%|█▋        | 182319/1108752 [00:18<01:31, 10096.30 examples/s]\u001b[A\nGenerating train examples...:  17%|█▋        | 183333/1108752 [00:18<01:31, 10108.01 examples/s]\u001b[A\nGenerating train examples...:  17%|█▋        | 184344/1108752 [00:18<01:31, 10098.30 examples/s]\u001b[A\nGenerating train examples...:  17%|█▋        | 185354/1108752 [00:18<01:31, 10062.94 examples/s]\u001b[A\nGenerating train examples...:  17%|█▋        | 186377/1108752 [00:18<01:31, 10110.44 examples/s]\u001b[A\nGenerating train examples...:  17%|█▋        | 187396/1108752 [00:18<01:30, 10131.70 examples/s]\u001b[A\nGenerating train examples...:  17%|█▋        | 188410/1108752 [00:19<01:31, 10061.83 examples/s]\u001b[A\nGenerating train examples...:  17%|█▋        | 189417/1108752 [00:19<01:32, 9921.87 examples/s] \u001b[A\nGenerating train examples...:  17%|█▋        | 190410/1108752 [00:19<01:33, 9871.41 examples/s]\u001b[A\nGenerating train examples...:  17%|█▋        | 191398/1108752 [00:19<01:33, 9836.73 examples/s]\u001b[A\nGenerating train examples...:  17%|█▋        | 192411/1108752 [00:19<01:32, 9922.01 examples/s]\u001b[A\nGenerating train examples...:  17%|█▋        | 193419/1108752 [00:19<01:31, 9966.97 examples/s]\u001b[A\nGenerating train examples...:  18%|█▊        | 194416/1108752 [00:19<01:32, 9916.41 examples/s]\u001b[A\nGenerating train examples...:  18%|█▊        | 195408/1108752 [00:19<01:33, 9796.48 examples/s]\u001b[A\nGenerating train examples...:  18%|█▊        | 196389/1108752 [00:19<01:33, 9718.26 examples/s]\u001b[A\nGenerating train examples...:  18%|█▊        | 197362/1108752 [00:19<01:36, 9400.38 examples/s]\u001b[A\nGenerating train examples...:  18%|█▊        | 198305/1108752 [00:20<01:39, 9187.91 examples/s]\u001b[A\nGenerating train examples...:  18%|█▊        | 199226/1108752 [00:20<01:39, 9154.06 examples/s]\u001b[A\nGenerating train examples...:  18%|█▊        | 200143/1108752 [00:20<01:39, 9119.21 examples/s]\u001b[A\nGenerating train examples...:  18%|█▊        | 201056/1108752 [00:20<01:39, 9121.99 examples/s]\u001b[A\nGenerating train examples...:  18%|█▊        | 201969/1108752 [00:20<01:39, 9071.53 examples/s]\u001b[A\nGenerating train examples...:  18%|█▊        | 202877/1108752 [00:20<01:40, 9036.74 examples/s]\u001b[A\nGenerating train examples...:  18%|█▊        | 203781/1108752 [00:20<01:42, 8818.72 examples/s]\u001b[A\nGenerating train examples...:  18%|█▊        | 204664/1108752 [00:20<01:44, 8684.66 examples/s]\u001b[A\nGenerating train examples...:  19%|█▊        | 205534/1108752 [00:20<01:45, 8538.11 examples/s]\u001b[A\nGenerating train examples...:  19%|█▊        | 206430/1108752 [00:20<01:44, 8659.77 examples/s]\u001b[A\nGenerating train examples...:  19%|█▊        | 207416/1108752 [00:21<01:40, 9009.37 examples/s]\u001b[A\nGenerating train examples...:  19%|█▉        | 208389/1108752 [00:21<01:37, 9220.51 examples/s]\u001b[A\nGenerating train examples...:  19%|█▉        | 209366/1108752 [00:21<01:35, 9382.47 examples/s]\u001b[A\nGenerating train examples...:  19%|█▉        | 210359/1108752 [00:21<01:34, 9544.05 examples/s]\u001b[A\nGenerating train examples...:  19%|█▉        | 211343/1108752 [00:21<01:33, 9631.83 examples/s]\u001b[A\nGenerating train examples...:  19%|█▉        | 212307/1108752 [00:21<01:33, 9624.75 examples/s]\u001b[A\nGenerating train examples...:  19%|█▉        | 213321/1108752 [00:21<01:31, 9777.08 examples/s]\u001b[A\nGenerating train examples...:  19%|█▉        | 214339/1108752 [00:21<01:30, 9894.78 examples/s]\u001b[A\nGenerating train examples...:  19%|█▉        | 215352/1108752 [00:21<01:29, 9962.10 examples/s]\u001b[A\nGenerating train examples...:  20%|█▉        | 216373/1108752 [00:21<01:28, 10035.45 examples/s]\u001b[A\nGenerating train examples...:  20%|█▉        | 217393/1108752 [00:22<01:28, 10083.58 examples/s]\u001b[A\nGenerating train examples...:  20%|█▉        | 218413/1108752 [00:22<01:28, 10117.43 examples/s]\u001b[A\nGenerating train examples...:  20%|█▉        | 219444/1108752 [00:22<01:27, 10173.12 examples/s]\u001b[A\nGenerating train examples...:  20%|█▉        | 220467/1108752 [00:22<01:27, 10189.87 examples/s]\u001b[A\nGenerating train examples...:  20%|█▉        | 221487/1108752 [00:22<01:27, 10176.33 examples/s]\u001b[A\nGenerating train examples...:  20%|██        | 222505/1108752 [00:22<01:27, 10174.34 examples/s]\u001b[A\nGenerating train examples...:  20%|██        | 223523/1108752 [00:22<01:27, 10136.34 examples/s]\u001b[A\nGenerating train examples...:  20%|██        | 224546/1108752 [00:22<01:27, 10162.44 examples/s]\u001b[A\nGenerating train examples...:  20%|██        | 225565/1108752 [00:22<01:26, 10168.60 examples/s]\u001b[A\nGenerating train examples...:  20%|██        | 226582/1108752 [00:22<01:27, 10121.40 examples/s]\u001b[A\nGenerating train examples...:  21%|██        | 227595/1108752 [00:23<01:28, 10002.51 examples/s]\u001b[A\nGenerating train examples...:  21%|██        | 228596/1108752 [00:23<01:28, 9970.50 examples/s] \u001b[A\nGenerating train examples...:  21%|██        | 229594/1108752 [00:23<01:29, 9800.05 examples/s]\u001b[A\nGenerating train examples...:  21%|██        | 230575/1108752 [00:23<01:30, 9752.42 examples/s]\u001b[A\nGenerating train examples...:  21%|██        | 231582/1108752 [00:23<01:29, 9845.14 examples/s]\u001b[A\nGenerating train examples...:  21%|██        | 232567/1108752 [00:23<01:30, 9716.92 examples/s]\u001b[A\nGenerating train examples...:  21%|██        | 233540/1108752 [00:23<01:31, 9581.60 examples/s]\u001b[A\nGenerating train examples...:  21%|██        | 234499/1108752 [00:23<01:32, 9450.06 examples/s]\u001b[A\nGenerating train examples...:  21%|██        | 235500/1108752 [00:23<01:30, 9611.28 examples/s]\u001b[A\nGenerating train examples...:  21%|██▏       | 236521/1108752 [00:24<01:29, 9786.87 examples/s]\u001b[A\nGenerating train examples...:  21%|██▏       | 237501/1108752 [00:24<01:29, 9757.58 examples/s]\u001b[A\nGenerating train examples...:  22%|██▏       | 238485/1108752 [00:24<01:28, 9780.85 examples/s]\u001b[A\nGenerating train examples...:  22%|██▏       | 239473/1108752 [00:24<01:28, 9809.70 examples/s]\u001b[A\nGenerating train examples...:  22%|██▏       | 240470/1108752 [00:24<01:28, 9854.85 examples/s]\u001b[A\nGenerating train examples...:  22%|██▏       | 241486/1108752 [00:24<01:27, 9945.01 examples/s]\u001b[A\nGenerating train examples...:  22%|██▏       | 242506/1108752 [00:24<01:26, 10018.94 examples/s]\u001b[A\nGenerating train examples...:  22%|██▏       | 243509/1108752 [00:24<01:26, 10018.64 examples/s]\u001b[A\nGenerating train examples...:  22%|██▏       | 244530/1108752 [00:24<01:25, 10075.17 examples/s]\u001b[A\nGenerating train examples...:  22%|██▏       | 245553/1108752 [00:24<01:25, 10119.38 examples/s]\u001b[A\nGenerating train examples...:  22%|██▏       | 246566/1108752 [00:25<01:25, 10119.60 examples/s]\u001b[A\nGenerating train examples...:  22%|██▏       | 247579/1108752 [00:25<01:26, 9947.83 examples/s] \u001b[A\nGenerating train examples...:  22%|██▏       | 248575/1108752 [00:25<01:26, 9891.59 examples/s]\u001b[A\nGenerating train examples...:  23%|██▎       | 249593/1108752 [00:25<01:26, 9975.23 examples/s]\u001b[A\nGenerating train examples...:  23%|██▎       | 250615/1108752 [00:25<01:25, 10046.29 examples/s]\u001b[A\nGenerating train examples...:  23%|██▎       | 251634/1108752 [00:25<01:24, 10087.99 examples/s]\u001b[A\nGenerating train examples...:  23%|██▎       | 252646/1108752 [00:25<01:24, 10094.94 examples/s]\u001b[A\nGenerating train examples...:  23%|██▎       | 253660/1108752 [00:25<01:24, 10107.27 examples/s]\u001b[A\nGenerating train examples...:  23%|██▎       | 254671/1108752 [00:25<01:24, 10087.21 examples/s]\u001b[A\nGenerating train examples...:  23%|██▎       | 255680/1108752 [00:25<01:24, 10069.46 examples/s]\u001b[A\nGenerating train examples...:  23%|██▎       | 256688/1108752 [00:26<01:25, 9981.51 examples/s] \u001b[A\nGenerating train examples...:  23%|██▎       | 257687/1108752 [00:26<01:25, 9955.67 examples/s]\u001b[A\nGenerating train examples...:  23%|██▎       | 258683/1108752 [00:26<01:25, 9943.05 examples/s]\u001b[A\nGenerating train examples...:  23%|██▎       | 259685/1108752 [00:26<01:25, 9965.38 examples/s]\u001b[A\nGenerating train examples...:  24%|██▎       | 260682/1108752 [00:26<01:25, 9890.47 examples/s]\u001b[A\nGenerating train examples...:  24%|██▎       | 261676/1108752 [00:26<01:25, 9903.88 examples/s]\u001b[A\nGenerating train examples...:  24%|██▎       | 262667/1108752 [00:26<01:28, 9595.16 examples/s]\u001b[A\nGenerating train examples...:  24%|██▍       | 263629/1108752 [00:26<01:30, 9361.73 examples/s]\u001b[A\nGenerating train examples...:  24%|██▍       | 264568/1108752 [00:26<01:33, 9050.40 examples/s]\u001b[A\nGenerating train examples...:  24%|██▍       | 265476/1108752 [00:26<01:36, 8780.44 examples/s]\u001b[A\nGenerating train examples...:  24%|██▍       | 266366/1108752 [00:27<01:35, 8812.70 examples/s]\u001b[A\nGenerating train examples...:  24%|██▍       | 267250/1108752 [00:27<01:35, 8790.75 examples/s]\u001b[A\nGenerating train examples...:  24%|██▍       | 268131/1108752 [00:27<01:35, 8778.62 examples/s]\u001b[A\nGenerating train examples...:  24%|██▍       | 269011/1108752 [00:27<01:35, 8782.82 examples/s]\u001b[A\nGenerating train examples...:  24%|██▍       | 269890/1108752 [00:27<01:36, 8713.79 examples/s]\u001b[A\nGenerating train examples...:  24%|██▍       | 270835/1108752 [00:27<01:33, 8929.41 examples/s]\u001b[A\nGenerating train examples...:  25%|██▍       | 271744/1108752 [00:27<01:33, 8974.56 examples/s]\u001b[A\nGenerating train examples...:  25%|██▍       | 272679/1108752 [00:27<01:32, 9084.63 examples/s]\u001b[A\nGenerating train examples...:  25%|██▍       | 273688/1108752 [00:27<01:28, 9383.29 examples/s]\u001b[A\nGenerating train examples...:  25%|██▍       | 274701/1108752 [00:27<01:26, 9605.64 examples/s]\u001b[A\nGenerating train examples...:  25%|██▍       | 275711/1108752 [00:28<01:25, 9753.00 examples/s]\u001b[A\nGenerating train examples...:  25%|██▍       | 276724/1108752 [00:28<01:24, 9864.63 examples/s]\u001b[A\nGenerating train examples...:  25%|██▌       | 277741/1108752 [00:28<01:23, 9954.13 examples/s]\u001b[A\nGenerating train examples...:  25%|██▌       | 278765/1108752 [00:28<01:22, 10037.42 examples/s]\u001b[A\nGenerating train examples...:  25%|██▌       | 279781/1108752 [00:28<01:22, 10072.49 examples/s]\u001b[A\nGenerating train examples...:  25%|██▌       | 280793/1108752 [00:28<01:22, 10085.20 examples/s]\u001b[A\nGenerating train examples...:  25%|██▌       | 281802/1108752 [00:28<01:22, 10083.98 examples/s]\u001b[A\nGenerating train examples...:  26%|██▌       | 282826/1108752 [00:28<01:21, 10130.41 examples/s]\u001b[A\nGenerating train examples...:  26%|██▌       | 283848/1108752 [00:28<01:21, 10155.71 examples/s]\u001b[A\nGenerating train examples...:  26%|██▌       | 284864/1108752 [00:28<01:21, 10089.02 examples/s]\u001b[A\nGenerating train examples...:  26%|██▌       | 285874/1108752 [00:29<01:22, 9948.07 examples/s] \u001b[A\nGenerating train examples...:  26%|██▌       | 286870/1108752 [00:29<01:22, 9943.58 examples/s]\u001b[A\nGenerating train examples...:  26%|██▌       | 287865/1108752 [00:29<01:22, 9902.02 examples/s]\u001b[A\nGenerating train examples...:  26%|██▌       | 288856/1108752 [00:29<01:23, 9813.01 examples/s]\u001b[A\nGenerating train examples...:  26%|██▌       | 289859/1108752 [00:29<01:22, 9875.43 examples/s]\u001b[A\nGenerating train examples...:  26%|██▌       | 290847/1108752 [00:29<01:22, 9862.71 examples/s]\u001b[A\nGenerating train examples...:  26%|██▋       | 291834/1108752 [00:29<01:23, 9759.46 examples/s]\u001b[A\nGenerating train examples...:  26%|██▋       | 292811/1108752 [00:29<01:24, 9674.84 examples/s]\u001b[A\nGenerating train examples...:  26%|██▋       | 293786/1108752 [00:29<01:24, 9695.49 examples/s]\u001b[A\nGenerating train examples...:  27%|██▋       | 294798/1108752 [00:29<01:22, 9818.99 examples/s]\u001b[A\nGenerating train examples...:  27%|██▋       | 295812/1108752 [00:30<01:22, 9913.72 examples/s]\u001b[A\nGenerating train examples...:  27%|██▋       | 296816/1108752 [00:30<01:21, 9951.13 examples/s]\u001b[A\nGenerating train examples...:  27%|██▋       | 297834/1108752 [00:30<01:20, 10019.26 examples/s]\u001b[A\nGenerating train examples...:  27%|██▋       | 298847/1108752 [00:30<01:20, 10050.03 examples/s]\u001b[A\nGenerating train examples...:  27%|██▋       | 299862/1108752 [00:30<01:20, 10076.28 examples/s]\u001b[A\nGenerating train examples...:  27%|██▋       | 300881/1108752 [00:30<01:19, 10107.98 examples/s]\u001b[A\nGenerating train examples...:  27%|██▋       | 301898/1108752 [00:30<01:19, 10123.68 examples/s]\u001b[A\nGenerating train examples...:  27%|██▋       | 302917/1108752 [00:30<01:19, 10141.97 examples/s]\u001b[A\nGenerating train examples...:  27%|██▋       | 303932/1108752 [00:30<01:19, 10113.69 examples/s]\u001b[A\nGenerating train examples...:  28%|██▊       | 304944/1108752 [00:31<01:19, 10111.43 examples/s]\u001b[A\nGenerating train examples...:  28%|██▊       | 305958/1108752 [00:31<01:19, 10118.70 examples/s]\u001b[A\nGenerating train examples...:  28%|██▊       | 306972/1108752 [00:31<01:19, 10123.80 examples/s]\u001b[A\nGenerating train examples...:  28%|██▊       | 307993/1108752 [00:31<01:18, 10149.53 examples/s]\u001b[A\nGenerating train examples...:  28%|██▊       | 309014/1108752 [00:31<01:18, 10165.93 examples/s]\u001b[A\nGenerating train examples...:  28%|██▊       | 310041/1108752 [00:31<01:18, 10195.98 examples/s]\u001b[A\nGenerating train examples...:  28%|██▊       | 311061/1108752 [00:31<01:18, 10160.99 examples/s]\u001b[A\nGenerating train examples...:  28%|██▊       | 312078/1108752 [00:31<01:18, 10099.22 examples/s]\u001b[A\nGenerating train examples...:  28%|██▊       | 313089/1108752 [00:31<01:19, 10063.18 examples/s]\u001b[A\nGenerating train examples...:  28%|██▊       | 314098/1108752 [00:31<01:18, 10070.89 examples/s]\u001b[A\nGenerating train examples...:  28%|██▊       | 315106/1108752 [00:32<01:18, 10058.35 examples/s]\u001b[A\nGenerating train examples...:  29%|██▊       | 316123/1108752 [00:32<01:18, 10091.12 examples/s]\u001b[A\nGenerating train examples...:  29%|██▊       | 317144/1108752 [00:32<01:18, 10125.21 examples/s]\u001b[A\nGenerating train examples...:  29%|██▊       | 318157/1108752 [00:32<01:18, 10095.84 examples/s]\u001b[A\nGenerating train examples...:  29%|██▉       | 319178/1108752 [00:32<01:17, 10127.06 examples/s]\u001b[A\nGenerating train examples...:  29%|██▉       | 320191/1108752 [00:32<01:17, 10117.28 examples/s]\u001b[A\nGenerating train examples...:  29%|██▉       | 321203/1108752 [00:32<01:17, 10103.58 examples/s]\u001b[A\nGenerating train examples...:  29%|██▉       | 322214/1108752 [00:32<01:18, 9990.56 examples/s] \u001b[A\nGenerating train examples...:  29%|██▉       | 323214/1108752 [00:32<01:18, 9960.15 examples/s]\u001b[A\nGenerating train examples...:  29%|██▉       | 324211/1108752 [00:32<01:19, 9830.00 examples/s]\u001b[A\nGenerating train examples...:  29%|██▉       | 325195/1108752 [00:33<01:20, 9771.40 examples/s]\u001b[A\nGenerating train examples...:  29%|██▉       | 326173/1108752 [00:33<01:20, 9716.12 examples/s]\u001b[A\nGenerating train examples...:  30%|██▉       | 327145/1108752 [00:33<01:22, 9438.97 examples/s]\u001b[A\nGenerating train examples...:  30%|██▉       | 328097/1108752 [00:33<01:22, 9461.34 examples/s]\u001b[A\nGenerating train examples...:  30%|██▉       | 329045/1108752 [00:33<01:23, 9287.12 examples/s]\u001b[A\nGenerating train examples...:  30%|██▉       | 329994/1108752 [00:33<01:23, 9342.41 examples/s]\u001b[A\nGenerating train examples...:  30%|██▉       | 330998/1108752 [00:33<01:21, 9545.16 examples/s]\u001b[A\nGenerating train examples...:  30%|██▉       | 332013/1108752 [00:33<01:19, 9722.98 examples/s]\u001b[A\nGenerating train examples...:  30%|███       | 333002/1108752 [00:33<01:19, 9770.71 examples/s]\u001b[A\nGenerating train examples...:  30%|███       | 333980/1108752 [00:33<01:19, 9694.22 examples/s]\u001b[A\nGenerating train examples...:  30%|███       | 334951/1108752 [00:34<01:21, 9475.92 examples/s]\u001b[A\nGenerating train examples...:  30%|███       | 335900/1108752 [00:34<01:22, 9371.68 examples/s]\u001b[A\nGenerating train examples...:  30%|███       | 336840/1108752 [00:34<01:22, 9376.83 examples/s]\u001b[A\nGenerating train examples...:  30%|███       | 337837/1108752 [00:34<01:20, 9549.41 examples/s]\u001b[A\nGenerating train examples...:  31%|███       | 338852/1108752 [00:34<01:19, 9726.87 examples/s]\u001b[A\nGenerating train examples...:  31%|███       | 339862/1108752 [00:34<01:18, 9836.53 examples/s]\u001b[A\nGenerating train examples...:  31%|███       | 340876/1108752 [00:34<01:17, 9925.36 examples/s]\u001b[A\nGenerating train examples...:  31%|███       | 341892/1108752 [00:34<01:16, 9992.02 examples/s]\u001b[A\nGenerating train examples...:  31%|███       | 342909/1108752 [00:34<01:16, 10043.30 examples/s]\u001b[A\nGenerating train examples...:  31%|███       | 343923/1108752 [00:34<01:15, 10069.11 examples/s]\u001b[A\nGenerating train examples...:  31%|███       | 344931/1108752 [00:35<01:16, 10000.95 examples/s]\u001b[A\nGenerating train examples...:  31%|███       | 345932/1108752 [00:35<01:16, 10001.73 examples/s]\u001b[A\nGenerating train examples...:  31%|███▏      | 346933/1108752 [00:35<01:16, 9894.04 examples/s] \u001b[A\nGenerating train examples...:  31%|███▏      | 347923/1108752 [00:35<01:17, 9782.59 examples/s]\u001b[A\nGenerating train examples...:  31%|███▏      | 348915/1108752 [00:35<01:17, 9821.22 examples/s]\u001b[A\nGenerating train examples...:  32%|███▏      | 349898/1108752 [00:35<01:17, 9813.23 examples/s]\u001b[A\nGenerating train examples...:  32%|███▏      | 350880/1108752 [00:35<01:17, 9765.49 examples/s]\u001b[A\nGenerating train examples...:  32%|███▏      | 351857/1108752 [00:35<01:18, 9639.29 examples/s]\u001b[A\nGenerating train examples...:  32%|███▏      | 352822/1108752 [00:35<01:19, 9535.21 examples/s]\u001b[A\nGenerating train examples...:  32%|███▏      | 353818/1108752 [00:35<01:18, 9659.74 examples/s]\u001b[A\nGenerating train examples...:  32%|███▏      | 354831/1108752 [00:36<01:16, 9796.89 examples/s]\u001b[A\nGenerating train examples...:  32%|███▏      | 355847/1108752 [00:36<01:16, 9903.77 examples/s]\u001b[A\nGenerating train examples...:  32%|███▏      | 356854/1108752 [00:36<01:15, 9953.10 examples/s]\u001b[A\nGenerating train examples...:  32%|███▏      | 357867/1108752 [00:36<01:15, 10004.60 examples/s]\u001b[A\nGenerating train examples...:  32%|███▏      | 358884/1108752 [00:36<01:14, 10051.33 examples/s]\u001b[A\nGenerating train examples...:  32%|███▏      | 359890/1108752 [00:36<01:14, 10049.66 examples/s]\u001b[A\nGenerating train examples...:  33%|███▎      | 360896/1108752 [00:36<01:14, 10049.58 examples/s]\u001b[A\nGenerating train examples...:  33%|███▎      | 361911/1108752 [00:36<01:14, 10078.63 examples/s]\u001b[A\nGenerating train examples...:  33%|███▎      | 362926/1108752 [00:36<01:13, 10099.43 examples/s]\u001b[A\nGenerating train examples...:  33%|███▎      | 363936/1108752 [00:36<01:13, 10094.47 examples/s]\u001b[A\nGenerating train examples...:  33%|███▎      | 364946/1108752 [00:37<01:13, 10057.43 examples/s]\u001b[A\nGenerating train examples...:  33%|███▎      | 365952/1108752 [00:37<01:13, 10040.45 examples/s]\u001b[A\nGenerating train examples...:  33%|███▎      | 366959/1108752 [00:37<01:13, 10048.80 examples/s]\u001b[A\nGenerating train examples...:  33%|███▎      | 367975/1108752 [00:37<01:13, 10081.31 examples/s]\u001b[A\nGenerating train examples...:  33%|███▎      | 368985/1108752 [00:37<01:13, 10084.36 examples/s]\u001b[A\nGenerating train examples...:  33%|███▎      | 369995/1108752 [00:37<01:13, 10088.82 examples/s]\u001b[A\nGenerating train examples...:  33%|███▎      | 371004/1108752 [00:37<01:13, 10057.23 examples/s]\u001b[A\nGenerating train examples...:  34%|███▎      | 372019/1108752 [00:37<01:13, 10081.76 examples/s]\u001b[A\nGenerating train examples...:  34%|███▎      | 373033/1108752 [00:37<01:12, 10097.26 examples/s]\u001b[A\nGenerating train examples...:  34%|███▎      | 374051/1108752 [00:37<01:12, 10120.95 examples/s]\u001b[A\nGenerating train examples...:  34%|███▍      | 375064/1108752 [00:38<01:13, 9975.80 examples/s] \u001b[A\nGenerating train examples...:  34%|███▍      | 376063/1108752 [00:38<01:14, 9865.55 examples/s]\u001b[A\nGenerating train examples...:  34%|███▍      | 377051/1108752 [00:38<01:14, 9800.30 examples/s]\u001b[A\nGenerating train examples...:  34%|███▍      | 378060/1108752 [00:38<01:13, 9884.31 examples/s]\u001b[A\nGenerating train examples...:  34%|███▍      | 379069/1108752 [00:38<01:13, 9944.38 examples/s]\u001b[A\nGenerating train examples...:  34%|███▍      | 380071/1108752 [00:38<01:13, 9966.22 examples/s]\u001b[A\nGenerating train examples...:  34%|███▍      | 381069/1108752 [00:38<01:12, 9968.37 examples/s]\u001b[A\nGenerating train examples...:  34%|███▍      | 382067/1108752 [00:38<01:14, 9695.91 examples/s]\u001b[A\nGenerating train examples...:  35%|███▍      | 383039/1108752 [00:38<01:15, 9661.23 examples/s]\u001b[A\nGenerating train examples...:  35%|███▍      | 384007/1108752 [00:39<01:15, 9597.91 examples/s]\u001b[A\nGenerating train examples...:  35%|███▍      | 384988/1108752 [00:39<01:14, 9657.51 examples/s]\u001b[A\nGenerating train examples...:  35%|███▍      | 385970/1108752 [00:39<01:14, 9703.20 examples/s]\u001b[A\nGenerating train examples...:  35%|███▍      | 386941/1108752 [00:39<01:14, 9646.69 examples/s]\u001b[A\nGenerating train examples...:  35%|███▍      | 387945/1108752 [00:39<01:13, 9760.74 examples/s]\u001b[A\nGenerating train examples...:  35%|███▌      | 388949/1108752 [00:39<01:13, 9842.19 examples/s]\u001b[A\nGenerating train examples...:  35%|███▌      | 389934/1108752 [00:39<01:13, 9807.71 examples/s]\u001b[A\nGenerating train examples...:  35%|███▌      | 390916/1108752 [00:39<01:14, 9684.15 examples/s]\u001b[A\nGenerating train examples...:  35%|███▌      | 391898/1108752 [00:39<01:13, 9723.08 examples/s]\u001b[A\nGenerating train examples...:  35%|███▌      | 392871/1108752 [00:39<01:15, 9481.10 examples/s]\u001b[A\nGenerating train examples...:  36%|███▌      | 393821/1108752 [00:40<01:15, 9431.88 examples/s]\u001b[A\nGenerating train examples...:  36%|███▌      | 394766/1108752 [00:40<01:17, 9259.60 examples/s]\u001b[A\nGenerating train examples...:  36%|███▌      | 395693/1108752 [00:40<01:17, 9160.78 examples/s]\u001b[A\nGenerating train examples...:  36%|███▌      | 396610/1108752 [00:40<01:19, 8972.48 examples/s]\u001b[A\nGenerating train examples...:  36%|███▌      | 397509/1108752 [00:40<01:20, 8863.42 examples/s]\u001b[A\nGenerating train examples...:  36%|███▌      | 398396/1108752 [00:40<01:20, 8801.13 examples/s]\u001b[A\nGenerating train examples...:  36%|███▌      | 399300/1108752 [00:40<01:19, 8868.58 examples/s]\u001b[A\nGenerating train examples...:  36%|███▌      | 400222/1108752 [00:40<01:18, 8971.60 examples/s]\u001b[A\nGenerating train examples...:  36%|███▌      | 401132/1108752 [00:40<01:18, 9009.41 examples/s]\u001b[A\nGenerating train examples...:  36%|███▋      | 402034/1108752 [00:40<01:18, 8989.88 examples/s]\u001b[A\nGenerating train examples...:  36%|███▋      | 402943/1108752 [00:41<01:18, 9018.68 examples/s]\u001b[A\nGenerating train examples...:  36%|███▋      | 403885/1108752 [00:41<01:17, 9135.61 examples/s]\u001b[A\nGenerating train examples...:  37%|███▋      | 404805/1108752 [00:41<01:16, 9154.29 examples/s]\u001b[A\nGenerating train examples...:  37%|███▋      | 405793/1108752 [00:41<01:15, 9368.46 examples/s]\u001b[A\nGenerating train examples...:  37%|███▋      | 406800/1108752 [00:41<01:13, 9577.99 examples/s]\u001b[A\nGenerating train examples...:  37%|███▋      | 407810/1108752 [00:41<01:12, 9730.66 examples/s]\u001b[A\nGenerating train examples...:  37%|███▋      | 408797/1108752 [00:41<01:11, 9770.03 examples/s]\u001b[A\nGenerating train examples...:  37%|███▋      | 409775/1108752 [00:41<01:12, 9633.50 examples/s]\u001b[A\nGenerating train examples...:  37%|███▋      | 410791/1108752 [00:41<01:11, 9788.69 examples/s]\u001b[A\nGenerating train examples...:  37%|███▋      | 411799/1108752 [00:41<01:10, 9872.86 examples/s]\u001b[A\nGenerating train examples...:  37%|███▋      | 412794/1108752 [00:42<01:10, 9895.10 examples/s]\u001b[A\nGenerating train examples...:  37%|███▋      | 413796/1108752 [00:42<01:09, 9930.34 examples/s]\u001b[A\nGenerating train examples...:  37%|███▋      | 414808/1108752 [00:42<01:09, 9984.66 examples/s]\u001b[A\nGenerating train examples...:  38%|███▊      | 415816/1108752 [00:42<01:09, 10010.95 examples/s]\u001b[A\nGenerating train examples...:  38%|███▊      | 416818/1108752 [00:42<01:09, 9989.62 examples/s] \u001b[A\nGenerating train examples...:  38%|███▊      | 417825/1108752 [00:42<01:09, 10013.36 examples/s]\u001b[A\nGenerating train examples...:  38%|███▊      | 418840/1108752 [00:42<01:08, 10052.98 examples/s]\u001b[A\nGenerating train examples...:  38%|███▊      | 419846/1108752 [00:42<01:08, 10051.03 examples/s]\u001b[A\nGenerating train examples...:  38%|███▊      | 420852/1108752 [00:42<01:08, 10019.35 examples/s]\u001b[A\nGenerating train examples...:  38%|███▊      | 421854/1108752 [00:42<01:08, 9989.19 examples/s] \u001b[A\nGenerating train examples...:  38%|███▊      | 422853/1108752 [00:43<01:10, 9710.99 examples/s]\u001b[A\nGenerating train examples...:  38%|███▊      | 423826/1108752 [00:43<01:10, 9647.27 examples/s]\u001b[A\nGenerating train examples...:  38%|███▊      | 424792/1108752 [00:43<01:11, 9543.70 examples/s]\u001b[A\nGenerating train examples...:  38%|███▊      | 425748/1108752 [00:43<01:12, 9473.66 examples/s]\u001b[A\nGenerating train examples...:  38%|███▊      | 426746/1108752 [00:43<01:10, 9619.55 examples/s]\u001b[A\nGenerating train examples...:  39%|███▊      | 427727/1108752 [00:43<01:10, 9672.72 examples/s]\u001b[A\nGenerating train examples...:  39%|███▊      | 428714/1108752 [00:43<01:09, 9730.09 examples/s]\u001b[A\nGenerating train examples...:  39%|███▉      | 429723/1108752 [00:43<01:09, 9835.98 examples/s]\u001b[A\nGenerating train examples...:  39%|███▉      | 430728/1108752 [00:43<01:08, 9899.08 examples/s]\u001b[A\nGenerating train examples...:  39%|███▉      | 431719/1108752 [00:43<01:08, 9901.34 examples/s]\u001b[A\nGenerating train examples...:  39%|███▉      | 432713/1108752 [00:44<01:08, 9910.76 examples/s]\u001b[A\nGenerating train examples...:  39%|███▉      | 433705/1108752 [00:44<01:08, 9912.96 examples/s]\u001b[A\nGenerating train examples...:  39%|███▉      | 434721/1108752 [00:44<01:07, 9985.69 examples/s]\u001b[A\nGenerating train examples...:  39%|███▉      | 435727/1108752 [00:44<01:07, 10007.64 examples/s]\u001b[A\nGenerating train examples...:  39%|███▉      | 436730/1108752 [00:44<01:07, 10012.64 examples/s]\u001b[A\nGenerating train examples...:  39%|███▉      | 437734/1108752 [00:44<01:06, 10019.96 examples/s]\u001b[A\nGenerating train examples...:  40%|███▉      | 438745/1108752 [00:44<01:06, 10045.36 examples/s]\u001b[A\nGenerating train examples...:  40%|███▉      | 439751/1108752 [00:44<01:06, 10047.76 examples/s]\u001b[A\nGenerating train examples...:  40%|███▉      | 440757/1108752 [00:44<01:06, 10050.72 examples/s]\u001b[A\nGenerating train examples...:  40%|███▉      | 441763/1108752 [00:44<01:06, 10052.55 examples/s]\u001b[A\nGenerating train examples...:  40%|███▉      | 442771/1108752 [00:45<01:06, 10060.55 examples/s]\u001b[A\nGenerating train examples...:  40%|████      | 443787/1108752 [00:45<01:05, 10087.40 examples/s]\u001b[A\nGenerating train examples...:  40%|████      | 444799/1108752 [00:45<01:05, 10095.87 examples/s]\u001b[A\nGenerating train examples...:  40%|████      | 445809/1108752 [00:45<01:05, 10077.12 examples/s]\u001b[A\nGenerating train examples...:  40%|████      | 446817/1108752 [00:45<01:05, 10057.96 examples/s]\u001b[A\nGenerating train examples...:  40%|████      | 447823/1108752 [00:45<01:05, 10039.99 examples/s]\u001b[A\nGenerating train examples...:  40%|████      | 448828/1108752 [00:45<01:05, 10003.54 examples/s]\u001b[A\nGenerating train examples...:  41%|████      | 449829/1108752 [00:45<01:05, 9997.00 examples/s] \u001b[A\nGenerating train examples...:  41%|████      | 450829/1108752 [00:45<01:06, 9961.95 examples/s]\u001b[A\nGenerating train examples...:  41%|████      | 451845/1108752 [00:45<01:05, 10020.39 examples/s]\u001b[A\nGenerating train examples...:  41%|████      | 452848/1108752 [00:46<01:05, 10007.14 examples/s]\u001b[A\nGenerating train examples...:  41%|████      | 453849/1108752 [00:46<01:06, 9841.22 examples/s] \u001b[A\nGenerating train examples...:  41%|████      | 454841/1108752 [00:46<01:06, 9861.81 examples/s]\u001b[A\nGenerating train examples...:  41%|████      | 455828/1108752 [00:46<01:08, 9595.21 examples/s]\u001b[A\nGenerating train examples...:  41%|████      | 456790/1108752 [00:46<01:10, 9307.34 examples/s]\u001b[A\nGenerating train examples...:  41%|████▏     | 457724/1108752 [00:46<01:11, 9121.68 examples/s]\u001b[A\nGenerating train examples...:  41%|████▏     | 458639/1108752 [00:46<01:12, 8999.88 examples/s]\u001b[A\nGenerating train examples...:  41%|████▏     | 459541/1108752 [00:46<01:12, 8911.99 examples/s]\u001b[A\nGenerating train examples...:  42%|████▏     | 460463/1108752 [00:46<01:12, 8999.33 examples/s]\u001b[A\nGenerating train examples...:  42%|████▏     | 461392/1108752 [00:47<01:11, 9082.94 examples/s]\u001b[A\nGenerating train examples...:  42%|████▏     | 462308/1108752 [00:47<01:11, 9103.56 examples/s]\u001b[A\nGenerating train examples...:  42%|████▏     | 463291/1108752 [00:47<01:09, 9318.57 examples/s]\u001b[A\nGenerating train examples...:  42%|████▏     | 464257/1108752 [00:47<01:08, 9417.17 examples/s]\u001b[A\nGenerating train examples...:  42%|████▏     | 465207/1108752 [00:47<01:08, 9440.19 examples/s]\u001b[A\nGenerating train examples...:  42%|████▏     | 466175/1108752 [00:47<01:07, 9511.39 examples/s]\u001b[A\nGenerating train examples...:  42%|████▏     | 467167/1108752 [00:47<01:06, 9632.61 examples/s]\u001b[A\nGenerating train examples...:  42%|████▏     | 468131/1108752 [00:47<01:16, 8425.06 examples/s]\u001b[A\nGenerating train examples...:  42%|████▏     | 469133/1108752 [00:47<01:12, 8856.70 examples/s]\u001b[A\nGenerating train examples...:  42%|████▏     | 470146/1108752 [00:47<01:09, 9211.26 examples/s]\u001b[A\nGenerating train examples...:  42%|████▏     | 471142/1108752 [00:48<01:07, 9422.92 examples/s]\u001b[A\nGenerating train examples...:  43%|████▎     | 472135/1108752 [00:48<01:06, 9569.43 examples/s]\u001b[A\nGenerating train examples...:  43%|████▎     | 473136/1108752 [00:48<01:05, 9698.12 examples/s]\u001b[A\nGenerating train examples...:  43%|████▎     | 474134/1108752 [00:48<01:04, 9779.96 examples/s]\u001b[A\nGenerating train examples...:  43%|████▎     | 475138/1108752 [00:48<01:04, 9856.13 examples/s]\u001b[A\nGenerating train examples...:  43%|████▎     | 476138/1108752 [00:48<01:03, 9897.05 examples/s]\u001b[A\nGenerating train examples...:  43%|████▎     | 477131/1108752 [00:48<01:04, 9775.82 examples/s]\u001b[A\nGenerating train examples...:  43%|████▎     | 478138/1108752 [00:48<01:03, 9862.13 examples/s]\u001b[A\nGenerating train examples...:  43%|████▎     | 479155/1108752 [00:48<01:03, 9951.60 examples/s]\u001b[A\nGenerating train examples...:  43%|████▎     | 480165/1108752 [00:48<01:02, 9994.41 examples/s]\u001b[A\nGenerating train examples...:  43%|████▎     | 481168/1108752 [00:49<01:02, 10004.66 examples/s]\u001b[A\nGenerating train examples...:  43%|████▎     | 482170/1108752 [00:49<01:02, 9973.68 examples/s] \u001b[A\nGenerating train examples...:  44%|████▎     | 483168/1108752 [00:49<01:02, 9955.61 examples/s]\u001b[A\nGenerating train examples...:  44%|████▎     | 484165/1108752 [00:49<01:02, 9957.76 examples/s]\u001b[A\nGenerating train examples...:  44%|████▍     | 485165/1108752 [00:49<01:02, 9969.29 examples/s]\u001b[A\nGenerating train examples...:  44%|████▍     | 486188/1108752 [00:49<01:01, 10046.84 examples/s]\u001b[A\nGenerating train examples...:  44%|████▍     | 487193/1108752 [00:49<01:02, 10014.24 examples/s]\u001b[A\nGenerating train examples...:  44%|████▍     | 488195/1108752 [00:49<01:01, 10011.67 examples/s]\u001b[A\nGenerating train examples...:  44%|████▍     | 489200/1108752 [00:49<01:01, 10019.96 examples/s]\u001b[A\nGenerating train examples...:  44%|████▍     | 490215/1108752 [00:49<01:01, 10058.12 examples/s]\u001b[A\nGenerating train examples...:  44%|████▍     | 491221/1108752 [00:50<01:01, 10020.83 examples/s]\u001b[A\nGenerating train examples...:  44%|████▍     | 492224/1108752 [00:50<01:02, 9922.60 examples/s] \u001b[A\nGenerating train examples...:  44%|████▍     | 493224/1108752 [00:50<01:01, 9944.12 examples/s]\u001b[A\nGenerating train examples...:  45%|████▍     | 494232/1108752 [00:50<01:01, 9982.23 examples/s]\u001b[A\nGenerating train examples...:  45%|████▍     | 495239/1108752 [00:50<01:01, 10006.23 examples/s]\u001b[A\nGenerating train examples...:  45%|████▍     | 496248/1108752 [00:50<01:01, 10031.03 examples/s]\u001b[A\nGenerating train examples...:  45%|████▍     | 497252/1108752 [00:50<01:01, 9965.60 examples/s] \u001b[A\nGenerating train examples...:  45%|████▍     | 498252/1108752 [00:50<01:01, 9973.49 examples/s]\u001b[A\nGenerating train examples...:  45%|████▌     | 499251/1108752 [00:50<01:01, 9975.99 examples/s]\u001b[A\nGenerating train examples...:  45%|████▌     | 500261/1108752 [00:50<01:00, 10012.35 examples/s]\u001b[A\nGenerating train examples...:  45%|████▌     | 501265/1108752 [00:51<01:00, 10019.16 examples/s]\u001b[A\nGenerating train examples...:  45%|████▌     | 502267/1108752 [00:51<01:00, 9997.10 examples/s] \u001b[A\nGenerating train examples...:  45%|████▌     | 503274/1108752 [00:51<01:00, 10016.23 examples/s]\u001b[A\nGenerating train examples...:  45%|████▌     | 504276/1108752 [00:51<01:00, 10015.08 examples/s]\u001b[A\nGenerating train examples...:  46%|████▌     | 505278/1108752 [00:51<01:00, 9947.73 examples/s] \u001b[A\nGenerating train examples...:  46%|████▌     | 506282/1108752 [00:51<01:00, 9973.82 examples/s]\u001b[A\nGenerating train examples...:  46%|████▌     | 507280/1108752 [00:51<01:00, 9926.76 examples/s]\u001b[A\nGenerating train examples...:  46%|████▌     | 508284/1108752 [00:51<01:00, 9958.76 examples/s]\u001b[A\nGenerating train examples...:  46%|████▌     | 509293/1108752 [00:51<00:59, 9995.54 examples/s]\u001b[A\nGenerating train examples...:  46%|████▌     | 510314/1108752 [00:51<00:59, 10059.34 examples/s]\u001b[A\nGenerating train examples...:  46%|████▌     | 511326/1108752 [00:52<00:59, 10074.94 examples/s]\u001b[A\nGenerating train examples...:  46%|████▌     | 512334/1108752 [00:52<00:59, 9964.33 examples/s] \u001b[A\nGenerating train examples...:  46%|████▋     | 513339/1108752 [00:52<00:59, 9988.26 examples/s]\u001b[A\nGenerating train examples...:  46%|████▋     | 514342/1108752 [00:52<00:59, 9998.91 examples/s]\u001b[A\nGenerating train examples...:  46%|████▋     | 515343/1108752 [00:52<00:59, 9998.56 examples/s]\u001b[A\nGenerating train examples...:  47%|████▋     | 516343/1108752 [00:52<00:59, 9959.81 examples/s]\u001b[A\nGenerating train examples...:  47%|████▋     | 517340/1108752 [00:52<01:01, 9666.63 examples/s]\u001b[A\nGenerating train examples...:  47%|████▋     | 518336/1108752 [00:52<01:00, 9752.17 examples/s]\u001b[A\nGenerating train examples...:  47%|████▋     | 519341/1108752 [00:52<00:59, 9837.94 examples/s]\u001b[A\nGenerating train examples...:  47%|████▋     | 520326/1108752 [00:53<01:00, 9760.16 examples/s]\u001b[A\nGenerating train examples...:  47%|████▋     | 521312/1108752 [00:53<01:00, 9788.14 examples/s]\u001b[A\nGenerating train examples...:  47%|████▋     | 522292/1108752 [00:53<01:01, 9560.51 examples/s]\u001b[A\nGenerating train examples...:  47%|████▋     | 523250/1108752 [00:53<01:01, 9517.52 examples/s]\u001b[A\nGenerating train examples...:  47%|████▋     | 524203/1108752 [00:53<01:01, 9470.79 examples/s]\u001b[A\nGenerating train examples...:  47%|████▋     | 525151/1108752 [00:53<01:02, 9360.53 examples/s]\u001b[A\nGenerating train examples...:  47%|████▋     | 526088/1108752 [00:53<01:04, 8965.59 examples/s]\u001b[A\nGenerating train examples...:  48%|████▊     | 526988/1108752 [00:53<01:09, 8339.69 examples/s]\u001b[A\nGenerating train examples...:  48%|████▊     | 527876/1108752 [00:53<01:08, 8486.84 examples/s]\u001b[A\nGenerating train examples...:  48%|████▊     | 528792/1108752 [00:53<01:06, 8675.66 examples/s]\u001b[A\nGenerating train examples...:  48%|████▊     | 529712/1108752 [00:54<01:05, 8824.39 examples/s]\u001b[A\nGenerating train examples...:  48%|████▊     | 530650/1108752 [00:54<01:04, 8985.85 examples/s]\u001b[A\nGenerating train examples...:  48%|████▊     | 531574/1108752 [00:54<01:03, 9059.21 examples/s]\u001b[A\nGenerating train examples...:  48%|████▊     | 532552/1108752 [00:54<01:02, 9271.33 examples/s]\u001b[A\nGenerating train examples...:  48%|████▊     | 533482/1108752 [00:54<01:02, 9237.99 examples/s]\u001b[A\nGenerating train examples...:  48%|████▊     | 534470/1108752 [00:54<01:00, 9426.23 examples/s]\u001b[A\nGenerating train examples...:  48%|████▊     | 535415/1108752 [00:54<01:03, 9080.30 examples/s]\u001b[A\nGenerating train examples...:  48%|████▊     | 536327/1108752 [00:54<01:04, 8854.16 examples/s]\u001b[A\nGenerating train examples...:  48%|████▊     | 537332/1108752 [00:54<01:02, 9197.12 examples/s]\u001b[A\nGenerating train examples...:  49%|████▊     | 538335/1108752 [00:54<01:00, 9439.19 examples/s]\u001b[A\nGenerating train examples...:  49%|████▊     | 539336/1108752 [00:55<00:59, 9603.97 examples/s]\u001b[A\nGenerating train examples...:  49%|████▊     | 540328/1108752 [00:55<00:58, 9696.62 examples/s]\u001b[A\nGenerating train examples...:  49%|████▉     | 541325/1108752 [00:55<00:58, 9777.17 examples/s]\u001b[A\nGenerating train examples...:  49%|████▉     | 542325/1108752 [00:55<00:57, 9842.96 examples/s]\u001b[A\nGenerating train examples...:  49%|████▉     | 543319/1108752 [00:55<00:57, 9869.80 examples/s]\u001b[A\nGenerating train examples...:  49%|████▉     | 544315/1108752 [00:55<00:57, 9894.68 examples/s]\u001b[A\nGenerating train examples...:  49%|████▉     | 545306/1108752 [00:55<00:56, 9888.83 examples/s]\u001b[A\nGenerating train examples...:  49%|████▉     | 546312/1108752 [00:55<00:56, 9939.13 examples/s]\u001b[A\nGenerating train examples...:  49%|████▉     | 547311/1108752 [00:55<00:56, 9953.67 examples/s]\u001b[A\nGenerating train examples...:  49%|████▉     | 548307/1108752 [00:55<00:56, 9944.13 examples/s]\u001b[A\nGenerating train examples...:  50%|████▉     | 549309/1108752 [00:56<00:56, 9965.43 examples/s]\u001b[A\nGenerating train examples...:  50%|████▉     | 550306/1108752 [00:56<00:56, 9941.13 examples/s]\u001b[A\nGenerating train examples...:  50%|████▉     | 551302/1108752 [00:56<00:56, 9946.15 examples/s]\u001b[A\nGenerating train examples...:  50%|████▉     | 552303/1108752 [00:56<00:55, 9963.11 examples/s]\u001b[A\nGenerating train examples...:  50%|████▉     | 553300/1108752 [00:56<00:55, 9965.04 examples/s]\u001b[A\nGenerating train examples...:  50%|████▉     | 554297/1108752 [00:56<00:55, 9950.80 examples/s]\u001b[A\nGenerating train examples...:  50%|█████     | 555297/1108752 [00:56<00:55, 9963.72 examples/s]\u001b[A\nGenerating train examples...:  50%|█████     | 556294/1108752 [00:56<00:55, 9957.81 examples/s]\u001b[A\nGenerating train examples...:  50%|█████     | 557290/1108752 [00:56<00:55, 9956.72 examples/s]\u001b[A\nGenerating train examples...:  50%|█████     | 558298/1108752 [00:56<00:55, 9992.22 examples/s]\u001b[A\nGenerating train examples...:  50%|█████     | 559298/1108752 [00:57<00:55, 9989.59 examples/s]\u001b[A\nGenerating train examples...:  51%|█████     | 560297/1108752 [00:57<00:55, 9965.88 examples/s]\u001b[A\nGenerating train examples...:  51%|█████     | 561303/1108752 [00:57<00:54, 9990.01 examples/s]\u001b[A\nGenerating train examples...:  51%|█████     | 562303/1108752 [00:57<00:54, 9983.12 examples/s]\u001b[A\nGenerating train examples...:  51%|█████     | 563302/1108752 [00:57<00:54, 9938.08 examples/s]\u001b[A\nGenerating train examples...:  51%|█████     | 564296/1108752 [00:57<00:54, 9932.68 examples/s]\u001b[A\nGenerating train examples...:  51%|█████     | 565298/1108752 [00:57<00:54, 9958.02 examples/s]\u001b[A\nGenerating train examples...:  51%|█████     | 566296/1108752 [00:57<00:54, 9962.33 examples/s]\u001b[A\nGenerating train examples...:  51%|█████     | 567296/1108752 [00:57<00:54, 9973.01 examples/s]\u001b[A\nGenerating train examples...:  51%|█████▏    | 568298/1108752 [00:57<00:54, 9986.06 examples/s]\u001b[A\nGenerating train examples...:  51%|█████▏    | 569297/1108752 [00:58<00:54, 9956.99 examples/s]\u001b[A\nGenerating train examples...:  51%|█████▏    | 570293/1108752 [00:58<00:54, 9939.53 examples/s]\u001b[A\nGenerating train examples...:  52%|█████▏    | 571296/1108752 [00:58<00:53, 9963.95 examples/s]\u001b[A\nGenerating train examples...:  52%|█████▏    | 572293/1108752 [00:58<00:53, 9960.41 examples/s]\u001b[A\nGenerating train examples...:  52%|█████▏    | 573308/1108752 [00:58<00:53, 10014.95 examples/s]\u001b[A\nGenerating train examples...:  52%|█████▏    | 574310/1108752 [00:58<00:53, 9944.15 examples/s] \u001b[A\nGenerating train examples...:  52%|█████▏    | 575305/1108752 [00:58<00:53, 9891.86 examples/s]\u001b[A\nGenerating train examples...:  52%|█████▏    | 576301/1108752 [00:58<00:53, 9910.38 examples/s]\u001b[A\nGenerating train examples...:  52%|█████▏    | 577307/1108752 [00:58<00:53, 9954.61 examples/s]\u001b[A\nGenerating train examples...:  52%|█████▏    | 578315/1108752 [00:59<00:53, 9991.70 examples/s]\u001b[A\nGenerating train examples...:  52%|█████▏    | 579315/1108752 [00:59<00:53, 9878.55 examples/s]\u001b[A\nGenerating train examples...:  52%|█████▏    | 580304/1108752 [00:59<00:53, 9799.33 examples/s]\u001b[A\nGenerating train examples...:  52%|█████▏    | 581300/1108752 [00:59<00:53, 9845.89 examples/s]\u001b[A\nGenerating train examples...:  53%|█████▎    | 582285/1108752 [00:59<00:53, 9804.49 examples/s]\u001b[A\nGenerating train examples...:  53%|█████▎    | 583266/1108752 [00:59<00:54, 9668.57 examples/s]\u001b[A\nGenerating train examples...:  53%|█████▎    | 584234/1108752 [00:59<00:54, 9616.62 examples/s]\u001b[A\nGenerating train examples...:  53%|█████▎    | 585201/1108752 [00:59<00:54, 9631.42 examples/s]\u001b[A\nGenerating train examples...:  53%|█████▎    | 586165/1108752 [00:59<00:54, 9631.70 examples/s]\u001b[A\nGenerating train examples...:  53%|█████▎    | 587129/1108752 [00:59<00:54, 9609.57 examples/s]\u001b[A\nGenerating train examples...:  53%|█████▎    | 588091/1108752 [01:00<00:54, 9474.21 examples/s]\u001b[A\nGenerating train examples...:  53%|█████▎    | 589039/1108752 [01:00<00:54, 9456.65 examples/s]\u001b[A\nGenerating train examples...:  53%|█████▎    | 589985/1108752 [01:00<00:56, 9254.23 examples/s]\u001b[A\nGenerating train examples...:  53%|█████▎    | 590971/1108752 [01:00<00:54, 9430.56 examples/s]\u001b[A\nGenerating train examples...:  53%|█████▎    | 591971/1108752 [01:00<00:53, 9596.77 examples/s]\u001b[A\nGenerating train examples...:  53%|█████▎    | 592932/1108752 [01:00<00:53, 9584.59 examples/s]\u001b[A\nGenerating train examples...:  54%|█████▎    | 593892/1108752 [01:00<00:53, 9589.02 examples/s]\u001b[A\nGenerating train examples...:  54%|█████▎    | 594879/1108752 [01:00<00:53, 9672.15 examples/s]\u001b[A\nGenerating train examples...:  54%|█████▎    | 595847/1108752 [01:00<00:53, 9657.49 examples/s]\u001b[A\nGenerating train examples...:  54%|█████▍    | 596814/1108752 [01:00<00:53, 9642.55 examples/s]\u001b[A\nGenerating train examples...:  54%|█████▍    | 597817/1108752 [01:01<00:52, 9757.21 examples/s]\u001b[A\nGenerating train examples...:  54%|█████▍    | 598810/1108752 [01:01<00:51, 9806.69 examples/s]\u001b[A\nGenerating train examples...:  54%|█████▍    | 599822/1108752 [01:01<00:51, 9899.89 examples/s]\u001b[A\nGenerating train examples...:  54%|█████▍    | 600813/1108752 [01:01<00:52, 9686.30 examples/s]\u001b[A\nGenerating train examples...:  54%|█████▍    | 601783/1108752 [01:01<00:52, 9671.84 examples/s]\u001b[A\nGenerating train examples...:  54%|█████▍    | 602790/1108752 [01:01<00:51, 9787.71 examples/s]\u001b[A\nGenerating train examples...:  54%|█████▍    | 603770/1108752 [01:01<00:51, 9743.39 examples/s]\u001b[A\nGenerating train examples...:  55%|█████▍    | 604772/1108752 [01:01<00:51, 9825.00 examples/s]\u001b[A\nGenerating train examples...:  55%|█████▍    | 605755/1108752 [01:02<01:31, 5511.63 examples/s]\u001b[A\nGenerating train examples...:  55%|█████▍    | 606742/1108752 [01:02<01:19, 6351.35 examples/s]\u001b[A\nGenerating train examples...:  55%|█████▍    | 607749/1108752 [01:02<01:10, 7153.16 examples/s]\u001b[A\nGenerating train examples...:  55%|█████▍    | 608763/1108752 [01:02<01:03, 7855.86 examples/s]\u001b[A\nGenerating train examples...:  55%|█████▍    | 609773/1108752 [01:02<00:59, 8419.25 examples/s]\u001b[A\nGenerating train examples...:  55%|█████▌    | 610765/1108752 [01:02<00:56, 8814.90 examples/s]\u001b[A\nGenerating train examples...:  55%|█████▌    | 611763/1108752 [01:02<00:54, 9133.02 examples/s]\u001b[A\nGenerating train examples...:  55%|█████▌    | 612760/1108752 [01:02<00:52, 9367.35 examples/s]\u001b[A\nGenerating train examples...:  55%|█████▌    | 613772/1108752 [01:02<00:51, 9582.20 examples/s]\u001b[A\nGenerating train examples...:  55%|█████▌    | 614791/1108752 [01:03<00:50, 9755.80 examples/s]\u001b[A\nGenerating train examples...:  56%|█████▌    | 615789/1108752 [01:03<00:50, 9812.18 examples/s]\u001b[A\nGenerating train examples...:  56%|█████▌    | 616787/1108752 [01:03<00:50, 9646.50 examples/s]\u001b[A\nGenerating train examples...:  56%|█████▌    | 617766/1108752 [01:03<00:50, 9687.15 examples/s]\u001b[A\nGenerating train examples...:  56%|█████▌    | 618775/1108752 [01:03<00:49, 9804.83 examples/s]\u001b[A\nGenerating train examples...:  56%|█████▌    | 619779/1108752 [01:03<00:49, 9871.91 examples/s]\u001b[A\nGenerating train examples...:  56%|█████▌    | 620783/1108752 [01:03<00:49, 9921.18 examples/s]\u001b[A\nGenerating train examples...:  56%|█████▌    | 621779/1108752 [01:03<00:49, 9923.06 examples/s]\u001b[A\nGenerating train examples...:  56%|█████▌    | 622794/1108752 [01:03<00:48, 9989.27 examples/s]\u001b[A\nGenerating train examples...:  56%|█████▋    | 623805/1108752 [01:03<00:48, 10024.37 examples/s]\u001b[A\nGenerating train examples...:  56%|█████▋    | 624821/1108752 [01:04<00:48, 10063.72 examples/s]\u001b[A\nGenerating train examples...:  56%|█████▋    | 625829/1108752 [01:04<00:48, 10003.64 examples/s]\u001b[A\nGenerating train examples...:  57%|█████▋    | 626830/1108752 [01:04<00:48, 9960.60 examples/s] \u001b[A\nGenerating train examples...:  57%|█████▋    | 627838/1108752 [01:04<00:48, 9994.73 examples/s]\u001b[A\nGenerating train examples...:  57%|█████▋    | 628847/1108752 [01:04<00:47, 10021.49 examples/s]\u001b[A\nGenerating train examples...:  57%|█████▋    | 629850/1108752 [01:04<00:47, 10021.20 examples/s]\u001b[A\nGenerating train examples...:  57%|█████▋    | 630853/1108752 [01:04<00:47, 10005.75 examples/s]\u001b[A\nGenerating train examples...:  57%|█████▋    | 631854/1108752 [01:04<00:47, 9984.20 examples/s] \u001b[A\nGenerating train examples...:  57%|█████▋    | 632857/1108752 [01:04<00:47, 9996.27 examples/s]\u001b[A\nGenerating train examples...:  57%|█████▋    | 633879/1108752 [01:04<00:47, 10061.05 examples/s]\u001b[A\nGenerating train examples...:  57%|█████▋    | 634891/1108752 [01:05<00:47, 10076.85 examples/s]\u001b[A\nGenerating train examples...:  57%|█████▋    | 635899/1108752 [01:05<00:46, 10074.52 examples/s]\u001b[A\nGenerating train examples...:  57%|█████▋    | 636913/1108752 [01:05<00:46, 10091.42 examples/s]\u001b[A\nGenerating train examples...:  58%|█████▊    | 637924/1108752 [01:05<00:46, 10096.82 examples/s]\u001b[A\nGenerating train examples...:  58%|█████▊    | 638934/1108752 [01:05<00:46, 10079.54 examples/s]\u001b[A\nGenerating train examples...:  58%|█████▊    | 639942/1108752 [01:05<00:46, 10005.44 examples/s]\u001b[A\nGenerating train examples...:  58%|█████▊    | 640943/1108752 [01:05<00:47, 9942.21 examples/s] \u001b[A\nGenerating train examples...:  58%|█████▊    | 641948/1108752 [01:05<00:46, 9974.09 examples/s]\u001b[A\nGenerating train examples...:  58%|█████▊    | 642946/1108752 [01:05<00:47, 9907.98 examples/s]\u001b[A\nGenerating train examples...:  58%|█████▊    | 643937/1108752 [01:05<00:47, 9887.09 examples/s]\u001b[A\nGenerating train examples...:  58%|█████▊    | 644932/1108752 [01:06<00:46, 9905.25 examples/s]\u001b[A\nGenerating train examples...:  58%|█████▊    | 645937/1108752 [01:06<00:46, 9946.19 examples/s]\u001b[A\nGenerating train examples...:  58%|█████▊    | 646960/1108752 [01:06<00:46, 10029.39 examples/s]\u001b[A\nGenerating train examples...:  58%|█████▊    | 647979/1108752 [01:06<00:45, 10074.88 examples/s]\u001b[A\nGenerating train examples...:  59%|█████▊    | 648987/1108752 [01:06<00:45, 10063.96 examples/s]\u001b[A\nGenerating train examples...:  59%|█████▊    | 649994/1108752 [01:06<00:45, 10009.18 examples/s]\u001b[A\nGenerating train examples...:  59%|█████▊    | 650996/1108752 [01:06<00:46, 9905.98 examples/s] \u001b[A\nGenerating train examples...:  59%|█████▉    | 651987/1108752 [01:06<00:46, 9726.89 examples/s]\u001b[A\nGenerating train examples...:  59%|█████▉    | 652961/1108752 [01:06<00:47, 9593.61 examples/s]\u001b[A\nGenerating train examples...:  59%|█████▉    | 653922/1108752 [01:06<00:48, 9461.67 examples/s]\u001b[A\nGenerating train examples...:  59%|█████▉    | 654869/1108752 [01:07<00:49, 9209.39 examples/s]\u001b[A\nGenerating train examples...:  59%|█████▉    | 655792/1108752 [01:07<00:49, 9115.26 examples/s]\u001b[A\nGenerating train examples...:  59%|█████▉    | 656705/1108752 [01:07<00:50, 9001.03 examples/s]\u001b[A\nGenerating train examples...:  59%|█████▉    | 657606/1108752 [01:07<00:50, 9000.74 examples/s]\u001b[A\nGenerating train examples...:  59%|█████▉    | 658507/1108752 [01:07<00:50, 8970.11 examples/s]\u001b[A\nGenerating train examples...:  59%|█████▉    | 659418/1108752 [01:07<00:49, 9010.30 examples/s]\u001b[A\nGenerating train examples...:  60%|█████▉    | 660360/1108752 [01:07<00:49, 9129.77 examples/s]\u001b[A\nGenerating train examples...:  60%|█████▉    | 661357/1108752 [01:07<00:47, 9377.35 examples/s]\u001b[A\nGenerating train examples...:  60%|█████▉    | 662365/1108752 [01:07<00:46, 9585.50 examples/s]\u001b[A\nGenerating train examples...:  60%|█████▉    | 663360/1108752 [01:07<00:45, 9692.24 examples/s]\u001b[A\nGenerating train examples...:  60%|█████▉    | 664376/1108752 [01:08<00:45, 9830.20 examples/s]\u001b[A\nGenerating train examples...:  60%|██████    | 665360/1108752 [01:08<00:45, 9832.58 examples/s]\u001b[A\nGenerating train examples...:  60%|██████    | 666365/1108752 [01:08<00:44, 9896.50 examples/s]\u001b[A\nGenerating train examples...:  60%|██████    | 667369/1108752 [01:08<00:44, 9938.17 examples/s]\u001b[A\nGenerating train examples...:  60%|██████    | 668378/1108752 [01:08<00:44, 9980.86 examples/s]\u001b[A\nGenerating train examples...:  60%|██████    | 669377/1108752 [01:08<00:44, 9972.37 examples/s]\u001b[A\nGenerating train examples...:  60%|██████    | 670375/1108752 [01:08<00:44, 9958.78 examples/s]\u001b[A\nGenerating train examples...:  61%|██████    | 671384/1108752 [01:08<00:43, 9996.71 examples/s]\u001b[A\nGenerating train examples...:  61%|██████    | 672384/1108752 [01:08<00:43, 9986.89 examples/s]\u001b[A\nGenerating train examples...:  61%|██████    | 673388/1108752 [01:08<00:43, 10000.36 examples/s]\u001b[A\nGenerating train examples...:  61%|██████    | 674398/1108752 [01:09<00:43, 10027.89 examples/s]\u001b[A\nGenerating train examples...:  61%|██████    | 675401/1108752 [01:09<00:43, 10016.14 examples/s]\u001b[A\nGenerating train examples...:  61%|██████    | 676414/1108752 [01:09<00:43, 10048.16 examples/s]\u001b[A\nGenerating train examples...:  61%|██████    | 677430/1108752 [01:09<00:42, 10078.59 examples/s]\u001b[A\nGenerating train examples...:  61%|██████    | 678440/1108752 [01:09<00:42, 10083.35 examples/s]\u001b[A\nGenerating train examples...:  61%|██████▏   | 679452/1108752 [01:09<00:42, 10093.01 examples/s]\u001b[A\nGenerating train examples...:  61%|██████▏   | 680462/1108752 [01:09<00:42, 10072.57 examples/s]\u001b[A\nGenerating train examples...:  61%|██████▏   | 681470/1108752 [01:09<00:42, 10071.13 examples/s]\u001b[A\nGenerating train examples...:  62%|██████▏   | 682478/1108752 [01:09<00:42, 10068.94 examples/s]\u001b[A\nGenerating train examples...:  62%|██████▏   | 683490/1108752 [01:09<00:42, 10083.47 examples/s]\u001b[A\nGenerating train examples...:  62%|██████▏   | 684505/1108752 [01:10<00:42, 10099.88 examples/s]\u001b[A\nGenerating train examples...:  62%|██████▏   | 685515/1108752 [01:10<00:42, 9895.62 examples/s] \u001b[A\nGenerating train examples...:  62%|██████▏   | 686508/1108752 [01:10<00:42, 9904.67 examples/s]\u001b[A\nGenerating train examples...:  62%|██████▏   | 687500/1108752 [01:10<00:42, 9901.35 examples/s]\u001b[A\nGenerating train examples...:  62%|██████▏   | 688491/1108752 [01:10<00:42, 9874.89 examples/s]\u001b[A\nGenerating train examples...:  62%|██████▏   | 689481/1108752 [01:10<00:42, 9881.73 examples/s]\u001b[A\nGenerating train examples...:  62%|██████▏   | 690473/1108752 [01:10<00:42, 9891.71 examples/s]\u001b[A\nGenerating train examples...:  62%|██████▏   | 691484/1108752 [01:10<00:41, 9956.53 examples/s]\u001b[A\nGenerating train examples...:  62%|██████▏   | 692481/1108752 [01:10<00:41, 9958.60 examples/s]\u001b[A\nGenerating train examples...:  63%|██████▎   | 693477/1108752 [01:10<00:42, 9871.96 examples/s]\u001b[A\nGenerating train examples...:  63%|██████▎   | 694465/1108752 [01:11<00:41, 9873.49 examples/s]\u001b[A\nGenerating train examples...:  63%|██████▎   | 695458/1108752 [01:11<00:41, 9887.80 examples/s]\u001b[A\nGenerating train examples...:  63%|██████▎   | 696447/1108752 [01:11<00:41, 9848.03 examples/s]\u001b[A\nGenerating train examples...:  63%|██████▎   | 697432/1108752 [01:11<00:41, 9835.21 examples/s]\u001b[A\nGenerating train examples...:  63%|██████▎   | 698416/1108752 [01:11<00:41, 9773.66 examples/s]\u001b[A\nGenerating train examples...:  63%|██████▎   | 699394/1108752 [01:11<00:41, 9767.77 examples/s]\u001b[A\nGenerating train examples...:  63%|██████▎   | 700371/1108752 [01:11<00:41, 9768.23 examples/s]\u001b[A\nGenerating train examples...:  63%|██████▎   | 701365/1108752 [01:11<00:41, 9817.35 examples/s]\u001b[A\nGenerating train examples...:  63%|██████▎   | 702347/1108752 [01:11<00:41, 9747.99 examples/s]\u001b[A\nGenerating train examples...:  63%|██████▎   | 703322/1108752 [01:11<00:41, 9683.27 examples/s]\u001b[A\nGenerating train examples...:  64%|██████▎   | 704291/1108752 [01:12<00:42, 9627.59 examples/s]\u001b[A\nGenerating train examples...:  64%|██████▎   | 705295/1108752 [01:12<00:41, 9749.56 examples/s]\u001b[A\nGenerating train examples...:  64%|██████▎   | 706305/1108752 [01:12<00:40, 9850.46 examples/s]\u001b[A\nGenerating train examples...:  64%|██████▍   | 707315/1108752 [01:12<00:40, 9922.74 examples/s]\u001b[A\nGenerating train examples...:  64%|██████▍   | 708308/1108752 [01:12<00:40, 9904.98 examples/s]\u001b[A\nGenerating train examples...:  64%|██████▍   | 709300/1108752 [01:12<00:40, 9907.58 examples/s]\u001b[A\nGenerating train examples...:  64%|██████▍   | 710291/1108752 [01:12<00:40, 9898.38 examples/s]\u001b[A\nGenerating train examples...:  64%|██████▍   | 711291/1108752 [01:12<00:40, 9927.02 examples/s]\u001b[A\nGenerating train examples...:  64%|██████▍   | 712286/1108752 [01:12<00:39, 9932.93 examples/s]\u001b[A\nGenerating train examples...:  64%|██████▍   | 713280/1108752 [01:13<00:39, 9923.81 examples/s]\u001b[A\nGenerating train examples...:  64%|██████▍   | 714273/1108752 [01:13<00:39, 9918.05 examples/s]\u001b[A\nGenerating train examples...:  65%|██████▍   | 715265/1108752 [01:13<00:40, 9788.06 examples/s]\u001b[A\nGenerating train examples...:  65%|██████▍   | 716258/1108752 [01:13<00:39, 9828.62 examples/s]\u001b[A\nGenerating train examples...:  65%|██████▍   | 717242/1108752 [01:13<00:41, 9387.18 examples/s]\u001b[A\nGenerating train examples...:  65%|██████▍   | 718185/1108752 [01:13<00:42, 9290.77 examples/s]\u001b[A\nGenerating train examples...:  65%|██████▍   | 719117/1108752 [01:13<00:42, 9083.49 examples/s]\u001b[A\nGenerating train examples...:  65%|██████▍   | 720028/1108752 [01:13<00:42, 9090.51 examples/s]\u001b[A\nGenerating train examples...:  65%|██████▌   | 721003/1108752 [01:13<00:41, 9281.24 examples/s]\u001b[A\nGenerating train examples...:  65%|██████▌   | 721975/1108752 [01:13<00:41, 9408.39 examples/s]\u001b[A\nGenerating train examples...:  65%|██████▌   | 722918/1108752 [01:14<00:41, 9233.61 examples/s]\u001b[A\nGenerating train examples...:  65%|██████▌   | 723843/1108752 [01:14<00:42, 9109.80 examples/s]\u001b[A\nGenerating train examples...:  65%|██████▌   | 724761/1108752 [01:14<00:42, 9128.80 examples/s]\u001b[A\nGenerating train examples...:  65%|██████▌   | 725680/1108752 [01:14<00:41, 9144.79 examples/s]\u001b[A\nGenerating train examples...:  66%|██████▌   | 726596/1108752 [01:14<00:42, 9073.36 examples/s]\u001b[A\nGenerating train examples...:  66%|██████▌   | 727583/1108752 [01:14<00:40, 9307.17 examples/s]\u001b[A\nGenerating train examples...:  66%|██████▌   | 728580/1108752 [01:14<00:40, 9503.28 examples/s]\u001b[A\nGenerating train examples...:  66%|██████▌   | 729577/1108752 [01:14<00:39, 9640.86 examples/s]\u001b[A\nGenerating train examples...:  66%|██████▌   | 730573/1108752 [01:14<00:38, 9735.55 examples/s]\u001b[A\nGenerating train examples...:  66%|██████▌   | 731567/1108752 [01:14<00:38, 9796.24 examples/s]\u001b[A\nGenerating train examples...:  66%|██████▌   | 732547/1108752 [01:15<00:38, 9781.76 examples/s]\u001b[A\nGenerating train examples...:  66%|██████▌   | 733539/1108752 [01:15<00:38, 9820.55 examples/s]\u001b[A\nGenerating train examples...:  66%|██████▌   | 734548/1108752 [01:15<00:37, 9900.23 examples/s]\u001b[A\nGenerating train examples...:  66%|██████▋   | 735556/1108752 [01:15<00:37, 9953.71 examples/s]\u001b[A\nGenerating train examples...:  66%|██████▋   | 736552/1108752 [01:15<00:37, 9873.37 examples/s]\u001b[A\nGenerating train examples...:  67%|██████▋   | 737540/1108752 [01:15<00:37, 9861.40 examples/s]\u001b[A\nGenerating train examples...:  67%|██████▋   | 738530/1108752 [01:15<00:37, 9872.76 examples/s]\u001b[A\nGenerating train examples...:  67%|██████▋   | 739527/1108752 [01:15<00:37, 9900.23 examples/s]\u001b[A\nGenerating train examples...:  67%|██████▋   | 740525/1108752 [01:15<00:37, 9922.98 examples/s]\u001b[A\nGenerating train examples...:  67%|██████▋   | 741523/1108752 [01:15<00:36, 9938.42 examples/s]\u001b[A\nGenerating train examples...:  67%|██████▋   | 742522/1108752 [01:16<00:36, 9951.62 examples/s]\u001b[A\nGenerating train examples...:  67%|██████▋   | 743528/1108752 [01:16<00:36, 9982.25 examples/s]\u001b[A\nGenerating train examples...:  67%|██████▋   | 744528/1108752 [01:16<00:36, 9985.09 examples/s]\u001b[A\nGenerating train examples...:  67%|██████▋   | 745528/1108752 [01:16<00:36, 9987.29 examples/s]\u001b[A\nGenerating train examples...:  67%|██████▋   | 746527/1108752 [01:16<00:36, 9979.69 examples/s]\u001b[A\nGenerating train examples...:  67%|██████▋   | 747528/1108752 [01:16<00:36, 9986.69 examples/s]\u001b[A\nGenerating train examples...:  68%|██████▊   | 748527/1108752 [01:16<00:36, 9944.32 examples/s]\u001b[A\nGenerating train examples...:  68%|██████▊   | 749527/1108752 [01:16<00:36, 9959.10 examples/s]\u001b[A\nGenerating train examples...:  68%|██████▊   | 750524/1108752 [01:16<00:35, 9961.64 examples/s]\u001b[A\nGenerating train examples...:  68%|██████▊   | 751536/1108752 [01:16<00:35, 10006.79 examples/s]\u001b[A\nGenerating train examples...:  68%|██████▊   | 752541/1108752 [01:17<00:35, 10018.59 examples/s]\u001b[A\nGenerating train examples...:  68%|██████▊   | 753543/1108752 [01:17<00:35, 10017.37 examples/s]\u001b[A\nGenerating train examples...:  68%|██████▊   | 754545/1108752 [01:17<00:35, 9944.35 examples/s] \u001b[A\nGenerating train examples...:  68%|██████▊   | 755540/1108752 [01:17<00:36, 9806.98 examples/s]\u001b[A\nGenerating train examples...:  68%|██████▊   | 756522/1108752 [01:17<00:36, 9647.39 examples/s]\u001b[A\nGenerating train examples...:  68%|██████▊   | 757488/1108752 [01:17<00:37, 9423.50 examples/s]\u001b[A\nGenerating train examples...:  68%|██████▊   | 758464/1108752 [01:17<00:36, 9519.90 examples/s]\u001b[A\nGenerating train examples...:  68%|██████▊   | 759457/1108752 [01:17<00:36, 9637.58 examples/s]\u001b[A\nGenerating train examples...:  69%|██████▊   | 760422/1108752 [01:17<00:36, 9573.45 examples/s]\u001b[A\nGenerating train examples...:  69%|██████▊   | 761381/1108752 [01:17<00:36, 9532.23 examples/s]\u001b[A\nGenerating train examples...:  69%|██████▉   | 762335/1108752 [01:18<00:36, 9495.49 examples/s]\u001b[A\nGenerating train examples...:  69%|██████▉   | 763333/1108752 [01:18<00:35, 9637.37 examples/s]\u001b[A\nGenerating train examples...:  69%|██████▉   | 764298/1108752 [01:18<00:35, 9594.81 examples/s]\u001b[A\nGenerating train examples...:  69%|██████▉   | 765306/1108752 [01:18<00:35, 9736.34 examples/s]\u001b[A\nGenerating train examples...:  69%|██████▉   | 766307/1108752 [01:18<00:34, 9817.31 examples/s]\u001b[A\nGenerating train examples...:  69%|██████▉   | 767320/1108752 [01:18<00:34, 9908.00 examples/s]\u001b[A\nGenerating train examples...:  69%|██████▉   | 768329/1108752 [01:18<00:34, 9961.51 examples/s]\u001b[A\nGenerating train examples...:  69%|██████▉   | 769326/1108752 [01:18<00:34, 9961.81 examples/s]\u001b[A\nGenerating train examples...:  69%|██████▉   | 770323/1108752 [01:18<00:33, 9962.29 examples/s]\u001b[A\nGenerating train examples...:  70%|██████▉   | 771320/1108752 [01:18<00:34, 9908.61 examples/s]\u001b[A\nGenerating train examples...:  70%|██████▉   | 772311/1108752 [01:19<00:34, 9763.64 examples/s]\u001b[A\nGenerating train examples...:  70%|██████▉   | 773315/1108752 [01:19<00:34, 9843.27 examples/s]\u001b[A\nGenerating train examples...:  70%|██████▉   | 774308/1108752 [01:19<00:33, 9867.24 examples/s]\u001b[A\nGenerating train examples...:  70%|██████▉   | 775302/1108752 [01:19<00:33, 9887.95 examples/s]\u001b[A\nGenerating train examples...:  70%|███████   | 776296/1108752 [01:19<00:33, 9902.25 examples/s]\u001b[A\nGenerating train examples...:  70%|███████   | 777294/1108752 [01:19<00:33, 9924.17 examples/s]\u001b[A\nGenerating train examples...:  70%|███████   | 778291/1108752 [01:19<00:33, 9936.45 examples/s]\u001b[A\nGenerating train examples...:  70%|███████   | 779290/1108752 [01:19<00:33, 9950.55 examples/s]\u001b[A\nGenerating train examples...:  70%|███████   | 780286/1108752 [01:19<00:33, 9867.26 examples/s]\u001b[A\nGenerating train examples...:  70%|███████   | 781273/1108752 [01:19<00:33, 9843.33 examples/s]\u001b[A\nGenerating train examples...:  71%|███████   | 782258/1108752 [01:20<00:34, 9512.22 examples/s]\u001b[A\nGenerating train examples...:  71%|███████   | 783212/1108752 [01:20<00:35, 9281.78 examples/s]\u001b[A\nGenerating train examples...:  71%|███████   | 784143/1108752 [01:20<00:35, 9031.20 examples/s]\u001b[A\nGenerating train examples...:  71%|███████   | 785049/1108752 [01:20<00:36, 8991.23 examples/s]\u001b[A\nGenerating train examples...:  71%|███████   | 785950/1108752 [01:20<00:36, 8807.43 examples/s]\u001b[A\nGenerating train examples...:  71%|███████   | 786859/1108752 [01:20<00:36, 8886.66 examples/s]\u001b[A\nGenerating train examples...:  71%|███████   | 787807/1108752 [01:20<00:35, 9058.79 examples/s]\u001b[A\nGenerating train examples...:  71%|███████   | 788715/1108752 [01:20<00:35, 8895.09 examples/s]\u001b[A\nGenerating train examples...:  71%|███████   | 789606/1108752 [01:20<00:35, 8871.46 examples/s]\u001b[A\nGenerating train examples...:  71%|███████▏  | 790495/1108752 [01:21<00:35, 8860.53 examples/s]\u001b[A\nGenerating train examples...:  71%|███████▏  | 791396/1108752 [01:21<00:35, 8902.90 examples/s]\u001b[A\nGenerating train examples...:  71%|███████▏  | 792326/1108752 [01:21<00:35, 9019.20 examples/s]\u001b[A\nGenerating train examples...:  72%|███████▏  | 793311/1108752 [01:21<00:34, 9264.43 examples/s]\u001b[A\nGenerating train examples...:  72%|███████▏  | 794310/1108752 [01:21<00:33, 9479.14 examples/s]\u001b[A\nGenerating train examples...:  72%|███████▏  | 795315/1108752 [01:21<00:32, 9646.71 examples/s]\u001b[A\nGenerating train examples...:  72%|███████▏  | 796317/1108752 [01:21<00:32, 9757.36 examples/s]\u001b[A\nGenerating train examples...:  72%|███████▏  | 797316/1108752 [01:21<00:31, 9826.75 examples/s]\u001b[A\nGenerating train examples...:  72%|███████▏  | 798326/1108752 [01:21<00:31, 9905.94 examples/s]\u001b[A\nGenerating train examples...:  72%|███████▏  | 799330/1108752 [01:21<00:31, 9945.06 examples/s]\u001b[A\nGenerating train examples...:  72%|███████▏  | 800325/1108752 [01:22<00:31, 9870.58 examples/s]\u001b[A\nGenerating train examples...:  72%|███████▏  | 801313/1108752 [01:22<00:31, 9868.32 examples/s]\u001b[A\nGenerating train examples...:  72%|███████▏  | 802317/1108752 [01:22<00:30, 9919.36 examples/s]\u001b[A\nGenerating train examples...:  72%|███████▏  | 803319/1108752 [01:22<00:30, 9947.28 examples/s]\u001b[A\nGenerating train examples...:  73%|███████▎  | 804326/1108752 [01:22<00:30, 9983.17 examples/s]\u001b[A\nGenerating train examples...:  73%|███████▎  | 805325/1108752 [01:22<00:30, 9978.14 examples/s]\u001b[A\nGenerating train examples...:  73%|███████▎  | 806335/1108752 [01:22<00:30, 10014.30 examples/s]\u001b[A\nGenerating train examples...:  73%|███████▎  | 807343/1108752 [01:22<00:30, 10030.96 examples/s]\u001b[A\nGenerating train examples...:  73%|███████▎  | 808347/1108752 [01:22<00:29, 10020.99 examples/s]\u001b[A\nGenerating train examples...:  73%|███████▎  | 809350/1108752 [01:22<00:29, 10016.48 examples/s]\u001b[A\nGenerating train examples...:  73%|███████▎  | 810360/1108752 [01:23<00:29, 10040.07 examples/s]\u001b[A\nGenerating train examples...:  73%|███████▎  | 811365/1108752 [01:23<00:29, 10027.03 examples/s]\u001b[A\nGenerating train examples...:  73%|███████▎  | 812368/1108752 [01:23<00:29, 9981.43 examples/s] \u001b[A\nGenerating train examples...:  73%|███████▎  | 813367/1108752 [01:23<00:29, 9860.21 examples/s]\u001b[A\nGenerating train examples...:  73%|███████▎  | 814354/1108752 [01:23<00:30, 9801.30 examples/s]\u001b[A\nGenerating train examples...:  74%|███████▎  | 815335/1108752 [01:23<00:30, 9746.28 examples/s]\u001b[A\nGenerating train examples...:  74%|███████▎  | 816310/1108752 [01:23<00:30, 9744.27 examples/s]\u001b[A\nGenerating train examples...:  74%|███████▎  | 817311/1108752 [01:23<00:29, 9821.53 examples/s]\u001b[A\nGenerating train examples...:  74%|███████▍  | 818294/1108752 [01:23<00:29, 9771.43 examples/s]\u001b[A\nGenerating train examples...:  74%|███████▍  | 819272/1108752 [01:23<00:29, 9650.33 examples/s]\u001b[A\nGenerating train examples...:  74%|███████▍  | 820238/1108752 [01:24<00:30, 9616.92 examples/s]\u001b[A\nGenerating train examples...:  74%|███████▍  | 821234/1108752 [01:24<00:29, 9717.57 examples/s]\u001b[A\nGenerating train examples...:  74%|███████▍  | 822239/1108752 [01:24<00:29, 9815.43 examples/s]\u001b[A\nGenerating train examples...:  74%|███████▍  | 823246/1108752 [01:24<00:28, 9889.49 examples/s]\u001b[A\nGenerating train examples...:  74%|███████▍  | 824253/1108752 [01:24<00:28, 9941.42 examples/s]\u001b[A\nGenerating train examples...:  74%|███████▍  | 825261/1108752 [01:24<00:28, 9982.45 examples/s]\u001b[A\nGenerating train examples...:  75%|███████▍  | 826263/1108752 [01:24<00:28, 9993.26 examples/s]\u001b[A\nGenerating train examples...:  75%|███████▍  | 827270/1108752 [01:24<00:28, 10016.06 examples/s]\u001b[A\nGenerating train examples...:  75%|███████▍  | 828276/1108752 [01:24<00:27, 10027.74 examples/s]\u001b[A\nGenerating train examples...:  75%|███████▍  | 829291/1108752 [01:24<00:27, 10062.31 examples/s]\u001b[A\nGenerating train examples...:  75%|███████▍  | 830298/1108752 [01:25<00:27, 10055.91 examples/s]\u001b[A\nGenerating train examples...:  75%|███████▍  | 831304/1108752 [01:25<00:27, 9924.25 examples/s] \u001b[A\nGenerating train examples...:  75%|███████▌  | 832297/1108752 [01:25<00:27, 9883.56 examples/s]\u001b[A\nGenerating train examples...:  75%|███████▌  | 833300/1108752 [01:25<00:27, 9924.44 examples/s]\u001b[A\nGenerating train examples...:  75%|███████▌  | 834307/1108752 [01:25<00:27, 9966.91 examples/s]\u001b[A\nGenerating train examples...:  75%|███████▌  | 835314/1108752 [01:25<00:27, 9995.29 examples/s]\u001b[A\nGenerating train examples...:  75%|███████▌  | 836317/1108752 [01:25<00:27, 10003.31 examples/s]\u001b[A\nGenerating train examples...:  76%|███████▌  | 837322/1108752 [01:25<00:27, 10016.44 examples/s]\u001b[A\nGenerating train examples...:  76%|███████▌  | 838324/1108752 [01:25<00:26, 10016.02 examples/s]\u001b[A\nGenerating train examples...:  76%|███████▌  | 839328/1108752 [01:25<00:26, 10020.73 examples/s]\u001b[A\nGenerating train examples...:  76%|███████▌  | 840331/1108752 [01:26<00:26, 10006.78 examples/s]\u001b[A\nGenerating train examples...:  76%|███████▌  | 841332/1108752 [01:26<00:26, 9918.46 examples/s] \u001b[A\nGenerating train examples...:  76%|███████▌  | 842325/1108752 [01:26<00:26, 9907.07 examples/s]\u001b[A\nGenerating train examples...:  76%|███████▌  | 843338/1108752 [01:26<00:26, 9973.37 examples/s]\u001b[A\nGenerating train examples...:  76%|███████▌  | 844336/1108752 [01:26<00:26, 9959.88 examples/s]\u001b[A\nGenerating train examples...:  76%|███████▌  | 845333/1108752 [01:26<00:26, 9950.66 examples/s]\u001b[A\nGenerating train examples...:  76%|███████▋  | 846329/1108752 [01:26<00:26, 9840.50 examples/s]\u001b[A\nGenerating train examples...:  76%|███████▋  | 847314/1108752 [01:26<00:26, 9836.91 examples/s]\u001b[A\nGenerating train examples...:  77%|███████▋  | 848298/1108752 [01:26<00:27, 9576.08 examples/s]\u001b[A\nGenerating train examples...:  77%|███████▋  | 849258/1108752 [01:26<00:27, 9382.49 examples/s]\u001b[A\nGenerating train examples...:  77%|███████▋  | 850198/1108752 [01:27<00:28, 9203.98 examples/s]\u001b[A\nGenerating train examples...:  77%|███████▋  | 851120/1108752 [01:27<00:28, 8895.59 examples/s]\u001b[A\nGenerating train examples...:  77%|███████▋  | 852012/1108752 [01:27<00:28, 8892.31 examples/s]\u001b[A\nGenerating train examples...:  77%|███████▋  | 852938/1108752 [01:27<00:28, 8996.41 examples/s]\u001b[A\nGenerating train examples...:  77%|███████▋  | 853865/1108752 [01:27<00:28, 9073.86 examples/s]\u001b[A\nGenerating train examples...:  77%|███████▋  | 854774/1108752 [01:27<00:28, 8984.05 examples/s]\u001b[A\nGenerating train examples...:  77%|███████▋  | 855674/1108752 [01:27<00:28, 8917.18 examples/s]\u001b[A\nGenerating train examples...:  77%|███████▋  | 856567/1108752 [01:27<00:28, 8888.70 examples/s]\u001b[A\nGenerating train examples...:  77%|███████▋  | 857460/1108752 [01:27<00:28, 8900.13 examples/s]\u001b[A\nGenerating train examples...:  77%|███████▋  | 858415/1108752 [01:28<00:27, 9092.27 examples/s]\u001b[A\nGenerating train examples...:  78%|███████▊  | 859417/1108752 [01:28<00:26, 9365.80 examples/s]\u001b[A\nGenerating train examples...:  78%|███████▊  | 860419/1108752 [01:28<00:25, 9558.58 examples/s]\u001b[A\nGenerating train examples...:  78%|███████▊  | 861421/1108752 [01:28<00:25, 9696.02 examples/s]\u001b[A\nGenerating train examples...:  78%|███████▊  | 862421/1108752 [01:28<00:25, 9785.77 examples/s]\u001b[A\nGenerating train examples...:  78%|███████▊  | 863428/1108752 [01:28<00:24, 9870.49 examples/s]\u001b[A\nGenerating train examples...:  78%|███████▊  | 864431/1108752 [01:28<00:24, 9917.89 examples/s]\u001b[A\nGenerating train examples...:  78%|███████▊  | 865434/1108752 [01:28<00:24, 9949.15 examples/s]\u001b[A\nGenerating train examples...:  78%|███████▊  | 866434/1108752 [01:28<00:24, 9962.75 examples/s]\u001b[A\nGenerating train examples...:  78%|███████▊  | 867439/1108752 [01:28<00:24, 9988.27 examples/s]\u001b[A\nGenerating train examples...:  78%|███████▊  | 868444/1108752 [01:29<00:24, 10004.51 examples/s]\u001b[A\nGenerating train examples...:  78%|███████▊  | 869445/1108752 [01:29<00:24, 9923.18 examples/s] \u001b[A\nGenerating train examples...:  79%|███████▊  | 870438/1108752 [01:29<00:24, 9888.59 examples/s]\u001b[A\nGenerating train examples...:  79%|███████▊  | 871427/1108752 [01:29<00:24, 9761.94 examples/s]\u001b[A\nGenerating train examples...:  79%|███████▊  | 872404/1108752 [01:29<00:24, 9722.37 examples/s]\u001b[A\nGenerating train examples...:  79%|███████▉  | 873377/1108752 [01:29<00:24, 9652.25 examples/s]\u001b[A\nGenerating train examples...:  79%|███████▉  | 874353/1108752 [01:29<00:24, 9682.99 examples/s]\u001b[A\nGenerating train examples...:  79%|███████▉  | 875322/1108752 [01:29<00:24, 9661.83 examples/s]\u001b[A\nGenerating train examples...:  79%|███████▉  | 876290/1108752 [01:29<00:24, 9665.77 examples/s]\u001b[A\nGenerating train examples...:  79%|███████▉  | 877277/1108752 [01:29<00:23, 9725.09 examples/s]\u001b[A\nGenerating train examples...:  79%|███████▉  | 878269/1108752 [01:30<00:23, 9782.35 examples/s]\u001b[A\nGenerating train examples...:  79%|███████▉  | 879248/1108752 [01:30<00:23, 9724.78 examples/s]\u001b[A\nGenerating train examples...:  79%|███████▉  | 880251/1108752 [01:30<00:23, 9813.34 examples/s]\u001b[A\nGenerating train examples...:  79%|███████▉  | 881246/1108752 [01:30<00:23, 9850.55 examples/s]\u001b[A\nGenerating train examples...:  80%|███████▉  | 882251/1108752 [01:30<00:22, 9909.74 examples/s]\u001b[A\nGenerating train examples...:  80%|███████▉  | 883255/1108752 [01:30<00:22, 9948.42 examples/s]\u001b[A\nGenerating train examples...:  80%|███████▉  | 884250/1108752 [01:30<00:22, 9927.78 examples/s]\u001b[A\nGenerating train examples...:  80%|███████▉  | 885245/1108752 [01:30<00:22, 9933.93 examples/s]\u001b[A\nGenerating train examples...:  80%|███████▉  | 886251/1108752 [01:30<00:22, 9969.47 examples/s]\u001b[A\nGenerating train examples...:  80%|████████  | 887259/1108752 [01:30<00:22, 10000.42 examples/s]\u001b[A\nGenerating train examples...:  80%|████████  | 888261/1108752 [01:31<00:22, 10005.15 examples/s]\u001b[A\nGenerating train examples...:  80%|████████  | 889262/1108752 [01:31<00:21, 9988.15 examples/s] \u001b[A\nGenerating train examples...:  80%|████████  | 890263/1108752 [01:31<00:21, 9994.05 examples/s]\u001b[A\nGenerating train examples...:  80%|████████  | 891271/1108752 [01:31<00:21, 10018.95 examples/s]\u001b[A\nGenerating train examples...:  80%|████████  | 892273/1108752 [01:31<00:21, 9996.60 examples/s] \u001b[A\nGenerating train examples...:  81%|████████  | 893273/1108752 [01:31<00:21, 9974.49 examples/s]\u001b[A\nGenerating train examples...:  81%|████████  | 894271/1108752 [01:31<00:21, 9951.74 examples/s]\u001b[A\nGenerating train examples...:  81%|████████  | 895267/1108752 [01:31<00:21, 9950.24 examples/s]\u001b[A\nGenerating train examples...:  81%|████████  | 896263/1108752 [01:31<00:21, 9940.97 examples/s]\u001b[A\nGenerating train examples...:  81%|████████  | 897279/1108752 [01:31<00:21, 10006.03 examples/s]\u001b[A\nGenerating train examples...:  81%|████████  | 898280/1108752 [01:32<00:21, 10004.92 examples/s]\u001b[A\nGenerating train examples...:  81%|████████  | 899281/1108752 [01:32<00:21, 9946.61 examples/s] \u001b[A\nGenerating train examples...:  81%|████████  | 900288/1108752 [01:32<00:20, 9983.00 examples/s]\u001b[A\nGenerating train examples...:  81%|████████▏ | 901307/1108752 [01:32<00:20, 10043.73 examples/s]\u001b[A\nGenerating train examples...:  81%|████████▏ | 902312/1108752 [01:32<00:20, 10022.72 examples/s]\u001b[A\nGenerating train examples...:  81%|████████▏ | 903317/1108752 [01:32<00:20, 10028.35 examples/s]\u001b[A\nGenerating train examples...:  82%|████████▏ | 904320/1108752 [01:32<00:20, 10021.91 examples/s]\u001b[A\nGenerating train examples...:  82%|████████▏ | 905324/1108752 [01:32<00:20, 10024.97 examples/s]\u001b[A\nGenerating train examples...:  82%|████████▏ | 906337/1108752 [01:32<00:20, 10054.56 examples/s]\u001b[A\nGenerating train examples...:  82%|████████▏ | 907343/1108752 [01:32<00:20, 10053.42 examples/s]\u001b[A\nGenerating train examples...:  82%|████████▏ | 908358/1108752 [01:33<00:19, 10081.63 examples/s]\u001b[A\nGenerating train examples...:  82%|████████▏ | 909367/1108752 [01:33<00:19, 10019.40 examples/s]\u001b[A\nGenerating train examples...:  82%|████████▏ | 910395/1108752 [01:33<00:19, 10096.33 examples/s]\u001b[A\nGenerating train examples...:  82%|████████▏ | 911405/1108752 [01:33<00:19, 10084.73 examples/s]\u001b[A\nGenerating train examples...:  82%|████████▏ | 912414/1108752 [01:33<00:19, 10038.41 examples/s]\u001b[A\nGenerating train examples...:  82%|████████▏ | 913421/1108752 [01:33<00:19, 10046.82 examples/s]\u001b[A\nGenerating train examples...:  82%|████████▏ | 914426/1108752 [01:33<00:19, 9866.38 examples/s] \u001b[A\nGenerating train examples...:  83%|████████▎ | 915414/1108752 [01:33<00:19, 9829.25 examples/s]\u001b[A\nGenerating train examples...:  83%|████████▎ | 916398/1108752 [01:33<00:19, 9790.19 examples/s]\u001b[A\nGenerating train examples...:  83%|████████▎ | 917378/1108752 [01:33<00:19, 9719.44 examples/s]\u001b[A\nGenerating train examples...:  83%|████████▎ | 918369/1108752 [01:34<00:19, 9774.02 examples/s]\u001b[A\nGenerating train examples...:  83%|████████▎ | 919347/1108752 [01:34<00:19, 9565.48 examples/s]\u001b[A\nGenerating train examples...:  83%|████████▎ | 920305/1108752 [01:34<00:20, 9412.19 examples/s]\u001b[A\nGenerating train examples...:  83%|████████▎ | 921248/1108752 [01:34<00:20, 9283.43 examples/s]\u001b[A\nGenerating train examples...:  83%|████████▎ | 922199/1108752 [01:34<00:19, 9347.69 examples/s]\u001b[A\nGenerating train examples...:  83%|████████▎ | 923145/1108752 [01:34<00:19, 9379.64 examples/s]\u001b[A\nGenerating train examples...:  83%|████████▎ | 924084/1108752 [01:34<00:19, 9365.16 examples/s]\u001b[A\nGenerating train examples...:  83%|████████▎ | 925087/1108752 [01:34<00:19, 9561.48 examples/s]\u001b[A\nGenerating train examples...:  84%|████████▎ | 926092/1108752 [01:34<00:18, 9704.83 examples/s]\u001b[A\nGenerating train examples...:  84%|████████▎ | 927096/1108752 [01:34<00:18, 9804.63 examples/s]\u001b[A\nGenerating train examples...:  84%|████████▎ | 928091/1108752 [01:35<00:18, 9846.15 examples/s]\u001b[A\nGenerating train examples...:  84%|████████▍ | 929080/1108752 [01:35<00:18, 9859.07 examples/s]\u001b[A\nGenerating train examples...:  84%|████████▍ | 930081/1108752 [01:35<00:18, 9902.38 examples/s]\u001b[A\nGenerating train examples...:  84%|████████▍ | 931074/1108752 [01:35<00:17, 9909.15 examples/s]\u001b[A\nGenerating train examples...:  84%|████████▍ | 932066/1108752 [01:35<00:17, 9910.90 examples/s]\u001b[A\nGenerating train examples...:  84%|████████▍ | 933058/1108752 [01:35<00:17, 9784.52 examples/s]\u001b[A\nGenerating train examples...:  84%|████████▍ | 934061/1108752 [01:35<00:17, 9855.50 examples/s]\u001b[A\nGenerating train examples...:  84%|████████▍ | 935065/1108752 [01:35<00:17, 9909.12 examples/s]\u001b[A\nGenerating train examples...:  84%|████████▍ | 936058/1108752 [01:35<00:17, 9912.75 examples/s]\u001b[A\nGenerating train examples...:  85%|████████▍ | 937050/1108752 [01:35<00:17, 9874.21 examples/s]\u001b[A\nGenerating train examples...:  85%|████████▍ | 938047/1108752 [01:36<00:17, 9900.58 examples/s]\u001b[A\nGenerating train examples...:  85%|████████▍ | 939038/1108752 [01:36<00:17, 9779.35 examples/s]\u001b[A\nGenerating train examples...:  85%|████████▍ | 940017/1108752 [01:36<00:17, 9771.61 examples/s]\u001b[A\nGenerating train examples...:  85%|████████▍ | 941008/1108752 [01:36<00:17, 9812.21 examples/s]\u001b[A\nGenerating train examples...:  85%|████████▍ | 942009/1108752 [01:36<00:16, 9870.19 examples/s]\u001b[A\nGenerating train examples...:  85%|████████▌ | 943014/1108752 [01:36<00:16, 9921.81 examples/s]\u001b[A\nGenerating train examples...:  85%|████████▌ | 944009/1108752 [01:36<00:16, 9930.02 examples/s]\u001b[A\nGenerating train examples...:  85%|████████▌ | 945015/1108752 [01:36<00:16, 9967.39 examples/s]\u001b[A\nGenerating train examples...:  85%|████████▌ | 946023/1108752 [01:36<00:16, 9999.23 examples/s]\u001b[A\nGenerating train examples...:  85%|████████▌ | 947023/1108752 [01:36<00:16, 9994.76 examples/s]\u001b[A\nGenerating train examples...:  86%|████████▌ | 948023/1108752 [01:37<00:16, 9987.88 examples/s]\u001b[A\nGenerating train examples...:  86%|████████▌ | 949032/1108752 [01:37<00:15, 10018.38 examples/s]\u001b[A\nGenerating train examples...:  86%|████████▌ | 950042/1108752 [01:37<00:15, 10040.87 examples/s]\u001b[A\nGenerating train examples...:  86%|████████▌ | 951051/1108752 [01:37<00:15, 10054.76 examples/s]\u001b[A\nGenerating train examples...:  86%|████████▌ | 952057/1108752 [01:37<00:15, 10019.03 examples/s]\u001b[A\nGenerating train examples...:  86%|████████▌ | 953059/1108752 [01:37<00:15, 10014.72 examples/s]\u001b[A\nGenerating train examples...:  86%|████████▌ | 954061/1108752 [01:37<00:15, 10008.59 examples/s]\u001b[A\nGenerating train examples...:  86%|████████▌ | 955068/1108752 [01:37<00:15, 10025.31 examples/s]\u001b[A\nGenerating train examples...:  86%|████████▌ | 956071/1108752 [01:37<00:15, 10017.25 examples/s]\u001b[A\nGenerating train examples...:  86%|████████▋ | 957082/1108752 [01:37<00:15, 10042.25 examples/s]\u001b[A\nGenerating train examples...:  86%|████████▋ | 958087/1108752 [01:38<00:15, 10028.34 examples/s]\u001b[A\nGenerating train examples...:  87%|████████▋ | 959090/1108752 [01:38<00:14, 10022.40 examples/s]\u001b[A\nGenerating train examples...:  87%|████████▋ | 960100/1108752 [01:38<00:14, 10044.11 examples/s]\u001b[A\nGenerating train examples...:  87%|████████▋ | 961118/1108752 [01:38<00:14, 10082.29 examples/s]\u001b[A\nGenerating train examples...:  87%|████████▋ | 962127/1108752 [01:38<00:14, 10042.22 examples/s]\u001b[A\nGenerating train examples...:  87%|████████▋ | 963133/1108752 [01:38<00:14, 10047.16 examples/s]\u001b[A\nGenerating train examples...:  87%|████████▋ | 964138/1108752 [01:38<00:14, 9928.06 examples/s] \u001b[A\nGenerating train examples...:  87%|████████▋ | 965132/1108752 [01:38<00:14, 9927.82 examples/s]\u001b[A\nGenerating train examples...:  87%|████████▋ | 966126/1108752 [01:38<00:14, 9929.58 examples/s]\u001b[A\nGenerating train examples...:  87%|████████▋ | 967120/1108752 [01:39<00:14, 9814.21 examples/s]\u001b[A\nGenerating train examples...:  87%|████████▋ | 968102/1108752 [01:39<00:14, 9811.83 examples/s]\u001b[A\nGenerating train examples...:  87%|████████▋ | 969105/1108752 [01:39<00:14, 9873.82 examples/s]\u001b[A\nGenerating train examples...:  87%|████████▋ | 970116/1108752 [01:39<00:13, 9943.23 examples/s]\u001b[A\nGenerating train examples...:  88%|████████▊ | 971129/1108752 [01:39<00:13, 9997.37 examples/s]\u001b[A\nGenerating train examples...:  88%|████████▊ | 972129/1108752 [01:39<00:13, 9990.17 examples/s]\u001b[A\nGenerating train examples...:  88%|████████▊ | 973137/1108752 [01:39<00:13, 10013.98 examples/s]\u001b[A\nGenerating train examples...:  88%|████████▊ | 974141/1108752 [01:39<00:13, 10021.23 examples/s]\u001b[A\nGenerating train examples...:  88%|████████▊ | 975150/1108752 [01:39<00:13, 10040.16 examples/s]\u001b[A\nGenerating train examples...:  88%|████████▊ | 976165/1108752 [01:39<00:13, 10072.68 examples/s]\u001b[A\nGenerating train examples...:  88%|████████▊ | 977176/1108752 [01:40<00:13, 10082.63 examples/s]\u001b[A\nGenerating train examples...:  88%|████████▊ | 978185/1108752 [01:40<00:13, 10034.38 examples/s]\u001b[A\nGenerating train examples...:  88%|████████▊ | 979189/1108752 [01:40<00:12, 9973.10 examples/s] \u001b[A\nGenerating train examples...:  88%|████████▊ | 980195/1108752 [01:40<00:12, 9997.01 examples/s]\u001b[A\nGenerating train examples...:  88%|████████▊ | 981195/1108752 [01:40<00:13, 9739.36 examples/s]\u001b[A\nGenerating train examples...:  89%|████████▊ | 982179/1108752 [01:40<00:12, 9768.24 examples/s]\u001b[A\nGenerating train examples...:  89%|████████▊ | 983157/1108752 [01:40<00:13, 9623.95 examples/s]\u001b[A\nGenerating train examples...:  89%|████████▉ | 984121/1108752 [01:40<00:13, 9450.16 examples/s]\u001b[A\nGenerating train examples...:  89%|████████▉ | 985071/1108752 [01:40<00:13, 9463.95 examples/s]\u001b[A\nGenerating train examples...:  89%|████████▉ | 986032/1108752 [01:40<00:12, 9504.88 examples/s]\u001b[A\nGenerating train examples...:  89%|████████▉ | 986994/1108752 [01:41<00:12, 9538.83 examples/s]\u001b[A\nGenerating train examples...:  89%|████████▉ | 987949/1108752 [01:41<00:12, 9478.02 examples/s]\u001b[A\nGenerating train examples...:  89%|████████▉ | 988943/1108752 [01:41<00:12, 9612.97 examples/s]\u001b[A\nGenerating train examples...:  89%|████████▉ | 989905/1108752 [01:41<00:12, 9397.93 examples/s]\u001b[A\nGenerating train examples...:  89%|████████▉ | 990847/1108752 [01:41<00:12, 9284.33 examples/s]\u001b[A\nGenerating train examples...:  89%|████████▉ | 991820/1108752 [01:41<00:12, 9413.38 examples/s]\u001b[A\nGenerating train examples...:  90%|████████▉ | 992763/1108752 [01:41<00:12, 9392.14 examples/s]\u001b[A\nGenerating train examples...:  90%|████████▉ | 993723/1108752 [01:41<00:12, 9451.28 examples/s]\u001b[A\nGenerating train examples...:  90%|████████▉ | 994669/1108752 [01:41<00:12, 9347.73 examples/s]\u001b[A\nGenerating train examples...:  90%|████████▉ | 995658/1108752 [01:41<00:11, 9505.41 examples/s]\u001b[A\nGenerating train examples...:  90%|████████▉ | 996636/1108752 [01:42<00:11, 9585.34 examples/s]\u001b[A\nGenerating train examples...:  90%|████████▉ | 997612/1108752 [01:42<00:11, 9636.07 examples/s]\u001b[A\nGenerating train examples...:  90%|█████████ | 998612/1108752 [01:42<00:11, 9742.94 examples/s]\u001b[A\nGenerating train examples...:  90%|█████████ | 999621/1108752 [01:42<00:11, 9844.10 examples/s]\u001b[A\nGenerating train examples...:  90%|█████████ | 1000630/1108752 [01:42<00:10, 9915.47 examples/s]\u001b[A\nGenerating train examples...:  90%|█████████ | 1001646/1108752 [01:42<00:10, 9988.13 examples/s]\u001b[A\nGenerating train examples...:  90%|█████████ | 1002660/1108752 [01:42<00:10, 10031.19 examples/s]\u001b[A\nGenerating train examples...:  91%|█████████ | 1003664/1108752 [01:42<00:10, 10029.09 examples/s]\u001b[A\nGenerating train examples...:  91%|█████████ | 1004681/1108752 [01:42<00:10, 10070.80 examples/s]\u001b[A\nGenerating train examples...:  91%|█████████ | 1005702/1108752 [01:42<00:10, 10112.05 examples/s]\u001b[A\nGenerating train examples...:  91%|█████████ | 1006723/1108752 [01:43<00:10, 10140.64 examples/s]\u001b[A\nGenerating train examples...:  91%|█████████ | 1007738/1108752 [01:43<00:09, 10107.12 examples/s]\u001b[A\nGenerating train examples...:  91%|█████████ | 1008761/1108752 [01:43<00:09, 10141.91 examples/s]\u001b[A\nGenerating train examples...:  91%|█████████ | 1009776/1108752 [01:43<00:09, 10103.79 examples/s]\u001b[A\nGenerating train examples...:  91%|█████████ | 1010787/1108752 [01:43<00:09, 10085.24 examples/s]\u001b[A\nGenerating train examples...:  91%|█████████▏| 1011796/1108752 [01:43<00:09, 10021.51 examples/s]\u001b[A\nGenerating train examples...:  91%|█████████▏| 1012799/1108752 [01:43<00:09, 10008.91 examples/s]\u001b[A\nGenerating train examples...:  91%|█████████▏| 1013800/1108752 [01:43<00:09, 9988.11 examples/s] \u001b[A\nGenerating train examples...:  92%|█████████▏| 1014799/1108752 [01:43<00:09, 9982.18 examples/s]\u001b[A\nGenerating train examples...:  92%|█████████▏| 1015810/1108752 [01:43<00:09, 10019.14 examples/s]\u001b[A\nGenerating train examples...:  92%|█████████▏| 1016817/1108752 [01:44<00:09, 10032.46 examples/s]\u001b[A\nGenerating train examples...:  92%|█████████▏| 1017821/1108752 [01:44<00:09, 9872.78 examples/s] \u001b[A\nGenerating train examples...:  92%|█████████▏| 1018828/1108752 [01:44<00:09, 9929.41 examples/s]\u001b[A\nGenerating train examples...:  92%|█████████▏| 1019837/1108752 [01:44<00:08, 9975.59 examples/s]\u001b[A\nGenerating train examples...:  92%|█████████▏| 1020856/1108752 [01:44<00:08, 10038.87 examples/s]\u001b[A\nGenerating train examples...:  92%|█████████▏| 1021875/1108752 [01:44<00:08, 10082.54 examples/s]\u001b[A\nGenerating train examples...:  92%|█████████▏| 1022884/1108752 [01:44<00:08, 10035.88 examples/s]\u001b[A\nGenerating train examples...:  92%|█████████▏| 1023893/1108752 [01:44<00:08, 10050.38 examples/s]\u001b[A\nGenerating train examples...:  92%|█████████▏| 1024899/1108752 [01:44<00:08, 10030.91 examples/s]\u001b[A\nGenerating train examples...:  93%|█████████▎| 1025906/1108752 [01:44<00:08, 10039.95 examples/s]\u001b[A\nGenerating train examples...:  93%|█████████▎| 1026916/1108752 [01:45<00:08, 10054.90 examples/s]\u001b[A\nGenerating train examples...:  93%|█████████▎| 1027922/1108752 [01:45<00:08, 10033.52 examples/s]\u001b[A\nGenerating train examples...:  93%|█████████▎| 1028929/1108752 [01:45<00:07, 10040.29 examples/s]\u001b[A\nGenerating train examples...:  93%|█████████▎| 1029941/1108752 [01:45<00:07, 10062.06 examples/s]\u001b[A\nGenerating train examples...:  93%|█████████▎| 1030950/1108752 [01:45<00:07, 10067.68 examples/s]\u001b[A\nGenerating train examples...:  93%|█████████▎| 1031957/1108752 [01:45<00:07, 10049.59 examples/s]\u001b[A\nGenerating train examples...:  93%|█████████▎| 1032962/1108752 [01:45<00:07, 9987.13 examples/s] \u001b[A\nGenerating train examples...:  93%|█████████▎| 1033961/1108752 [01:45<00:07, 9954.76 examples/s]\u001b[A\nGenerating train examples...:  93%|█████████▎| 1034957/1108752 [01:45<00:07, 9940.03 examples/s]\u001b[A\nGenerating train examples...:  93%|█████████▎| 1035967/1108752 [01:45<00:07, 9985.89 examples/s]\u001b[A\nGenerating train examples...:  94%|█████████▎| 1036974/1108752 [01:46<00:07, 10010.83 examples/s]\u001b[A\nGenerating train examples...:  94%|█████████▎| 1037976/1108752 [01:46<00:07, 9966.28 examples/s] \u001b[A\nGenerating train examples...:  94%|█████████▎| 1038975/1108752 [01:46<00:06, 9970.08 examples/s]\u001b[A\nGenerating train examples...:  94%|█████████▍| 1039993/1108752 [01:46<00:06, 10030.26 examples/s]\u001b[A\nGenerating train examples...:  94%|█████████▍| 1040997/1108752 [01:46<00:06, 10025.87 examples/s]\u001b[A\nGenerating train examples...:  94%|█████████▍| 1042008/1108752 [01:46<00:06, 10050.80 examples/s]\u001b[A\nGenerating train examples...:  94%|█████████▍| 1043014/1108752 [01:46<00:06, 10015.96 examples/s]\u001b[A\nGenerating train examples...:  94%|█████████▍| 1044016/1108752 [01:46<00:06, 10006.06 examples/s]\u001b[A\nGenerating train examples...:  94%|█████████▍| 1045017/1108752 [01:46<00:06, 9962.06 examples/s] \u001b[A\nGenerating train examples...:  94%|█████████▍| 1046014/1108752 [01:46<00:06, 9744.28 examples/s]\u001b[A\nGenerating train examples...:  94%|█████████▍| 1046990/1108752 [01:47<00:06, 9673.79 examples/s]\u001b[A\nGenerating train examples...:  95%|█████████▍| 1047959/1108752 [01:47<00:06, 9448.68 examples/s]\u001b[A\nGenerating train examples...:  95%|█████████▍| 1048906/1108752 [01:47<00:06, 9392.94 examples/s]\u001b[A\nGenerating train examples...:  95%|█████████▍| 1049901/1108752 [01:47<00:06, 9555.12 examples/s]\u001b[A\nGenerating train examples...:  95%|█████████▍| 1050901/1108752 [01:47<00:05, 9685.36 examples/s]\u001b[A\nGenerating train examples...:  95%|█████████▍| 1051871/1108752 [01:47<00:05, 9531.79 examples/s]\u001b[A\nGenerating train examples...:  95%|█████████▍| 1052826/1108752 [01:47<00:05, 9393.77 examples/s]\u001b[A\nGenerating train examples...:  95%|█████████▌| 1053767/1108752 [01:47<00:05, 9258.74 examples/s]\u001b[A\nGenerating train examples...:  95%|█████████▌| 1054760/1108752 [01:47<00:05, 9452.99 examples/s]\u001b[A\nGenerating train examples...:  95%|█████████▌| 1055747/1108752 [01:48<00:05, 9574.53 examples/s]\u001b[A\nGenerating train examples...:  95%|█████████▌| 1056706/1108752 [01:48<00:05, 9529.20 examples/s]\u001b[A\nGenerating train examples...:  95%|█████████▌| 1057660/1108752 [01:48<00:05, 9455.90 examples/s]\u001b[A\nGenerating train examples...:  95%|█████████▌| 1058656/1108752 [01:48<00:05, 9603.95 examples/s]\u001b[A\nGenerating train examples...:  96%|█████████▌| 1059670/1108752 [01:48<00:05, 9761.12 examples/s]\u001b[A\nGenerating train examples...:  96%|█████████▌| 1060678/1108752 [01:48<00:04, 9854.19 examples/s]\u001b[A\nGenerating train examples...:  96%|█████████▌| 1061686/1108752 [01:48<00:04, 9918.86 examples/s]\u001b[A\nGenerating train examples...:  96%|█████████▌| 1062679/1108752 [01:48<00:04, 9875.75 examples/s]\u001b[A\nGenerating train examples...:  96%|█████████▌| 1063667/1108752 [01:48<00:04, 9821.78 examples/s]\u001b[A\nGenerating train examples...:  96%|█████████▌| 1064662/1108752 [01:48<00:04, 9857.18 examples/s]\u001b[A\nGenerating train examples...:  96%|█████████▌| 1065668/1108752 [01:49<00:04, 9915.71 examples/s]\u001b[A\nGenerating train examples...:  96%|█████████▌| 1066660/1108752 [01:49<00:04, 9888.17 examples/s]\u001b[A\nGenerating train examples...:  96%|█████████▋| 1067658/1108752 [01:49<00:04, 9915.00 examples/s]\u001b[A\nGenerating train examples...:  96%|█████████▋| 1068666/1108752 [01:49<00:04, 9963.94 examples/s]\u001b[A\nGenerating train examples...:  96%|█████████▋| 1069677/1108752 [01:49<00:03, 10005.14 examples/s]\u001b[A\nGenerating train examples...:  97%|█████████▋| 1070681/1108752 [01:49<00:03, 10013.90 examples/s]\u001b[A\nGenerating train examples...:  97%|█████████▋| 1071683/1108752 [01:49<00:03, 9968.24 examples/s] \u001b[A\nGenerating train examples...:  97%|█████████▋| 1072680/1108752 [01:49<00:03, 9950.85 examples/s]\u001b[A\nGenerating train examples...:  97%|█████████▋| 1073686/1108752 [01:49<00:03, 9983.16 examples/s]\u001b[A\nGenerating train examples...:  97%|█████████▋| 1074698/1108752 [01:49<00:03, 10021.37 examples/s]\u001b[A\nGenerating train examples...:  97%|█████████▋| 1075712/1108752 [01:50<00:03, 10056.44 examples/s]\u001b[A\nGenerating train examples...:  97%|█████████▋| 1076718/1108752 [01:50<00:03, 9989.61 examples/s] \u001b[A\nGenerating train examples...:  97%|█████████▋| 1077718/1108752 [01:50<00:03, 9981.54 examples/s]\u001b[A\nGenerating train examples...:  97%|█████████▋| 1078717/1108752 [01:50<00:03, 9907.08 examples/s]\u001b[A\nGenerating train examples...:  97%|█████████▋| 1079708/1108752 [01:50<00:02, 9896.03 examples/s]\u001b[A\nGenerating train examples...:  97%|█████████▋| 1080714/1108752 [01:50<00:02, 9942.29 examples/s]\u001b[A\nGenerating train examples...:  98%|█████████▊| 1081709/1108752 [01:50<00:02, 9927.41 examples/s]\u001b[A\nGenerating train examples...:  98%|█████████▊| 1082702/1108752 [01:50<00:02, 9923.00 examples/s]\u001b[A\nGenerating train examples...:  98%|█████████▊| 1083707/1108752 [01:50<00:02, 9958.82 examples/s]\u001b[A\nGenerating train examples...:  98%|█████████▊| 1084710/1108752 [01:50<00:02, 9977.66 examples/s]\u001b[A\nGenerating train examples...:  98%|█████████▊| 1085713/1108752 [01:51<00:02, 9991.80 examples/s]\u001b[A\nGenerating train examples...:  98%|█████████▊| 1086713/1108752 [01:51<00:02, 9992.32 examples/s]\u001b[A\nGenerating train examples...:  98%|█████████▊| 1087719/1108752 [01:51<00:02, 10010.47 examples/s]\u001b[A\nGenerating train examples...:  98%|█████████▊| 1088727/1108752 [01:51<00:01, 10029.70 examples/s]\u001b[A\nGenerating train examples...:  98%|█████████▊| 1089733/1108752 [01:51<00:01, 10036.92 examples/s]\u001b[A\nGenerating train examples...:  98%|█████████▊| 1090744/1108752 [01:51<00:01, 10057.64 examples/s]\u001b[A\nGenerating train examples...:  98%|█████████▊| 1091750/1108752 [01:51<00:01, 10028.08 examples/s]\u001b[A\nGenerating train examples...:  99%|█████████▊| 1092762/1108752 [01:51<00:01, 10055.22 examples/s]\u001b[A\nGenerating train examples...:  99%|█████████▊| 1093775/1108752 [01:51<00:01, 10075.15 examples/s]\u001b[A\nGenerating train examples...:  99%|█████████▊| 1094783/1108752 [01:51<00:01, 10074.64 examples/s]\u001b[A\nGenerating train examples...:  99%|█████████▉| 1095791/1108752 [01:52<00:01, 10071.86 examples/s]\u001b[A\nGenerating train examples...:  99%|█████████▉| 1096799/1108752 [01:52<00:01, 10045.98 examples/s]\u001b[A\nGenerating train examples...:  99%|█████████▉| 1097804/1108752 [01:52<00:01, 10042.17 examples/s]\u001b[A\nGenerating train examples...:  99%|█████████▉| 1098812/1108752 [01:52<00:00, 10051.26 examples/s]\u001b[A\nGenerating train examples...:  99%|█████████▉| 1099824/1108752 [01:52<00:00, 10069.06 examples/s]\u001b[A\nGenerating train examples...:  99%|█████████▉| 1100835/1108752 [01:52<00:00, 10079.96 examples/s]\u001b[A\nGenerating train examples...:  99%|█████████▉| 1101844/1108752 [01:52<00:00, 9992.41 examples/s] \u001b[A\nGenerating train examples...:  99%|█████████▉| 1102849/1108752 [01:52<00:00, 10007.40 examples/s]\u001b[A\nGenerating train examples...: 100%|█████████▉| 1103855/1108752 [01:52<00:00, 10021.43 examples/s]\u001b[A\nGenerating train examples...: 100%|█████████▉| 1104859/1108752 [01:52<00:00, 10025.39 examples/s]\u001b[A\nGenerating train examples...: 100%|█████████▉| 1105862/1108752 [01:53<00:00, 10006.50 examples/s]\u001b[A\nGenerating train examples...: 100%|█████████▉| 1106863/1108752 [01:53<00:00, 9974.09 examples/s] \u001b[A\nGenerating train examples...: 100%|█████████▉| 1107869/1108752 [01:53<00:00, 9997.16 examples/s]\u001b[A\n                                                                                                \u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:   0%|          | 0/1108752 [00:00<?, ? examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:   0%|          | 1/1108752 [00:02<710:26:58,  2.31s/ examples]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:   2%|▏         | 27472/1108752 [00:02<01:07, 16020.96 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:   5%|▌         | 56918/1108752 [00:02<00:28, 37434.05 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:   8%|▊         | 86976/1108752 [00:02<00:16, 63482.13 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  11%|█         | 117397/1108752 [00:02<00:10, 93305.75 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  13%|█▎        | 147631/1108752 [00:02<00:07, 124737.85 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  16%|█▌        | 178137/1108752 [00:02<00:05, 156621.88 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  19%|█▉        | 208839/1108752 [00:03<00:04, 186950.19 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  22%|██▏       | 239659/1108752 [00:03<00:04, 214064.82 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  24%|██▍       | 270575/1108752 [00:03<00:03, 237087.89 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  27%|██▋       | 301244/1108752 [00:03<00:03, 254972.70 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  30%|██▉       | 332197/1108752 [00:03<00:02, 269620.24 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  33%|███▎      | 363050/1108752 [00:03<00:02, 280404.68 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  36%|███▌      | 393988/1108752 [00:03<00:02, 288622.36 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  38%|███▊      | 424769/1108752 [00:03<00:02, 294151.49 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  41%|████      | 455512/1108752 [00:03<00:02, 297906.29 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  44%|████▍     | 486403/1108752 [00:03<00:02, 301138.36 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  47%|████▋     | 517291/1108752 [00:04<00:01, 303424.80 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  49%|████▉     | 548283/1108752 [00:04<00:01, 305350.40 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  52%|█████▏    | 579148/1108752 [00:04<00:01, 306224.84 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  55%|█████▌    | 610002/1108752 [00:04<00:01, 306213.19 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  58%|█████▊    | 640914/1108752 [00:04<00:01, 307077.57 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  61%|██████    | 671841/1108752 [00:04<00:01, 307727.31 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  63%|██████▎   | 702879/1108752 [00:04<00:01, 308513.26 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  66%|██████▌   | 733800/1108752 [00:04<00:01, 308717.81 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  69%|██████▉   | 764712/1108752 [00:04<00:01, 307382.60 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  72%|███████▏  | 795480/1108752 [00:04<00:01, 301236.89 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  74%|███████▍  | 826006/1108752 [00:05<00:00, 302418.05 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  77%|███████▋  | 856968/1108752 [00:05<00:00, 304549.33 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  80%|████████  | 887571/1108752 [00:05<00:00, 304984.43 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  83%|████████▎ | 918089/1108752 [00:05<00:00, 304424.26 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  86%|████████▌ | 948672/1108752 [00:05<00:00, 304839.58 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  88%|████████▊ | 979258/1108752 [00:05<00:00, 305141.32 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  91%|█████████ | 1010171/1108752 [00:05<00:00, 306330.76 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  94%|█████████▍| 1040852/1108752 [00:05<00:00, 306470.18 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  97%|█████████▋| 1071503/1108752 [00:05<00:00, 306371.47 examples/s]\u001b[A\nShuffling /kaggle/working/opus/medical/0.1.0.incompleteUHUDTS/opus-train.tfrecord*...:  99%|█████████▉| 1102331/1108752 [00:05<00:00, 306937.50 examples/s]\u001b[A\n                                                                                                                                                           \u001b[A\r","output_type":"stream"},{"name":"stdout","text":"\u001b[1mDataset opus downloaded and prepared to /kaggle/working/opus/medical/0.1.0. Subsequent calls will reuse this data.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Notice that TFDS returns a generator *function*, not a generator. This is because in Python, you cannot reset generators so you cannot go back to a previously yielded value. During deep learning training, you use Stochastic Gradient Descent and don't actually need to go back -- but it is sometimes good to be able to do that, and that's where the functions come in. It is actually very common to use generator functions in Python -- e.g., `zip` is a generator function. You can read more about [Python generators](https://book.pythontips.com/en/latest/generators.html) to understand why we use them. Let's print a a sample pair from our train and eval data. Notice that the raw ouput is represented in bytes (denoted by the `b'` prefix) and these will be converted to strings internally in the next steps.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_stream = train_stream_fn()\nprint(colored('train data (en, de) tuple:', 'red'), next(train_stream))\n\neval_stream = eval_stream_fn()\nprint(colored('eval data (en, de) tuple:', 'red'), next(eval_stream))","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:21:48.354177Z","iopub.execute_input":"2023-06-11T05:21:48.354850Z","iopub.status.idle":"2023-06-11T05:21:48.519046Z","shell.execute_reply.started":"2023-06-11T05:21:48.354822Z","shell.execute_reply":"2023-06-11T05:21:48.517836Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\u001b[31mtrain data (en, de) tuple:\u001b[0m (b'Tel: +421 2 57 103 777\\n', b'Tel: +421 2 57 103 777\\n')\n\u001b[31meval data (en, de) tuple:\u001b[0m (b'Subcutaneous use and intravenous use.\\n', b'Subkutane Anwendung und intraven\\xc3\\xb6se Anwendung.\\n')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"1-2\"></a>\n### 1.2 - Tokenization and Formatting\n\nNow that we have imported our corpus, we will be preprocessing the sentences into a format that our model can accept. This will be composed of several steps:\n\n**Tokenizing the sentences using subword representations:** As you've learned in the earlier courses of this specialization, we want to represent each sentence as an array of integers instead of strings. For our application, we will use *subword* representations to tokenize our sentences. This is a common technique to avoid out-of-vocabulary words by allowing parts of words to be represented separately. For example, instead of having separate entries in your vocabulary for --\"fear\", \"fearless\", \"fearsome\", \"some\", and \"less\"--, you can simply store --\"fear\", \"some\", and \"less\"-- then allow your tokenizer to combine these subwords when needed. This allows it to be more flexible so you won't have to save uncommon words explicitly in your vocabulary (e.g. *stylebender*, *nonce*, etc). Tokenizing is done with the `trax.data.Tokenize()` command and we have provided you the combined subword vocabulary for English and German (i.e. `ende_32k.subword`) saved in the `data` directory. Feel free to open this file to see how the subwords look like.","metadata":{}},{"cell_type":"code","source":"# global variables that state the filename and directory of the vocabulary file\nVOCAB_FILE = '/kaggle/input/subwords/ende_32k.subword'\nVOCAB_DIR = '/kaggle/input/subwords/'\n\n# Tokenize the dataset.\ntokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\ntokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)\nprint(next(train_stream))\nprint(next(tokenized_train_stream))\n","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:21:48.520312Z","iopub.execute_input":"2023-06-11T05:21:48.520671Z","iopub.status.idle":"2023-06-11T05:21:48.758814Z","shell.execute_reply.started":"2023-06-11T05:21:48.520641Z","shell.execute_reply":"2023-06-11T05:21:48.757467Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(b'During treatment with olanzapine, adolescents gained significantly more weight compared with adults.\\n', b'W\\xc3\\xa4hrend der Behandlung mit Olanzapin nahmen die Jugendlichen im Vergleich zu Erwachsenen signifikant mehr Gewicht zu.\\n')\n(array([ 2326, 13139,   605,  9214,  3337,  7932,  1047,    15,  1489,\n        4318,  6304,   331,  2326, 31329, 11722,     5, 16276, 14026,\n        2801, 11765, 14446,   363, 21981,   219, 26382,   500, 30650,\n        4729,   992]), array([ 2326, 13139,   605,  9214,  3337,  7932,  1047,    15,  1489,\n        4318,  6304,   331,  2326, 31329, 11722,     5, 16276, 14026,\n        2801, 11765, 14446,   363, 21981,   219, 26382,   500, 30650,\n        4729,   992]))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Append an end-of-sentence token to each sentence:** We will assign a token (i.e. in this case `1`) to mark the end of a sentence. This will be useful in inference/prediction so we'll know that the model has completed the translation.","metadata":{}},{"cell_type":"code","source":"# Append EOS at the end of each sentence.\n\n# Integer assigned as end-of-sentence (EOS)\nEOS = 1\n\n# generator helper function to append EOS to each sentence\ndef append_eos(stream):\n    for (inputs, targets) in stream:\n        inputs_with_eos = list(inputs) + [EOS]\n        targets_with_eos = list(targets) + [EOS]\n        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n\n# append EOS to the train data\ntokenized_train_stream = append_eos(tokenized_train_stream)\n\n# append EOS to the eval data\ntokenized_eval_stream = append_eos(tokenized_eval_stream)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:21:48.760251Z","iopub.execute_input":"2023-06-11T05:21:48.760678Z","iopub.status.idle":"2023-06-11T05:21:48.767694Z","shell.execute_reply.started":"2023-06-11T05:21:48.760648Z","shell.execute_reply":"2023-06-11T05:21:48.766392Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Filter long sentences:** We will place a limit on the number of tokens per sentence to ensure we won't run out of memory. This is done with the `trax.data.FilterByLength()` method and you can see its syntax below.","metadata":{}},{"cell_type":"code","source":"# Filter too long sentences to not run out of memory.\n# length_keys=[0, 1] means we filter both English and German sentences, so\n# both must be not longer than 512 tokens for training / 512 for eval.\nfiltered_train_stream = trax.data.FilterByLength(\n    max_length=512, length_keys=[0, 1])(tokenized_train_stream)\nfiltered_eval_stream = trax.data.FilterByLength(\n    max_length=512, length_keys=[0, 1])(tokenized_eval_stream)\n\n# print a sample input-target pair of tokenized sentences\ntrain_input, train_target = next(filtered_train_stream)\nprint(colored(f'Single tokenized example input:', 'red' ), train_input)\nprint(colored(f'Single tokenized example target:', 'red'), train_target)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:26:22.362386Z","iopub.execute_input":"2023-06-11T05:26:22.363438Z","iopub.status.idle":"2023-06-11T05:26:22.372081Z","shell.execute_reply.started":"2023-06-11T05:26:22.363392Z","shell.execute_reply":"2023-06-11T05:26:22.370895Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"\u001b[31mSingle tokenized example input:\u001b[0m [18798    13 32955    13 30650  4729   992     1]\n\u001b[31mSingle tokenized example target:\u001b[0m [ 6717    13 18798    13 32955    13 30650  4729   992     1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"1-3\"></a>\n### 1.3 - tokenize & detokenize Helper Functions\n\nGiven any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your trax models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: \n\n- <span style='color:blue'> word2Ind: </span> a dictionary mapping the word to its index.\n- <span style='color:blue'> ind2Word:</span> a dictionary mapping the index to its word.\n- <span style='color:blue'> word2Count:</span> a dictionary mapping the word to the number of times it appears. \n- <span style='color:blue'> num_words:</span> total number of words that have appeared. \n\nSince you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:\n\n- <span style='color:blue'> tokenize(): </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords (parts of words).\n- <span style='color:blue'> detokenize(): </span> converts a token list to its corresponding sentence (i.e. string).","metadata":{}},{"cell_type":"code","source":"# Setup helper functions for tokenizing and detokenizing sentences\n\ndef tokenize(input_str, vocab_file=None, vocab_dir=None):\n    \"\"\"Encodes a string to an array of integers\n\n    Args:\n        input_str (str): human-readable string to encode\n        vocab_file (str): filename of the vocabulary text file\n        vocab_dir (str): path to the vocabulary file\n  \n    Returns:\n        numpy.ndarray: tokenized version of the input string\n    \"\"\"\n    \n    # Set the encoding of the \"end of sentence\" as 1\n    EOS = 1\n    \n    # Use the trax.data.tokenize method. It takes streams and returns streams,\n    # we get around it by making a 1-element stream with `iter`.\n    inputs =  next(trax.data.tokenize(iter([input_str]),\n                                      vocab_file=vocab_file, vocab_dir=vocab_dir))\n    \n    # Mark the end of the sentence with EOS\n    inputs = list(inputs) + [EOS]\n    \n    # Adding the batch dimension to the front of the shape\n    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n    \n    return batch_inputs\n\n\ndef detokenize(integers, vocab_file=None, vocab_dir=None):\n    \"\"\"Decodes an array of integers to a human readable string\n\n    Args:\n        integers (numpy.ndarray): array of integers to decode\n        vocab_file (str): filename of the vocabulary text file\n        vocab_dir (str): path to the vocabulary file\n  \n    Returns:\n        str: the decoded sentence.\n    \"\"\"\n    \n    # Remove the dimensions of size 1\n    integers = list(np.squeeze(integers))\n    \n    # Set the encoding of the \"end of sentence\" as 1\n    EOS = 1\n    \n    # Remove the EOS to decode only the original tokens\n    if EOS in integers:\n        integers = integers[:integers.index(EOS)] \n    \n    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:26:24.205672Z","iopub.execute_input":"2023-06-11T05:26:24.206495Z","iopub.status.idle":"2023-06-11T05:26:24.216877Z","shell.execute_reply.started":"2023-06-11T05:26:24.206460Z","shell.execute_reply":"2023-06-11T05:26:24.215824Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"Let's see how we might use these functions:","metadata":{}},{"cell_type":"code","source":"# As declared earlier:\n# VOCAB_FILE = 'ende_32k.subword'\n# VOCAB_DIR = 'data/'\n\n# Detokenize an input-target pair of tokenized sentences\nprint(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\nprint(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\nprint()\n\n# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\n# See how it combines the subwords -- 'hell' and 'o'-- to form the word 'hello'.\nprint(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\nprint(colored(f\"detokenize([17332, 140, 1]): \", 'green'), detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:26:25.861398Z","iopub.execute_input":"2023-06-11T05:26:25.862256Z","iopub.status.idle":"2023-06-11T05:26:26.737550Z","shell.execute_reply.started":"2023-06-11T05:26:25.862223Z","shell.execute_reply":"2023-06-11T05:26:26.736472Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"\u001b[31mSingle detokenized example input:\u001b[0m 45a 42a\n\n\u001b[31mSingle detokenized example target:\u001b[0m 24a 45a 42a\n\n\n\u001b[32mtokenize('hello'): \u001b[0m [[17332   140     1]]\n\u001b[32mdetokenize([17332, 140, 1]): \u001b[0m hello\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"1-4\"></a>\n### 1.4 - Bucketing\n\nBucketing the tokenized sentences is an important technique used to speed up training in NLP.\nHere is a \n[nice article describing it in detail](https://medium.com/@rashmi.margani/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976)\nbut the gist is very simple. Our inputs have variable lengths and you want to make these the same when batching groups of sentences together. One way to do that is to pad each sentence to the length of the longest sentence in the dataset. This might lead to some wasted computation though. For example, if there are multiple short sentences with just two tokens, do we want to pad these when the longest sentence is composed of a 100 tokens? Instead of padding with 0s to the maximum length of a sentence each time, we can group our tokenized sentences by length and bucket, as on this image (from the article above):\n\n![alt text](https://miro.medium.com/max/700/1*hcGuja_d5Z_rFcgwe9dPow.png)\n\nWe batch the sentences with similar length together (e.g. the blue sentences in the image above) and only add minimal padding to make them have equal length (usually up to the nearest power of two). This allows to waste less computation when processing padded sequences.\nIn Trax, it is implemented in the [bucket_by_length](https://github.com/google/trax/blob/5fb8aa8c5cb86dabb2338938c745996d5d87d996/trax/supervised/inputs.py#L378) function.","metadata":{}},{"cell_type":"code","source":"# Bucketing to create streams of batches.\n\n# Buckets are defined in terms of boundaries and batch sizes.\n# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n# So below, we'll take a batch of 256 sentences of length < 8, 128 if length is\n# between 8 and 16, and so on -- and only 2 if length is over 512.\nboundaries =  [8,   16,  32, 64, 128, 256,512]\nbatch_sizes = [ 256,128,64, 32, 16,    8,   4, 2]\n\n# Create the generators.\ntrain_batch_stream = trax.data.BucketByLength(\n    boundaries, batch_sizes,\n    length_keys=[0, 1]  # As before: count inputs and targets to length.\n)(filtered_train_stream)\n\neval_batch_stream = trax.data.BucketByLength(\n    boundaries, batch_sizes,\n    length_keys=[0, 1]  # As before: count inputs and targets to length.\n)(filtered_eval_stream)\n\n# Add masking for the padding (0s).\ntrain_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\neval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:26:43.001257Z","iopub.execute_input":"2023-06-11T05:26:43.001639Z","iopub.status.idle":"2023-06-11T05:26:43.009479Z","shell.execute_reply.started":"2023-06-11T05:26:43.001610Z","shell.execute_reply":"2023-06-11T05:26:43.008329Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"1-5\"></a>\n### 1.5 - Exploring the Data\n\nWe will now be displaying some of our data. You will see that the functions defined above (i.e. `tokenize()` and `detokenize()`) do the same things you have been doing again and again throughout the specialization. We gave these so you can focus more on building the model from scratch. Let us first get the data generator and get one batch of the data.","metadata":{}},{"cell_type":"code","source":"input_batch, target_batch, mask_batch = next(train_batch_stream)\n\n# let's see the data type of a batch\nprint(\"input_batch data type: \", type(input_batch))\nprint(\"target_batch data type: \", type(target_batch))\n\n# let's see the shape of this particular batch (batch length, sentence length)\nprint(\"input_batch shape: \", input_batch.shape)\nprint(\"target_batch shape: \", target_batch.shape)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-06-11T05:26:46.218167Z","iopub.execute_input":"2023-06-11T05:26:46.218597Z","iopub.status.idle":"2023-06-11T05:26:46.301843Z","shell.execute_reply.started":"2023-06-11T05:26:46.218562Z","shell.execute_reply":"2023-06-11T05:26:46.300846Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"input_batch data type:  <class 'numpy.ndarray'>\ntarget_batch data type:  <class 'numpy.ndarray'>\ninput_batch shape:  (32, 64)\ntarget_batch shape:  (32, 64)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The `input_batch` and `target_batch` are Numpy arrays consisting of tokenized English sentences and German sentences respectively. These tokens will later be used to produce embedding vectors for each word in the sentence (so the embedding for a sentence will be a matrix). The number of sentences in each batch is usually a power of 2 for optimal computer memory usage. \n\nWe can now visually inspect some of the data. You can run the cell below several times to shuffle through the sentences. Just to note, while this is a standard data set that is used widely, it does have some known wrong translations. With that, let's pick a random sentence and print its tokenized representation.","metadata":{}},{"cell_type":"code","source":"# pick a random index less than the batch size.\nindex = random.randrange(len(input_batch))\n\n# use the index to grab an entry from the input and target batch\nprint(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\nprint(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\nprint(colored('THIS IS THE GERMAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\nprint(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:26:49.508522Z","iopub.execute_input":"2023-06-11T05:26:49.509456Z","iopub.status.idle":"2023-06-11T05:26:49.959052Z","shell.execute_reply.started":"2023-06-11T05:26:49.509420Z","shell.execute_reply":"2023-06-11T05:26:49.957798Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"\u001b[31mTHIS IS THE ENGLISH SENTENCE: \n\u001b[0m The arbitration procedure was discussed by the CHMP at its plenary meeting in November 2007 and a Rapporteur (Dr Harald Enzmann) and Co-Rapporteur (Dr.\n \n\n\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n \u001b[0m [   29 30415  2300   596  2086    53  3495    45     4  9204  9812     5\n    68    95  7697  1417     6  1068   811     8    13 14093 12073  1676\n    50  3903  5135 11395     5  5147  2945  7738    80     8  2873     5\n    15 14093 12073  1676    50  3903  3550 30650  4729   992     1     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0] \n\n\u001b[31mTHIS IS THE GERMAN TRANSLATION: \n\u001b[0m Harald Enzmann) und Mitberichterstatter (Dr.\n \n\n\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \n\u001b[0m [ 5135 11395     5  5147  2945  7738    80    12 18306 22851 14723   667\n    50  3903  3550 30650  4729   992     1     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0] \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"2\"></a>\n## 2 - Neural Machine Translation with Attention\n\nNow that you have the data generators and have handled the preprocessing, it is time for you to build the model. You will be implementing a neural machine translation model from scratch with attention.\n","metadata":{}},{"cell_type":"markdown","source":"<a name=\"2-1\"></a>\n### 2.1 - Attention Overview\n\nThe model we will be building uses an encoder-decoder architecture. This Recurrent Neural Network (RNN) will take in a tokenized version of a sentence in its encoder, then passes it on to the decoder for translation. As mentioned in the lectures, just using a a regular sequence-to-sequence model with LSTMs will work effectively for short to medium sentences but will start to degrade for longer ones. You can picture it like the figure below where all of the context of the input sentence is compressed into one vector that is passed into the decoder block. You can see how this will be an issue for very long sentences (e.g. 100 tokens or more) because the context of the first parts of the input will have very little effect on the final vector passed to the decoder.\n\nAdding an attention layer to this model avoids this problem by giving the decoder access to all parts of the input sentence. To illustrate, let's just use a 4-word input sentence as shown below. Remember that a hidden state is produced at each timestep of the encoder (represented by the orange rectangles). These are all passed to the attention layer and each are given a score given the current activation (i.e. hidden state) of the decoder. For instance, let's consider the figure below where the first prediction \"Wie\" is already made. To produce the next prediction, the attention layer will first receive all the encoder hidden states (i.e. orange rectangles) as well as the decoder hidden state when producing the word \"Wie\" (i.e. first green rectangle). Given this information, it will score each of the encoder hidden states to know which one the decoder should focus on to produce the next word. As a result of training, the model might have learned that it should align to the second encoder hidden state and subsequently assigns a high probability to the word \"geht\". If we are using greedy decoding, we will output the said word as the next symbol, then restart the process to produce the next word until we reach an end-of-sentence prediction.\n\n\nThere are different ways to implement attention and the one we'll use for this assignment is the Scaled Dot Product Attention which has the form:\n\n$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n\nYou will dive deeper into this equation in the next week but for now, you can think of it as computing scores using queries (Q) and keys (K), followed by a multiplication of values (V) to get a context vector at a particular timestep of the decoder. This context vector is fed to the decoder RNN to get a set of probabilities for the next predicted word. The division by square root of the keys dimensionality ($\\sqrt{d_k}$) is for improving model performance and you'll also learn more about it next week. For our machine translation application, the encoder activations (i.e. encoder hidden states) will be the keys and values, while the decoder activations (i.e. decoder hidden states) will be the queries.\n\nYou will see in the upcoming sections that this complex architecture and mechanism can be implemented with just a few lines of code. Let's get started!","metadata":{}},{"cell_type":"markdown","source":"<a name=\"2-2\"></a>\n### 2.2 - Helper Functions\n\nWe will first implement a few functions that we will use later on. These will be for the input encoder, pre-attention decoder, and preparation of the queries, keys, values, and mask.\n\n<a name=\"2-2-1\"></a>\n#### 2.2.1 - Input Encoder\n\nThe input encoder runs on the input tokens, creates its embeddings, and feeds it to an LSTM network. This outputs the activations that will be the keys and values for attention. It is a [Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial) network which uses:\n\n   - [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding): Converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: `tl.Embedding(vocab_size, d_model)`. `vocab_size` is the number of entries in the given vocabulary. `d_model` is the number of elements in the word embedding.\n  \n   - [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM): LSTM layer of size `d_model`. We want to be able to configure how many encoder layers we have so remember to create LSTM layers equal to the number of the `n_encoder_layers` parameter.\n\n<a name=\"ex-1\"></a>\n### Exercise 1 - input_encoder_fn\n\n**Instructions:** Implement the `input_encoder_fn` function.","metadata":{}},{"cell_type":"code","source":"# UNQ_C1\ndef input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\n    \"\"\" Input encoder runs on the input sentence and creates\n    activations that will be the keys and values for attention.\n    \n    Args:\n        input_vocab_size: int: vocab size of the input\n        d_model: int:  depth of embedding (n_units in the LSTM cell)\n        n_encoder_layers: int: number of LSTM layers in the encoder\n    Returns:\n        tl.Serial: The input encoder\n    \"\"\"\n    \n    # create a serial network\n    input_encoder = tl.Serial( \n        \n        ### START CODE HERE ###\n        # create an embedding layer to convert tokens to vectors\n        tl.Embedding(input_vocab_size, d_model),\n        \n        # feed the embeddings to the LSTM layers. It is a stack of n_encoder_layers LSTM layers\n        [tl.LSTM(d_model) for _ in range(n_encoder_layers)]\n        ### END CODE HERE ###\n    )\n    return input_encoder","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:26:51.183659Z","iopub.execute_input":"2023-06-11T05:26:51.184561Z","iopub.status.idle":"2023-06-11T05:26:51.190613Z","shell.execute_reply.started":"2023-06-11T05:26:51.184527Z","shell.execute_reply":"2023-06-11T05:26:51.189567Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"2-2-2\"></a>\n#### 2.2.2 - Pre-attention Decoder\n\nThe pre-attention decoder runs on the targets and creates activations that are used as queries in attention. This is a Serial network which is composed of the following:\n\n   - [tl.ShiftRight](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight): This pads a token to the beginning of your target tokens (e.g. `[8, 34, 12]` shifted right is `[0, 8, 34, 12]`). This will act like a start-of-sentence token that will be the first input to the decoder. During training, this shift also allows the target tokens to be passed as input to do teacher forcing.\n\n   - [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding): Like in the previous function, this converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: `tl.Embedding(vocab_size, d_model)`. `vocab_size` is the number of entries in the given vocabulary. `d_model` is the number of elements in the word embedding.\n   \n   - [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM): LSTM layer of size `d_model`.\n\n<a name=\"ex-2\"></a>\n### Exercise 2 - pre_attention_decoder_fn\n\n**Instructions:** Implement the `pre_attention_decoder_fn` function.\n","metadata":{}},{"cell_type":"code","source":"# UNQ_C2\n# GRADED FUNCTION\ndef pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n    \"\"\" Pre-attention decoder runs on the targets and creates\n    activations that are used as queries in attention.\n    \n    Args:\n        mode: str: 'train' or 'eval'\n        target_vocab_size: int: vocab size of the target\n        d_model: int:  depth of embedding (n_units in the LSTM cell)\n    Returns:\n        tl.Serial: The pre-attention decoder\n    \"\"\"\n    \n    # create a serial network\n    pre_attention_decoder = tl.Serial(\n        \n        ### START CODE HERE ###\n        # shift right to insert start-of-sentence token and implement\n        # teacher forcing during training\n        tl.ShiftRight(),\n\n        # run an embedding layer to convert tokens to vectors\n        tl.Embedding(target_vocab_size, d_model),\n\n        # feed to an LSTM layer\n        tl.LSTM(d_model)\n        ### END CODE HERE ###\n    )\n    \n    return pre_attention_decoder","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:26:52.928339Z","iopub.execute_input":"2023-06-11T05:26:52.929271Z","iopub.status.idle":"2023-06-11T05:26:52.935099Z","shell.execute_reply.started":"2023-06-11T05:26:52.929236Z","shell.execute_reply":"2023-06-11T05:26:52.934236Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"2-2-3\"></a>\n#### 2.2.3 - Preparing the Attention Input\n\nThis function will prepare the inputs to the attention layer. We want to take in the encoder and pre-attention decoder activations and assign it to the queries, keys, and values. In addition, another output here will be the mask to distinguish real tokens from padding tokens. This mask will be used internally by Trax when computing the softmax so padding tokens will not have an effect on the computated probabilities. From the data preparation steps in Section 1 of this assignment, you should know which tokens in the input correspond to padding.\n\nWe have filled the last two lines in composing the mask for you because it includes a concept that will be discussed further next week. This is related to *multiheaded attention* which you can think of right now as computing the attention multiple times to improve the model's predictions. It is required to consider this additional axis in the output so we've included it already but you don't need to analyze it just yet. What's important now is for you to know which should be the queries, keys, and values, as well as to initialize the mask.\n\n<a name=\"ex-3\"></a>\n### Exercise 3 - prepare_attention_input\n\n**Instructions:** Implement the  `prepare_attention_input` function\n","metadata":{}},{"cell_type":"code","source":"# UNQ_C3\n# GRADED FUNCTION\ndef prepare_attention_input(encoder_activations, decoder_activations, inputs):\n    \"\"\"Prepare queries, keys, values and mask for attention.\n    \n    Args:\n        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder\n        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder\n        inputs fastnp.array(batch_size, padded_input_length): input tokens\n    \n    Returns:\n        queries, keys, values and mask for attention.\n    \"\"\"\n    \n    ### START CODE HERE ###\n    \n    # set the keys and values to the encoder activations\n    keys = encoder_activations\n    values = encoder_activations\n\n    \n    # set the queries to the decoder activations\n    queries = decoder_activations\n    \n    # generate the mask to distinguish real tokens from padding\n    # hint: inputs is positive for real tokens and 0 where they are padding\n    mask = inputs > 0\n    \n    ### END CODE HERE ###\n    \n    # add axes to the mask for attention heads and decoder length.\n    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n    \n    # broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len].\n    # note: for this assignment, attention heads is set to 1.\n    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n        \n    return queries, keys, values, mask\n","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:26:54.333815Z","iopub.execute_input":"2023-06-11T05:26:54.334620Z","iopub.status.idle":"2023-06-11T05:26:54.342286Z","shell.execute_reply.started":"2023-06-11T05:26:54.334590Z","shell.execute_reply":"2023-06-11T05:26:54.341232Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"2-3\"></a>\n### 2.3 - Implementation Overview\n\nWe are now ready to implement our sequence-to-sequence model with attention. This will be a Serial network and is illustrated in the diagram below. It shows the layers you'll be using in Trax and you'll see that each step can be implemented quite easily with one line commands. We've placed several links to the documentation for each relevant layer in the discussion after the figure below.","metadata":{}},{"cell_type":"markdown","source":"<a name=\"ex-4\"></a>\n### Exercise 4 - NMTAttn\n**Instructions:** Implement the `NMTAttn` function below to define your machine translation model which uses attention. We have left hyperlinks below pointing to the Trax documentation of the relevant layers. Remember to consult it to get tips on what parameters to pass.\n\n**Step 0:** Prepare the input encoder and pre-attention decoder branches. You have already defined this earlier as helper functions so it's just a matter of calling those functions and assigning it to variables.\n\n**Step 1:** Create a Serial network. This will stack the layers in the next steps one after the other. Like the earlier exercises, you can use [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial).\n\n**Step 2:** Make a copy of the input and target tokens. As you see in the diagram above, the input and target tokens will be fed into different layers of the model. You can use [tl.Select](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Select) layer to create copies of these tokens. Arrange them as `[input tokens, target tokens, input tokens, target tokens]`.\n\n**Step 3:** Create a parallel branch to feed the input tokens to the `input_encoder` and the target tokens to the `pre_attention_decoder`. You can use [tl.Parallel](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Parallel) to create these sublayers in parallel. Remember to pass the variables you defined in Step 0 as parameters to this layer.\n\n**Step 4:** Next, call the `prepare_attention_input` function to convert the encoder and pre-attention decoder activations to a format that the attention layer will accept. You can use [tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn) to call this function. Note: Pass the `prepare_attention_input` function as the `f` parameter in `tl.Fn` without any arguments or parenthesis.\n\n**Step 5:** We will now feed the (queries, keys, values, and mask) to the [tl.AttentionQKV](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.AttentionQKV) layer. This computes the scaled dot product attention and outputs the attention weights and mask. Take note that although it is a one liner, this layer is actually composed of a deep network made up of several branches. We'll show the implementation taken [here](https://github.com/google/trax/blob/master/trax/layers/attention.py#L61) to see the different layers used. \n\n```python\ndef AttentionQKV(d_feature, n_heads=1, dropout=0.0, mode='train'):\n  \"\"\"Returns a layer that maps (q, k, v, mask) to (activations, mask).\n\n  See `Attention` above for further context/details.\n\n  Args:\n    d_feature: Depth/dimensionality of feature embedding.\n    n_heads: Number of attention heads.\n    dropout: Probababilistic rate for internal dropout applied to attention\n        activations (based on query-key pairs) before dotting them with values.\n    mode: Either 'train' or 'eval'.\n  \"\"\"\n  return cb.Serial(\n      cb.Parallel(\n          core.Dense(d_feature),\n          core.Dense(d_feature),\n          core.Dense(d_feature),\n      ),\n      PureAttention(  # pylint: disable=no-value-for-parameter\n          n_heads=n_heads, dropout=dropout, mode=mode),\n      core.Dense(d_feature),\n  )\n```\n\nHaving deep layers pose the risk of vanishing gradients during training and we would want to mitigate that. To improve the ability of the network to learn, we can insert a [tl.Residual](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) layer to add the output of AttentionQKV with the `queries` input. You can do this in trax by simply nesting the `AttentionQKV` layer inside the `Residual` layer. The library will take care of branching and adding for you.\n\n**Step 6:** We will not need the mask for the model we're building so we can safely drop it. At this point in the network, the signal stack currently has `[attention activations, mask, target tokens]` and you can use [tl.Select](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Select) to output just `[attention activations, target tokens]`.\n\n**Step 7:** We can now feed the attention weighted output to the LSTM decoder. We can stack multiple [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM) layers to improve the output so remember to append LSTMs equal to the number defined by `n_decoder_layers` parameter to the model.\n\n**Step 8:** We want to determine the probabilities of each subword in the vocabulary and you can set this up easily with a [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) layer by making its size equal to the size of our vocabulary.\n\n**Step 9:** Normalize the output to log probabilities by passing the activations in Step 8 to a [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax) layer.","metadata":{}},{"cell_type":"code","source":"# UNQ_C4\n# GRADED FUNCTION\ndef NMTAttn(input_vocab_size=33300,\n            target_vocab_size=33300,\n            d_model=1024,\n            n_encoder_layers=2,\n            n_decoder_layers=2,\n            n_attention_heads=4,\n            attention_dropout=0.0,\n            mode='train'):\n    \"\"\"Returns an LSTM sequence-to-sequence model with attention.\n\n    The input to the model is a pair (input tokens, target tokens), e.g.,\n    an English sentence (tokenized) and its translation into German (tokenized).\n\n    Args:\n    input_vocab_size: int: vocab size of the input\n    target_vocab_size: int: vocab size of the target\n    d_model: int:  depth of embedding (n_units in the LSTM cell)\n    n_encoder_layers: int: number of LSTM layers in the encoder\n    n_decoder_layers: int: number of LSTM layers in the decoder after attention\n    n_attention_heads: int: number of attention heads\n    attention_dropout: float, dropout for the attention layer\n    mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference\n\n    Returns:\n    A LSTM sequence-to-sequence model with attention.\n    \"\"\"\n\n    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\n    \n    # Step 0: call the helper function to create layers for the input encoder\n    input_encoder = input_encoder_fn(input_vocab_size, d_model, n_encoder_layers)\n\n    # Step 0: call the helper function to create layers for the pre-attention decoder\n    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\n\n    # Step 1: create a serial network\n    model = tl.Serial( \n        \n      # Step 2: copy input tokens and target tokens as they will be needed later.\n      tl.Select([0,1,0,1]),\n        \n      # Step 3: run input encoder on the input and pre-attention decoder the target.\n      tl.Parallel(input_encoder, pre_attention_decoder),\n        \n      # Step 4: prepare queries, keys, values and mask for attention.\n      tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n        \n      # Step 5: run the AttentionQKV layer\n      # nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)\n      tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n      \n      # Step 6: drop attention mask (i.e. index = None\n      tl.Select([0,2]),\n        \n      # Step 7: run the rest of the RNN decoder\n      [tl.LSTM(n_units=d_model) for _ in range(n_decoder_layers)],\n        \n      # Step 8: prepare output by making it the right size\n      tl.Dense(target_vocab_size),\n        \n      # Step 9: Log-softmax for output\n       tl.LogSoftmax()\n    )\n    \n    ### END CODE HERE\n    \n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:27:14.312544Z","iopub.execute_input":"2023-06-11T05:27:14.312913Z","iopub.status.idle":"2023-06-11T05:27:14.324145Z","shell.execute_reply.started":"2023-06-11T05:27:14.312885Z","shell.execute_reply":"2023-06-11T05:27:14.323010Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# print your model\nmodel = NMTAttn()\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:27:14.658700Z","iopub.execute_input":"2023-06-11T05:27:14.658994Z","iopub.status.idle":"2023-06-11T05:27:14.668619Z","shell.execute_reply.started":"2023-06-11T05:27:14.658970Z","shell.execute_reply":"2023-06-11T05:27:14.667605Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Serial_in2_out2[\n  Select[0,1,0,1]_in2_out4\n  Parallel_in2_out2[\n    Serial[\n      Embedding_33300_1024\n      LSTM_1024\n      LSTM_1024\n    ]\n    Serial[\n      Serial[\n        ShiftRight(1)\n      ]\n      Embedding_33300_1024\n      LSTM_1024\n    ]\n  ]\n  PrepareAttentionInput_in3_out4\n  Serial_in4_out2[\n    Branch_in4_out3[\n      None\n      Serial_in4_out2[\n        _in4_out4\n        Serial_in4_out2[\n          Parallel_in3_out3[\n            Dense_1024\n            Dense_1024\n            Dense_1024\n          ]\n          PureAttention_in4_out2\n          Dense_1024\n        ]\n        _in2_out2\n      ]\n    ]\n    Add_in2\n  ]\n  Select[0,2]_in3_out2\n  LSTM_1024\n  LSTM_1024\n  Dense_33300\n  LogSoftmax\n]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Expected Output:**\n\n```\nSerial_in2_out2[\n  Select[0,1,0,1]_in2_out4\n  Parallel_in2_out2[\n    Serial[\n      Embedding_33300_1024\n      LSTM_1024\n      LSTM_1024\n    ]\n    Serial[\n      Serial[\n        ShiftRight(1)\n      ]\n      Embedding_33300_1024\n      LSTM_1024\n    ]\n  ]\n  PrepareAttentionInput_in3_out4\n  Serial_in4_out2[\n    Branch_in4_out3[\n      None\n      Serial_in4_out2[\n        _in4_out4\n        Serial_in4_out2[\n          Parallel_in3_out3[\n            Dense_1024\n            Dense_1024\n            Dense_1024\n          ]\n          PureAttention_in4_out2\n          Dense_1024\n        ]\n        _in2_out2\n      ]\n    ]\n    Add_in2\n  ]\n  Select[0,2]_in3_out2\n  LSTM_1024\n  LSTM_1024\n  Dense_33300\n  LogSoftmax\n]\n```","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"3\"></a>\n## 3 - Training\n\nWe will now be training our model in this section. Doing supervised training in Trax is pretty straightforward (short example [here](https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html#Supervised-training)). We will be instantiating three classes for this: `TrainTask`, `EvalTask`, and `Loop`. Let's take a closer look at each of these in the sections below.\n","metadata":{}},{"cell_type":"markdown","source":"<a name=\"3-1\"></a>\n### 3.1 - TrainTask\n\nThe [TrainTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) class allows us to define the labeled data to use for training and the feedback mechanisms to compute the loss and update the weights. \n\n<a name=\"ex-5\"></a>\n### Exercise 5 - train_task_function\n\n**Instructions:** Instantiate a train task.","metadata":{}},{"cell_type":"code","source":"# UNQ_C5\n# GRADED PART\ndef train_task_function(train_batch_stream):\n    \"\"\"Returns a trax.training.TrainTask object.\n\n    Args:\n    train_batch_stream generator: labeled data generator\n\n    Returns:\n    A trax.training.TrainTask object.\n    \"\"\"\n    return training.TrainTask(\n\n    ### START CODE HERE\n    \n    # use the train batch stream as labeled data\n    labeled_data= train_batch_stream,\n    \n    # use the cross entropy loss\n    loss_layer= tl.CrossEntropyLoss(),\n    \n    # use the Adam optimizer with learning rate of 0.01\n    optimizer= trax.optimizers.Adam(0.01),\n    \n    # use the `trax.lr.warmup_and_rsqrt_decay` as the learning rate schedule\n    # have 1000 warmup steps with a max value of 0.01\n    lr_schedule= trax.lr.warmup_and_rsqrt_decay(1000, 0.01),\n    \n    # have a checkpoint every 10 steps\n    n_steps_per_checkpoint= 10,\n\n        ### END CODE HERE\n    )","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:27:16.282808Z","iopub.execute_input":"2023-06-11T05:27:16.283122Z","iopub.status.idle":"2023-06-11T05:27:16.289369Z","shell.execute_reply.started":"2023-06-11T05:27:16.283096Z","shell.execute_reply":"2023-06-11T05:27:16.288463Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"train_task = train_task_function(train_batch_stream)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:27:16.718983Z","iopub.execute_input":"2023-06-11T05:27:16.719381Z","iopub.status.idle":"2023-06-11T05:27:16.754937Z","shell.execute_reply.started":"2023-06-11T05:27:16.719349Z","shell.execute_reply":"2023-06-11T05:27:16.753777Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"3-2\"></a>\n### 3.2 - EvalTask\n\nThe [EvalTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) on the other hand allows us to see how the model is doing while training. For our application, we want it to report the cross entropy loss and accuracy.","metadata":{}},{"cell_type":"code","source":"eval_task = training.EvalTask(\n    \n    ## use the eval batch stream as labeled data\n    labeled_data=eval_batch_stream,\n    \n    ## use the cross entropy loss and accuracy as metrics\n    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:27:18.956229Z","iopub.execute_input":"2023-06-11T05:27:18.956927Z","iopub.status.idle":"2023-06-11T05:27:19.017046Z","shell.execute_reply.started":"2023-06-11T05:27:18.956892Z","shell.execute_reply":"2023-06-11T05:27:19.016090Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"3-3\"></a>\n### 3.3 - Loop\n\nThe [Loop](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) class defines the model we will train as well as the train and eval tasks to execute. Its `run()` method allows us to execute the training for a specified number of steps.","metadata":{}},{"cell_type":"code","source":"# define the output directory\noutput_dir = '/kaggle/working/'\n\n# remove old model if it exists. restarts training.\n!rm -f {output_dir}/model.pkl.gz\n\n# define the training loop\ntraining_loop = training.Loop(NMTAttn(mode='train'),\n                              train_task,\n                              eval_tasks=[eval_task],\n                              output_dir=output_dir)\n","metadata":{"lines_to_next_cell":2,"execution":{"iopub.status.busy":"2023-06-11T05:27:20.710045Z","iopub.execute_input":"2023-06-11T05:27:20.710902Z","iopub.status.idle":"2023-06-11T05:27:41.512392Z","shell.execute_reply.started":"2023-06-11T05:27:20.710869Z","shell.execute_reply":"2023-06-11T05:27:41.511011Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# NOTE: Execute the training loop. This will take around 11 minutes to complete.\ntraining_loop.run(10)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:27:48.873109Z","iopub.execute_input":"2023-06-11T05:27:48.873533Z","iopub.status.idle":"2023-06-11T05:32:08.498827Z","shell.execute_reply.started":"2023-06-11T05:27:48.873495Z","shell.execute_reply":"2023-06-11T05:32:08.497418Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"\nStep      1: Total number of trainable weights: 148492820\nStep      1: Ran 1 train steps in 97.85 secs\nStep      1: train CrossEntropyLoss |  10.42905998\nStep      1: eval  CrossEntropyLoss |  10.41372108\nStep      1: eval          Accuracy |  0.00000000\n\nStep     10: Ran 9 train steps in 150.88 secs\nStep     10: train CrossEntropyLoss |  10.27301407\nStep     10: eval  CrossEntropyLoss |  9.98818207\nStep     10: eval          Accuracy |  0.02503794\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"4\"></a>\n## 4 - Testing\n\nWe will now be using the model you just trained to translate English sentences to German. We will implement this with two functions: The first allows you to identify the next symbol (i.e. output token). The second one takes care of combining the entire translated string.\n\nWe will start by first loading in a pre-trained copy of the model you just coded. Please run the cell below to do just that.","metadata":{}},{"cell_type":"code","source":"# instantiate the model we built in eval mode\nmodel = NMTAttn(mode='eval')\n\n# initialize weights from a pre-trained model\nmodel.init_from_file(\"/kaggle/working/model.pkl.gz\", weights_only=True)\nmodel = tl.Accelerate(model)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:32:44.499126Z","iopub.execute_input":"2023-06-11T05:32:44.500256Z","iopub.status.idle":"2023-06-11T05:32:50.525451Z","shell.execute_reply.started":"2023-06-11T05:32:44.500216Z","shell.execute_reply":"2023-06-11T05:32:50.524108Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"4-1\"></a>\n### 4.1 - Decoding\n\nAs discussed in the lectures, there are several ways to get the next token when translating a sentence. For instance, we can just get the most probable token at each step (i.e. greedy decoding) or get a sample from a distribution. We can generalize the implementation of these two approaches by using the `tl.logsoftmax_sample()` method. Let's briefly look at its implementation:\n\n```python\ndef logsoftmax_sample(log_probs, temperature=1.0):  # pylint: disable=invalid-name\n  \"\"\"Returns a sample from a log-softmax output, with temperature.\n\n  Args:\n    log_probs: Logarithms of probabilities (often coming from LogSofmax)\n    temperature: For scaling before sampling (1.0 = default, 0.0 = pick argmax)\n  \"\"\"\n  # This is equivalent to sampling from a softmax with temperature.\n  u = np.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n  g = -np.log(-np.log(u))\n  return np.argmax(log_probs + g * temperature, axis=-1)\n```\n\nThe key things to take away here are: 1. it gets random samples with the same shape as your input (i.e. `log_probs`), and 2. the amount of \"noise\" added to the input by these random samples is scaled by a `temperature` setting. You'll notice that setting it to `0` will just make the return statement equal to getting the argmax of `log_probs`. This will come in handy later. \n\n<a name=\"ex-6\"></a>\n### Exercise 6 - next_symbol\n\n**Instructions:** Implement the `next_symbol()` function that takes in the `input_tokens` and the `cur_output_tokens`, then return the index of the next word. You can click below for hints in completing this exercise.\n\n<details>    \n<summary>\n    <font size=\"3\" color=\"darkgreen\"><b>Click Here for Hints</b></font>\n</summary>\n<p>\n<ul>\n    <li>To get the next power of two, you can compute <i>2^log_2(token_length + 1)</i> . We add 1 to avoid <i>log(0).</i></li>\n    <li>You can use <i>np.ceil()</i> to get the ceiling of a float.</li>\n    <li><i>np.log2()</i> will get the logarithm base 2 of a value</li>\n    <li><i>int()</i> will cast a value into an integer type</li>\n    <li>From the model diagram in part 2, you know that it takes two inputs. You can feed these with this syntax to get the model outputs: <i>model((input1, input2))</i>. It's up to you to determine which variables below to substitute for input1 and input2. Remember also from the diagram that the output has two elements: [log probabilities, target tokens]. You won't need the target tokens so we assigned it to _ below for you. </li>\n    <li> The log probabilities output will have the shape: (batch size, decoder length, vocab size). It will contain log probabilities for each token in the cur_output_tokens and in addition - the log probabilities for the next tokens. For example, if cur_output_tokens is [1, 2] (of length 2), the model would output log probabilities for [1, 2, 5, 8] (of length 4 due to batch padding). To generate the next symbol, you just want to get the log probabilities associated with the next token after the cur_output_tokens (i.e. token 5 at index 2 since indexing starts at 0). You can slice the model output at [0, 2, :] to get this. It will be up to you to generalize this for any length of cur_output_tokens </li>\n</ul>\n","metadata":{}},{"cell_type":"code","source":"# UNQ_C6\n# GRADED FUNCTION\ndef next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\n    \"\"\"Returns the index of the next token.\n\n    Args:\n        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n        input_tokens (np.ndarray 1 x n_tokens): tokenized representation of the input sentence\n        cur_output_tokens (list): tokenized representation of previously translated words\n        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n            0.0: same as argmax, always pick the most probable token\n            1.0: sampling from the distribution (can sometimes say random things)\n\n    Returns:\n        int: index of the next token in the translated sentence\n        float: log probability of the next symbol\n    \"\"\"\n\n    ### START CODE HERE ###\n\n   # set the length of the current output tokens\n    token_length = len(cur_output_tokens)\n\n    # calculate next power of 2 for padding length \n    padded_length = np.power(2, int(np.ceil(np.log2(token_length + 1))))\n\n    # pad cur_output_tokens up to the padded_length\n    padded = cur_output_tokens + [0] * (padded_length - token_length)\n    \n    \n    # model expects the output to have an axis for the batch size in front so\n    # convert `padded` list to a numpy array with shape (None, <padded_length>) where\n    # None is a placeholder for the batch size\n    padded_with_batch = np.expand_dims(padded, axis=0)\n\n    # get the model prediction (remember to use the `NMAttn` argument defined above)\n    output, _ = NMTAttn((input_tokens, padded_with_batch))\n    \n    # get log probabilities from the last token output\n    log_probs = output[0, token_length, :]\n\n    # get the next symbol by getting a logsoftmax sample (*hint: cast to an int)\n    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n    \n    ### END CODE HERE ###\n\n    return symbol, float(log_probs[symbol])","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:34:09.410891Z","iopub.execute_input":"2023-06-11T05:34:09.411695Z","iopub.status.idle":"2023-06-11T05:34:09.421682Z","shell.execute_reply.started":"2023-06-11T05:34:09.411657Z","shell.execute_reply":"2023-06-11T05:34:09.420206Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now you will implement the `sampling_decode()` function. This will call the `next_symbol()` function above several times until the next output is the end-of-sentence token (i.e. `EOS`). It takes in an input string and returns the translated version of that string.\n\n<a name=\"ex-7\"></a>\n### Exercise 7 - sampling_decode\n\n**Instructions**: Implement the `sampling_decode()` function.","metadata":{}},{"cell_type":"code","source":"# UNQ_C7\n# GRADED FUNCTION\ndef sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n    \"\"\"Returns the translated sentence.\n\n    Args:\n        input_sentence (str): sentence to translate.\n        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n            0.0: same as argmax, always pick the most probable token\n            1.0: sampling from the distribution (can sometimes say random things)\n        vocab_file (str): filename of the vocabulary\n        vocab_dir (str): path to the vocabulary file\n\n    Returns:\n        tuple: (list, str, float)\n            list of int: tokenized version of the translated sentence\n            float: log probability of the translated sentence\n            str: the translated sentence\n    \"\"\"\n    \n    ### START CODE HERE ###\n    \n    # encode the input sentence\n    input_tokens = tokenize(input_sentence,vocab_file,vocab_dir)\n    \n    # initialize the list of output tokens\n    cur_output_tokens = []\n    \n    # initialize an integer that represents the current output index\n    cur_output = 0\n    \n    # Set the encoding of the \"end of sentence\" as 1\n    EOS = 1\n    \n    # check that the current output is not the end of sentence token\n    while cur_output != EOS:\n        \n        # update the current output token by getting the index of the next word (hint: use next_symbol)\n        cur_output, log_prob = next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature)\n        \n        # append the current output token to the list of output tokens\n        cur_output_tokens.append(cur_output)\n    \n    # detokenize the output tokens\n    sentence = detokenize(cur_output_tokens, vocab_file, vocab_dir)\n    \n    ### END CODE HERE ###\n    \n    return cur_output_tokens, log_prob, sentence","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:34:10.911118Z","iopub.execute_input":"2023-06-11T05:34:10.911592Z","iopub.status.idle":"2023-06-11T05:34:10.921995Z","shell.execute_reply.started":"2023-06-11T05:34:10.911555Z","shell.execute_reply":"2023-06-11T05:34:10.920747Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# Test the function above. Try varying the temperature setting with values from 0 to 1.\n# Run it several times with each setting and see how often the output changes. \nsampling_decode(\"I love languages.\", NMTAttn=model, temperature=0.0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)","metadata":{"execution":{"iopub.status.busy":"2023-06-11T05:34:11.149151Z","iopub.execute_input":"2023-06-11T05:34:11.149945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}